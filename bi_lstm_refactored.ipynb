{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "bidirectional-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuFernandez/twin-peaks-generator/blob/master/bi_lstm_refactored.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IZzJr-AWOuC"
      },
      "source": [
        "# Text Generation using Bidirectional LSTM and Doc2Vec models\n",
        "\n",
        "The purpose of [this article](https://medium.com/@david.campion/text-generation-using-bidirectional-lstm-and-doc2vec-models-1-3-8979eb65cb3a) is to discuss about text generation, using machine learning approaches, especially neural networks.\n",
        "\n",
        "It is not the first article about it, and probably not the last. Actually, there is a lot of litterature about text generation using \"AI\" techniques, and some codes are available to generate texts from existing novels, trying to create new chapters for **\"Game of Thrones\"**, **\"Harry Potter\"**, or a new piece in the style of **Shakespears**. Sometimes with interesting results.\n",
        "\n",
        "Mainly, these approaches are using classic LSTM networks, and the are pretty fun to be experimented.\n",
        "\n",
        "However, generated texts provide a taste of unachievement. Generated sentences seems quite right, whith correct grammar and syntax, as if the neural network was understanding correctly the structure of a sentence. But the whole new text does not have great sense. If it is not complete nosense. \n",
        "\n",
        "This problem could come from the approach itself, using only LSTM to generate text word by word. But how can we improve them ? In this article, I will try to investigate a new way to generate sentences.\n",
        "\n",
        "It does not mean that I will use something completely different from LTSM : I am not, I will use LTSM network to generate sequences of words. However I will try to go further than a classic LSTM neural network and I will use an additional neural network (LSTM again), to select the best phrases.\n",
        "\n",
        "Then, this article can be used as a tutorial. It describes :\n",
        " 1. **how to train a neural network to generate sentences** (i.e. sequences of words), based on existing novels. I will use a bidirectional LSTM Architecture to perform that.\n",
        " 2. **how to train a neural network to select the best next sentence for given paragraph** (i.e. a sequence of sentences). I will also use a bidirectional LSTM archicture, in addition to a Doc2Vec model of the target novels.\n",
        "\n",
        "\n",
        "### Note about Data inputs\n",
        "As data inputs, I will not use texts which are not free in term of intellectual properties. So I will not train the solution to create a new chapter for **\"Game of Throne\"** or **\"Harry Potter\"**.\n",
        "Sorry about that, there is plenty of \"free\" text to perform such texts generation exercices and we can dive into the [Gutemberg project](http://www.gutenberg.org), which provides huge amount of texts (from [William Shakespears](http://www.gutenberg.org/ebooks/author/65) to [H.P. Lovecraft](http://www.gutenberg.org/ebooks/author/34724), or other great authors).\n",
        "\n",
        "However, I am also a french author of fantasy and Science fiction. So I will use my personnal material to create a new chapter of my stories, hoping it can help me in my next work!\n",
        "\n",
        "So, I will base this exercice on **\"Artistes et Phalanges\"**, a french fantasy novel I wrote over the 10 past years, wich I hope will be fair enough in term of data inputs. It contains more than 830 000 charaters.\n",
        "\n",
        "By the way, if you're a french reader and found of fantasy, you can find it on iBook store and Amazon Kindle for free... Please note I provide also the data for free on my github repository. Enjoy it!\n",
        "\n",
        "## 1. a Neural Network for Generating Sentences\n",
        "\n",
        "The first step is to generate sentences in the style of a given author.\n",
        "\n",
        "There is huge litterature about it, espacially using LSTM to perform such task. As this kind of network are working well for this job, we will use them.\n",
        "\n",
        "The purpose of this note is not to deep dive into LSTM description, you can find very great article about them and I suggest you to read [this article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) from Andrej Karpathy.\n",
        "\n",
        "You can also find easily existing code to perform text generation using LSTM. On my github, you can find two tutorials, one using [Tensorflow](https://github.com/campdav/text-rnn-tensorflow), and another one using [Keras](https://github.com/campdav/text-rnn-keras) (over tensorflow), that is easier to understand.\n",
        "\n",
        "For this first part of these exercice, I will re-use these materials, but with few improvements :\n",
        " - Instead of a simple _LSTM_, I will use a _bidirectional LSTM_. This network configuration converge faster than a single LSTM (less epochs are required), and from empiric tests, seems better in term of accuracy. You can have a look at [this article](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/) from Jason Brownlee, for a good tutorial about bidirectional LSTM.\n",
        " - I will use Keras, which require less complexity to create the network of is more readible than conventional Tensorflow code.\n",
        "\n",
        "### 1.1. What is the neural network task in our case ?\n",
        "\n",
        "LSTM (Long Short Term Memory) are very good for analysing sequences of values and predicting the next values from them. For example, LSTM could be a very good choice if you want to predict the very next point of a given time serie (assuming a correlation exist in the sequence).\n",
        "\n",
        "Talking about sentences and texts ; phrases (sentences) are basically sequences of words. So, it is natural to assume LSTM could be usefull to generate the next word of a given sentence.\n",
        "\n",
        "In summary, the objective of a LSTM neural network in this situation is to guess the next word of a given sentence.\n",
        "\n",
        "For example:\n",
        "What is the next word of this following sentence : \"he is walking down the\"\n",
        "\n",
        "Our neural net will take the sequence of words as input : \"he\", \"is\", \"walking\", ...\n",
        "Its ouput will be a matrix providing the probability for each word from the dictionnary to be the next one of the given sentence.\n",
        "\n",
        "Then, how will we build the complete text ? Simply iterating the process, by switching the setence by one word, including the new guessed word at its end. Then, we guess a new word for this new sentence. ad vitam aeternam.\n",
        "\n",
        "### 1.1.1. Process\n",
        "\n",
        "In order to do that, first, we build a dictionary containing all words from the novels we want to use.\n",
        "\n",
        " 1. read the data (the novels we want to use),\n",
        " 1. create the dictionnary of words,\n",
        " 2. create the list of sentences,\n",
        " 3. create the neural network,\n",
        " 4. train the neural network,\n",
        " 5. generate new sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzDrR6o24gX1",
        "outputId": "1e7c1900-80b1-46b7-b868-961b2bdc0994"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74f1O2AYWOuH"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, Input, Flatten, Bidirectional\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
        "from keras.metrics import categorical_accuracy\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "from six.moves import cPickle\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWZS9VRdTs0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdda61d4-9e2a-40c8-b4ae-8531ec79468e"
      },
      "source": [
        "#chequeo estar usando la gpu y me fijo cuál\n",
        "import tensorflow as tf \n",
        "print(tf.test.gpu_device_name())\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 16126611674365283723\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14509932544\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 409739630373848938\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQlhpYLWOuI"
      },
      "source": [
        "We have raw text and a lot of things have to be done to use them: split them in words list, etc.\n",
        "In order to do that, I use the spacy library which is incredible to deal with texts. For this exercice, I will only use very few options from spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tBCLWPcZWOuI"
      },
      "source": [
        "#import spacy, and english model\n",
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A66-e91WOuJ"
      },
      "source": [
        "# parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "608Hi9WdwsUj"
      },
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "tz_bsas = pytz.timezone('America/Buenos_Aires') \n",
        "datetime_bsas = datetime.now(tz_bsas)\n",
        "date = datetime_bsas.strftime(\"%Y-%m-%d-%Hh_%Mm\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "V6S-FjXjWOuJ"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/twin-peaks-generator/data'# data directory containing input.txt\n",
        "save_dir = '/content/drive/MyDrive/twin-peaks-generator/save/' + date # directory to store models\n",
        "os.mkdir(save_dir)\n",
        "seq_length = 20 # sequence length\n",
        "sequences_step = 1 #step to create sequences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GSGnk4ayWOuK"
      },
      "source": [
        "# file_list = [\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\"]\n",
        "file_list = [\"101\",\"102\",\"103\",\"104\"]\n",
        "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLi_ZvgkWOuK"
      },
      "source": [
        "# read data\n",
        "\n",
        "I create a specific function to create a list of words from raw text. I use spacy library, with a specific function to retrieve only lower character of the words and remove carriage returns (\\n).\n",
        "\n",
        "I am doing that because I want to reduce the number of potential words in my dictionnary, and I assume we do not have to avoid capital letters. Indeed, they are only part of the syntax of the text, it's shape, and do not deals with its sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CLpVmxTGWOuK"
      },
      "source": [
        "def create_wordlist(doc):\n",
        "    wl = []\n",
        "    for word in doc:\n",
        "        if word.text not in (\"\\n\",\"\\n\\n\", '\\n\\n\\n', '\\n\\n\\n\\n', '\\n\\n\\n\\n\\n','\\u2009','\\xa0'):\n",
        "            wl.append(word.text.lower())\n",
        "    return wl"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwp-SqxEWOuL"
      },
      "source": [
        "Create the list of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7tSqMUNfWOuL"
      },
      "source": [
        "wordlist = []\n",
        "for file_name in file_list:\n",
        "    input_file = os.path.join(data_dir, file_name + \".txt\")\n",
        "    #read data\n",
        "    with codecs.open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "    #create sentences\n",
        "    doc = nlp(data)\n",
        "    wl = create_wordlist(doc)\n",
        "    wordlist = wordlist + wl"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiSXxyky7HYB",
        "outputId": "f1943173-e828-4505-e794-7c6ae50546d5"
      },
      "source": [
        "len(wordlist)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1CobyAiWOuM"
      },
      "source": [
        "## Create dictionary\n",
        "\n",
        "The first step is to create the dictionnary, it means, the list of all words contained in texts. For each word, we will assign an index to it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_g7kc5RWOuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6470fde5-f4ef-499e-d4ae-c1ad3aab2ce0"
      },
      "source": [
        "# count the number of words\n",
        "word_counts = collections.Counter(wordlist)\n",
        "\n",
        "# Mapping from index to word : that's the vocabulary\n",
        "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "\n",
        "# Mapping from word to index\n",
        "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "words = [x[0] for x in word_counts.most_common()]\n",
        "\n",
        "#size of the vocabulary\n",
        "vocab_size = len(words)\n",
        "print(\"vocab size: \", vocab_size)\n",
        "\n",
        "#save the words and vocabulary\n",
        "with open(os.path.join(vocab_file), 'wb') as f:\n",
        "    cPickle.dump((words, vocab, vocabulary_inv), f)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  4794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNDKcN_ZndB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df3172b-ed8d-46b3-e573-3722ea4b47db"
      },
      "source": [
        "print(vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, '!': 1, '\"': 2, '#': 3, '%': 4, \"'\": 5, \"'bout\": 6, \"'d\": 7, \"'em\": 8, \"'ll\": 9, \"'m\": 10, \"'re\": 11, \"'s\": 12, \"'ve\": 13, '(': 14, ')': 15, ',': 16, '-': 17, '-(stay': 18, '--': 19, '--and': 20, '--i': 21, '--jerry': 22, '--no': 23, '--sun': 24, '-james': 25, '-norma': 26, '-sweetie': 27, '.': 28, '..': 29, '...': 30, '.32': 31, '/': 32, '1': 33, '100': 34, '12': 35, '12:27': 36, '12:30': 37, '18': 38, '19': 39, '1942': 40, '1950': 41, '1959': 42, '1988': 43, '1989': 44, '1:18': 45, '22': 46, '2:24': 47, '3': 48, '315': 49, '36-footer': 50, '52': 51, '5th': 52, '64': 53, '6:18': 54, '8:17': 55, '9': 56, '9/4': 57, ':': 58, ';': 59, '?': 60, 'a': 61, 'a--': 62, 'a.f.o.': 63, 'a.m.': 64, 'a.s.a.p.': 65, 'abandoned': 66, 'abiding': 67, 'abject': 68, 'able': 69, 'about': 70, 'above': 71, 'abruptly': 72, 'absent': 73, 'absentmindedly': 74, 'absolute': 75, 'absolutely': 76, 'accept': 77, 'access': 78, 'accident': 79, 'accidental': 80, 'accommodations': 81, 'accompanied': 82, 'account': 83, 'accounts': 84, 'accurate': 85, 'acetylcholine': 86, 'achieving': 87, 'aching': 88, 'acids': 89, 'acknowledge': 90, 'acquaintance': 91, 'acquainted': 92, 'across': 93, 'acrylic': 94, 'act': 95, 'acting': 96, 'action': 97, 'actions': 98, 'activated': 99, 'actually': 100, 'add': 101, 'addicted': 102, 'addictive': 103, 'adding': 104, 'addition': 105, 'addressed': 106, 'adds': 107, 'admire': 108, 'advance': 109, 'advancing': 110, 'adversity': 111, 'advice': 112, 'advised': 113, 'affairs': 114, 'affected': 115, 'affection': 116, 'afford': 117, 'afraid': 118, 'after': 119, 'afternoon': 120, 'afternoons': 121, 'afterwards': 122, 'again': 123, 'against': 124, 'age': 125, 'aged': 126, 'agent': 127, 'aggressiveness': 128, 'agnostic': 129, 'ago': 130, 'agonizing': 131, 'agreed': 132, 'ah': 133, 'ahead': 134, 'ahh': 135, 'ahoooooooooooh': 136, 'aid': 137, 'aim': 138, 'air': 139, 'airplane': 140, 'alarm': 141, 'alarmed': 142, 'alarmingly': 143, 'albert': 144, 'alert': 145, 'algebra': 146, 'alibi': 147, 'alike': 148, 'all': 149, 'allows': 150, 'almost': 151, 'alone': 152, 'along': 153, 'aloud': 154, 'alphabetically': 155, 'already': 156, 'alright': 157, 'also': 158, 'altercation': 159, 'aluminum': 160, 'always': 161, 'always-': 162, 'am': 163, 'amateur': 164, 'amazed': 165, 'amazement': 166, 'amazing': 167, 'ambient': 168, 'ambiguously': 169, 'ambitions': 170, 'amen': 171, 'amidst': 172, 'ammo': 173, 'ammunition': 174, 'among': 175, 'amount': 176, 'amphetamines': 177, 'amused': 178, 'amusement': 179, 'an': 180, 'ancient': 181, 'and': 182, 'andrew': 183, 'andwarrants': 184, 'andy': 185, 'angel': 186, 'anger': 187, 'angle': 188, 'angrily': 189, 'angry': 190, 'animal': 191, 'ankles': 192, 'annette': 193, 'announcer': 194, 'annual': 195, 'another': 196, 'answer': 197, 'answered': 198, 'answers': 199, 'anticipate': 200, 'antlers': 201, 'anxiously': 202, 'any': 203, 'anybody': 204, 'anymore': 205, 'anyone': 206, 'anything': 207, 'anyway': 208, 'anywhere': 209, 'apart': 210, 'apartment': 211, 'apb': 212, 'apiece': 213, 'apparently': 214, 'appear': 215, 'appears': 216, 'appointment': 217, 'appreciate': 218, 'appreciates': 219, 'appreciation': 220, 'appreciatively': 221, 'approach': 222, 'approaching': 223, 'appropriate': 224, 'approval': 225, 'approximating': 226, 'april': 227, 'apron': 228, 'arcs': 229, 'ardent': 230, 'ardently': 231, 'are': 232, 'area': 233, 'argument': 234, 'aria': 235, 'arky': 236, 'arm': 237, 'armed': 238, 'arms': 239, 'armstrong': 240, 'around': 241, 'arrange': 242, 'arranged': 243, 'arrangements': 244, 'arrest': 245, 'arrive': 246, 'arrived': 247, 'arrives': 248, 'arriving': 249, 'arrow': 250, 'arson': 251, 'art': 252, 'arteries': 253, 'artificial': 254, 'artist': 255, 'as': 256, 'ascot': 257, 'ashamed': 258, 'ashore': 259, 'ashtray': 260, 'aside': 261, 'ask': 262, 'asked': 263, 'asleep': 264, 'aspiration': 265, 'ass': 266, 'assault': 267, 'assaulted': 268, 'assembly': 269, 'assent': 270, 'assist': 271, 'assistance': 272, 'assistant': 273, 'assistants': 274, 'assisted': 275, 'associates': 276, 'association': 277, 'assume': 278, 'assuming': 279, 'astonished': 280, 'at': 281, 'atavistic': 282, 'ate': 283, 'athlete': 284, 'atmosphere': 285, 'attached': 286, 'attaches': 287, 'attacked': 288, 'attempt': 289, 'attended': 290, 'attention': 291, 'attired': 292, 'attitude': 293, 'attorney': 294, 'attractive': 295, 'audio': 296, 'audrey': 297, 'aunt': 298, 'autodialer': 299, 'automobiles': 300, 'autopsy': 301, 'available': 302, 'awaiting': 303, 'away': 304, 'awe': 305, 'awful': 306, 'awhile': 307, 'awkward': 308, 'awoke': 309, 'axe': 310, 'b': 311, 'b.g': 312, 'baby': 313, 'back': 314, 'background': 315, 'backhands': 316, 'backing': 317, 'backlit': 318, 'backup': 319, 'backward': 320, 'backwater': 321, 'bacon': 322, 'bad': 323, 'badge': 324, 'badges': 325, 'badly': 326, 'bag': 327, 'bags': 328, 'baguette': 329, 'bail': 330, 'ball': 331, 'ballpark': 332, 'balls': 333, 'band': 334, 'bandage': 335, 'bang': 336, 'bank': 337, 'bankrupt': 338, 'bankruptcy': 339, 'banks': 340, 'banner': 341, 'baptized': 342, 'bar': 343, 'bare': 344, 'barely': 345, 'bark': 346, 'barks': 347, 'baron': 348, 'barring': 349, 'bars': 350, 'bartender': 351, 'base': 352, 'basement': 353, 'basket': 354, 'bastard': 355, 'bath': 356, 'bathrobe': 357, 'bathroom': 358, 'be': 359, 'beach': 360, 'beam': 361, 'beaming': 362, 'beams': 363, 'bear': 364, 'bearclaw': 365, 'bearclaws': 366, 'beast': 367, 'beat': 368, 'beats': 369, 'beautiful': 370, 'beauty': 371, 'beaver': 372, 'because': 373, 'beclouded': 374, 'become': 375, 'bed': 376, 'bedroom': 377, 'beds': 378, 'bedside': 379, 'bedspread': 380, 'been': 381, 'beep': 382, 'beer': 383, 'beers': 384, 'beestung': 385, 'before': 386, 'begging': 387, 'begin': 388, 'beginning': 389, 'begins': 390, 'beguiling': 391, 'begun': 392, 'behalf': 393, 'behave': 394, 'behavior': 395, 'behind': 396, 'beige': 397, 'being': 398, 'believe': 399, 'believed': 400, 'believes': 401, 'believeth': 402, 'bell': 403, 'bellhops': 404, 'belong': 405, 'belongs': 406, 'below': 407, 'ben': 408, 'bench': 409, 'bend': 410, 'bends': 411, 'beneath': 412, 'benefit': 413, 'benefits': 414, 'benignly': 415, 'benjamin': 416, 'bent': 417, 'berg': 418, 'bernard': 419, 'bernie': 420, 'beside': 421, 'besides': 422, 'best': 423, 'bestsmelling': 424, 'bet': 425, \"bet'cha\": 426, 'betrayed': 427, 'better': 428, 'betty': 429, 'between': 430, 'beyond': 431, 'bicker': 432, 'bidets': 433, 'big': 434, 'biggest': 435, 'bike': 436, 'billy': 437, 'bind': 438, 'binds': 439, 'binocular': 440, 'binoculars': 441, 'bird': 442, 'birdcall': 443, 'birds': 444, 'bit': 445, 'bite': 446, 'bites': 447, 'biting': 448, 'bitter': 449, 'black': 450, 'blackboard': 451, 'blackfeet': 452, 'blackfoot': 453, 'blackie': 454, 'blah': 455, 'blanches': 456, 'blank': 457, 'blanket': 458, 'bless': 459, 'blessed': 460, 'blessings': 461, 'blew': 462, 'blindly': 463, 'blinds': 464, 'blinking': 465, 'blinks': 466, 'blithering': 467, 'block': 468, 'blocks': 469, 'blood': 470, 'bloodstain': 471, 'bloodstained': 472, 'bloody': 473, 'bloom': 474, 'blow': 475, 'blowing': 476, 'blows': 477, 'blue': 478, 'blueprints': 479, 'blues': 480, 'blushes': 481, 'blushing': 482, 'board': 483, 'boat': 484, 'bob': 485, 'bobby': 486, 'bobcat': 487, 'body': 488, 'bogus': 489, 'boiler': 490, 'bold': 491, 'bolt': 492, 'bombshell': 493, 'boob': 494, 'book': 495, 'bookhouse': 496, 'books': 497, 'bookshelf': 498, 'bookshelves': 499, 'booth': 500, 'boots': 501, 'bop': 502, 'bopper': 503, 'border': 504, 'bordered': 505, 'bored': 506, 'born': 507, 'boss': 508, 'both': 509, 'bother': 510, 'bothering': 511, 'bottle': 512, 'bottom': 513, 'bouffant': 514, 'bought': 515, 'bound': 516, 'bounds': 517, 'bouquet': 518, 'bourbon': 519, 'bow': 520, 'bowl': 521, 'box': 522, 'boxer': 523, 'boxes': 524, 'boy': 525, 'boyfriend': 526, 'boys': 527, 'bracelet': 528, 'brag': 529, 'brains': 530, 'brakelights': 531, 'brakes': 532, 'branch': 533, 'brand': 534, 'brandeis': 535, 'brando': 536, 'brass': 537, 'brassy': 538, 'brave': 539, 'brawl': 540, 'brays': 541, 'break': 542, 'breakfast': 543, 'breaking': 544, 'breaks': 545, 'breast': 546, 'breath': 547, 'breathe': 548, 'breathless': 549, 'breed': 550, 'breeze': 551, 'breezes': 552, 'breezily': 553, 'brennan': 554, 'brewing': 555, 'brewskis': 556, 'brie': 557, 'brief': 558, 'briefly': 559, 'briefs': 560, 'brigade': 561, 'briggs': 562, 'bright': 563, 'brightening': 564, 'brightens': 565, 'brightly': 566, 'brilliant': 567, 'bring': 568, 'bringing': 569, 'brings': 570, 'broke': 571, 'broken': 572, 'brother': 573, 'brothers': 574, 'brought': 575, 'brown': 576, 'bruised': 577, 'bruises': 578, 'brushing': 579, 'brutally': 580, 'buck': 581, 'bucket': 582, 'bucks': 583, 'buddhism': 584, 'buds': 585, 'buffalo': 586, 'bugging': 587, 'building': 588, 'built': 589, 'bulbs': 590, 'bulgarian': 591, 'bulky': 592, 'bulldogged': 593, 'bullets': 594, 'bumblebee': 595, 'bumpkin': 596, 'bureau': 597, 'burgs': 598, 'burial': 599, 'buried': 600, 'burly': 601, 'burma': 602, 'burned': 603, 'burning': 604, 'burnished': 605, 'burns': 606, 'burrows': 607, 'burst': 608, 'bursts': 609, 'bury': 610, 'burying': 611, 'business': 612, 'businessman': 613, 'bust': 614, 'bustling': 615, 'busy': 616, 'but': 617, 'butt': 618, 'butte': 619, 'butter': 620, 'buttkicked': 621, 'button': 622, 'buttons': 623, 'buy': 624, 'buzz': 625, 'buzzes': 626, 'buzzing': 627, 'by': 628, 'bzzzzz': 629, 'c': 630, 'ca': 631, 'cab': 632, 'cabinet': 633, 'cable': 634, 'cajole': 635, 'cake': 636, 'calculate': 637, 'calculated': 638, 'calhoun': 639, 'call': 640, 'called': 641, 'calling': 642, 'calls': 643, 'calm': 644, 'calmly': 645, 'came': 646, 'camera': 647, 'can': 648, 'canada': 649, 'canadian': 650, 'candidly': 651, 'candies': 652, 'candlelight': 653, 'candles': 654, 'candy': 655, 'canuck': 656, 'capable': 657, 'captain': 658, 'captured': 659, 'car': 660, 'card': 661, 'cards': 662, 'care': 663, 'cared': 664, 'career': 665, 'careful': 666, 'carefully': 667, 'caressing': 668, 'caretaker': 669, 'carpet': 670, 'carry': 671, 'carrying': 672, 'cars': 673, 'cartridge': 674, 'carving': 675, 'case': 676, 'cases': 677, 'cash': 678, 'cashews': 679, 'casino': 680, 'casket': 681, 'caskets': 682, 'cassette': 683, 'catch': 684, 'catches': 685, 'catching': 686, 'catherine': 687, 'catty': 688, 'caught': 689, 'cause': 690, 'cautiously': 691, 'ceases': 692, 'ceiling': 693, 'cell': 694, 'cemetery': 695, 'center': 696, 'centuries': 697, 'ceremony': 698, 'certain': 699, 'certainly': 700, 'chain': 701, 'chair': 702, 'chairing': 703, 'chairs': 704, 'chalk': 705, 'chambermaids': 706, 'champion-(takes': 707, 'chance': 708, 'chances': 709, 'change': 710, 'changed': 711, 'changing': 712, 'chaos': 713, 'character': 714, 'characterize': 715, 'characters': 716, 'charge': 717, 'charged': 718, 'charging': 719, 'charity': 720, 'charm': 721, 'charming': 722, 'chasing': 723, 'chat': 724, 'check': 725, 'checkbook': 726, 'checked': 727, 'checking': 728, 'cheek': 729, 'cheer': 730, 'cheerleader': 731, 'cherry': 732, 'chest': 733, 'chet': 734, 'chew': 735, 'chewing': 736, 'chicken': 737, 'chickenfeed': 738, 'child': 739, 'childhood': 740, 'children': 741, 'chin': 742, 'china': 743, 'chinese': 744, 'chip': 745, 'chips': 746, 'chocolate': 747, 'choice': 748, 'choices': 749, 'chomps': 750, 'choose': 751, 'chop': 752, 'chopping': 753, 'chowderhead': 754, 'christ': 755, 'chuckle': 756, 'church': 757, 'cider': 758, 'cigar': 759, 'cigarette': 760, 'cigarettes': 761, 'circle': 762, 'circles': 763, 'circumstance': 764, 'circus': 765, 'citizens': 766, 'city': 767, 'clad': 768, 'clang': 769, 'clanks': 770, 'claps': 771, 'clarence': 772, 'class': 773, 'classroom': 774, 'classy': 775, 'claw': 776, 'clay': 777, 'clean': 778, 'cleaner': 779, 'cleaning': 780, 'cleans': 781, 'clear': 782, 'cleared': 783, 'clearing': 784, 'clears': 785, 'clench': 786, 'cliches': 787, 'cliches\"-cooper': 788, 'client': 789, 'climbs': 790, 'clinic': 791, 'clink': 792, 'clip': 793, 'clock': 794, 'clocking': 795, 'close': 796, 'closed': 797, 'closer': 798, 'closes': 799, 'closest': 800, 'closet': 801, 'closing': 802, 'closure': 803, 'clothes': 804, 'clouds': 805, 'club': 806, 'clubhouse': 807, 'clucking': 808, 'clues': 809, 'clutch': 810, 'clutches': 811, 'clutching': 812, 'coast': 813, 'coat': 814, 'coated': 815, 'cocaine': 816, 'cocks': 817, 'coconut': 818, 'coconuts': 819, 'code': 820, 'coexisting': 821, 'coffee': 822, 'coffees': 823, 'coffin': 824, 'coin': 825, 'coincidence': 826, 'coital': 827, 'cold': 828, 'cole': 829, 'collapses': 830, 'collar': 831, 'colleague': 832, 'collectively': 833, 'collects': 834, 'colleen': 835, 'college': 836, 'collides': 837, 'colonel': 838, 'colt': 839, 'coma': 840, 'combat': 841, 'come': 842, 'comes': 843, 'comfort': 844, 'comfortable': 845, 'comforted': 846, 'comforting': 847, 'comforts': 848, 'comin': 849, 'coming': 850, 'comments': 851, 'commercial': 852, 'commitment': 853, 'common': 854, 'commotion': 855, 'communist': 856, 'community': 857, 'company': 858, 'compare': 859, 'compartment': 860, 'compassion': 861, 'compassionate': 862, 'completely': 863, 'completing': 864, 'complexion': 865, 'composing': 866, 'composure': 867, 'compulsively': 868, 'computer': 869, 'concentrate': 870, 'concern': 871, 'concerning': 872, 'conclude': 873, 'concluded': 874, 'conclusion': 875, 'concussion': 876, 'condition': 877, 'conducting': 878, 'conference': 879, 'confide': 880, 'confident': 881, 'confidential': 882, 'confidentiality': 883, 'confidentially': 884, 'confirm': 885, 'confiscate': 886, 'confounds': 887, 'confronts': 888, 'confused': 889, 'confusion': 890, 'congenital': 891, 'connected': 892, 'connection': 893, 'connections': 894, 'consider': 895, 'considerably': 896, 'consideration': 897, 'considers': 898, 'consistent': 899, 'console': 900, 'consoles': 901, 'constitute': 902, 'construction': 903, 'consulting': 904, 'consults': 905, 'contact': 906, 'contain': 907, 'containing': 908, 'contemplative': 909, 'contents': 910, 'continue': 911, 'continues': 912, 'continuing': 913, 'contract': 914, 'contrariness': 915, 'contribute': 916, 'contributing': 917, 'control': 918, 'controls': 919, 'convenience': 920, 'convent': 921, 'conversation': 922, 'convert': 923, 'convince': 924, 'convinced': 925, 'cook': 926, 'cookie': 927, 'cooking': 928, 'cool': 929, 'coop': 930, 'cooper': 931, 'coordination': 932, 'coots': 933, 'corn': 934, 'corner': 935, 'correct': 936, 'correctly': 937, 'corridor': 938, 'corridors': 939, 'corvette': 940, 'cosmetics': 941, 'cost': 942, 'costs': 943, 'cot': 944, 'cots': 945, 'cottages': 946, 'cotton': 947, 'couch': 948, 'could': 949, 'counsel': 950, 'count': 951, 'counter': 952, 'counties': 953, 'countries': 954, 'country': 955, 'couple': 956, 'couples': 957, 'course': 958, 'cousin': 959, 'covered': 960, 'covers': 961, 'cowboy': 962, 'cozy': 963, 'cracker': 964, 'cracking': 965, 'cradling': 966, 'crap': 967, 'crash': 968, 'crazy': 969, 'cream': 970, 'creams': 971, 'credit': 972, 'credits': 973, 'creepy': 974, 'cremate': 975, 'cretins': 976, 'cries': 977, 'crime': 978, 'criminal': 979, 'crisis': 980, 'crisp': 981, 'crispy': 982, 'crop': 983, 'cross': 984, 'crosses': 985, 'crossing': 986, 'crouched': 987, 'crouches': 988, 'crowd': 989, 'crows': 990, 'cruching': 991, 'cruel': 992, 'cruise': 993, 'cruiser': 994, 'crusty': 995, 'cry': 996, 'crying': 997, 'cryptically': 998, 'cube': 999, 'cuff': 1000, 'culture': 1001, 'cup': 1002, 'cupped': 1003, 'cups': 1004, 'curious': 1005, 'current': 1006, 'curtain': 1007, 'curtains': 1008, 'custody': 1009, 'custom': 1010, 'customer': 1011, 'customers': 1012, 'customized': 1013, 'customs': 1014, 'cut': 1015, 'cute': 1016, 'cutest': 1017, 'cuts': 1018, 'cutting': 1019, 'cycle': 1020, 'dad': 1021, 'daddy': 1022, 'dalai': 1023, 'dale': 1024, 'damaged': 1025, 'damn': 1026, 'dance': 1027, 'dancer': 1028, 'dances': 1029, 'dancing': 1030, 'dandy': 1031, 'danger': 1032, 'dangerous': 1033, 'dark': 1034, 'darkens': 1035, 'darkness': 1036, 'darlene': 1037, 'darling': 1038, 'darn': 1039, 'dart': 1040, 'date': 1041, 'daughter': 1042, 'daughters': 1043, 'dawn': 1044, 'day': 1045, 'days': 1046, 'de': 1047, 'dead': 1048, 'deafening': 1049, 'deal': 1050, 'dealing': 1051, 'deals': 1052, 'dealt': 1053, 'dear': 1054, 'death': 1055, 'debonair': 1056, 'debt': 1057, 'decade': 1058, 'deceitful': 1059, 'decency': 1060, 'decent': 1061, 'decide': 1062, 'decided': 1063, 'decides': 1064, 'decision': 1065, 'decked': 1066, 'deckhand': 1067, 'declines': 1068, 'dedicated': 1069, 'deductive': 1070, 'deep': 1071, 'deepest': 1072, 'deeply': 1073, 'defending': 1074, 'defense': 1075, 'defensive': 1076, 'definitely': 1077, 'definition': 1078, 'degree': 1079, 'deleted': 1080, 'delicate': 1081, 'delight': 1082, 'delivered': 1083, 'delivering': 1084, 'delivery': 1085, 'demonstrate': 1086, 'demonstrates': 1087, 'demonstrating': 1088, 'demoralized': 1089, 'dental': 1090, 'department': 1091, 'departmental': 1092, 'departs': 1093, 'depend': 1094, 'depends': 1095, 'deposit': 1096, 'deputies': 1097, 'deputy': 1098, 'der': 1099, 'descend': 1100, 'descends': 1101, 'describe': 1102, 'describing': 1103, 'description': 1104, 'design': 1105, 'desire': 1106, 'desk': 1107, 'despair': 1108, 'desperate': 1109, 'desperately': 1110, 'despite': 1111, 'destination': 1112, 'destroy': 1113, 'destroyed': 1114, 'detached': 1115, 'detailed': 1116, 'determined': 1117, 'devilish': 1118, 'devotion': 1119, 'dialogue': 1120, 'dials': 1121, 'diane': 1122, 'diary': 1123, 'did': 1124, 'die': 1125, 'died': 1126, 'dies': 1127, 'diesel': 1128, 'diet': 1129, 'dieth': 1130, 'difference': 1131, 'different': 1132, 'difficult': 1133, 'difficulty': 1134, 'dig': 1135, 'digestive': 1136, 'digging': 1137, 'dignity': 1138, 'digs': 1139, \"dimm'd\": 1140, 'diner': 1141, 'dining': 1142, 'dinner': 1143, 'dinners': 1144, 'dips': 1145, 'direct': 1146, 'directed': 1147, 'direction': 1148, 'directions': 1149, 'directly': 1150, 'directs': 1151, 'dirt': 1152, 'dirty': 1153, 'disappear': 1154, 'disappoint': 1155, 'disappointed': 1156, 'disaster': 1157, 'discards': 1158, 'discernable': 1159, 'discharges': 1160, 'discourages': 1161, 'discover': 1162, 'discovered': 1163, 'discovers': 1164, 'discovery': 1165, 'discreet': 1166, 'discreetly': 1167, 'discretely': 1168, 'discuss': 1169, 'discussed': 1170, 'discussion': 1171, 'diseased': 1172, 'disguised': 1173, 'dishes': 1174, 'dismay': 1175, 'disperse': 1176, 'displayed': 1177, 'displays': 1178, 'displeased': 1179, 'dissolve': 1180, 'dissolved': 1181, 'distance': 1182, 'distant': 1183, 'distinctive': 1184, 'distinguished': 1185, 'distorted': 1186, 'distress': 1187, 'distressed': 1188, 'distributes': 1189, 'disturb': 1190, 'disturbed': 1191, 'disturbing': 1192, 'division': 1193, 'divorce': 1194, 'do': 1195, 'doc': 1196, 'dock': 1197, 'doctor': 1198, 'document': 1199, 'does': 1200, 'does--': 1201, 'dog': 1202, 'doggone': 1203, 'doing': 1204, 'dollar': 1205, 'dollars': 1206, 'dolts': 1207, 'domino': 1208, \"don't\": 1209, \"don't-(screams\": 1210, 'donald': 1211, 'done': 1212, 'donna': 1213, 'donut': 1214, 'donuts': 1215, 'door': 1216, 'doorbell': 1217, 'doors': 1218, 'doorway': 1219, 'doozy': 1220, 'dots': 1221, 'double': 1222, 'doublecross': 1223, 'doubles': 1224, 'doubt': 1225, 'doughnut': 1226, 'douglas': 1227, 'douses': 1228, 'down': 1229, 'downcast': 1230, 'downstairs': 1231, 'downtown': 1232, 'dr': 1233, 'dr.': 1234, 'drag': 1235, 'drags': 1236, 'drain': 1237, 'drape': 1238, 'draped': 1239, 'drapes': 1240, 'draw': 1241, 'drawer': 1242, 'drawers': 1243, 'drawing': 1244, 'drawn': 1245, 'draws': 1246, 'dream': 1247, 'dreamed': 1248, 'dreams': 1249, 'dress': 1250, 'dressed': 1251, 'dresser': 1252, 'dressing': 1253, 'drifts': 1254, 'drink': 1255, 'drinking': 1256, 'drinks': 1257, 'drips': 1258, 'drive': 1259, 'driver': 1260, 'drivers': 1261, 'drives': 1262, 'driveway': 1263, 'drivin': 1264, 'driving': 1265, 'drop': 1266, 'dropped': 1267, 'drops': 1268, 'drove': 1269, 'drug': 1270, 'drugged': 1271, 'drugs': 1272, 'drunk': 1273, 'dry': 1274, 'dubious': 1275, 'duck': 1276, 'ducks': 1277, 'dug': 1278, 'dull': 1279, 'dullards': 1280, 'dumb': 1281, 'dumbbells': 1282, 'dumps': 1283, 'dumpster': 1284, 'dunces': 1285, 'duration': 1286, 'during': 1287, 'dusk': 1288, 'dustbuster': 1289, 'dusters': 1290, 'dusty': 1291, 'duty': 1292, 'dwells': 1293, 'dying': 1294, 'e': 1295, 'each': 1296, 'eager': 1297, 'eagle': 1298, 'ear': 1299, 'earlier': 1300, 'early': 1301, 'ears': 1302, 'earth': 1303, 'easier': 1304, 'east': 1305, 'easy': 1306, 'eat': 1307, 'eating': 1308, 'econo': 1309, 'ecstatic': 1310, 'ed': 1311, 'edge': 1312, 'edges': 1313, 'edsker': 1314, 'education': 1315, 'effect': 1316, 'efficiency': 1317, 'effortlessly': 1318, 'egg': 1319, 'eggnog': 1320, 'eggs': 1321, 'eight': 1322, 'eighteen': 1323, 'eighty': 1324, 'eileen': 1325, 'either': 1326, 'eject': 1327, 'el': 1328, 'elaborate': 1329, 'elbow': 1330, 'elderly': 1331, 'electronic': 1332, 'elegant': 1333, 'elevator': 1334, 'elk': 1335, 'else': 1336, 'elvis': 1337, 'embedded': 1338, 'embrace': 1339, 'embraces': 1340, 'emerald': 1341, 'emerge': 1342, 'emerges': 1343, 'emotional': 1344, 'emotions': 1345, 'empire': 1346, 'empties': 1347, 'empty': 1348, 'en': 1349, 'encounter': 1350, 'encourage': 1351, 'encouraging': 1352, 'end': 1353, 'endeavors': 1354, 'ends': 1355, 'endures': 1356, 'engage': 1357, 'engine': 1358, 'english': 1359, 'engrossed': 1360, 'enigma': 1361, 'enigmatically': 1362, 'enjoy': 1363, 'enjoys': 1364, 'enlisting': 1365, 'enormous': 1366, 'enough': 1367, 'enter': 1368, 'entering': 1369, 'enters': 1370, 'entertaining': 1371, 'enthusiastic': 1372, 'entire': 1373, 'entirely': 1374, 'entrance': 1375, 'entrust': 1376, 'entry': 1377, 'envelope': 1378, 'episode': 1379, 'erase': 1380, 'erupting': 1381, 'escape': 1382, 'escort': 1383, 'escorted': 1384, 'escorts': 1385, 'especially': 1386, 'establish': 1387, 'established': 1388, 'establishing': 1389, 'establishment': 1390, 'estate': 1391, 'estates': 1392, 'estimated': 1393, 'eternal': 1394, 'evacuation': 1395, 'even': 1396, 'evening': 1397, 'event': 1398, 'events': 1399, 'eventually': 1400, 'ever': 1401, 'every': 1402, 'everybody': 1403, 'everyone': 1404, 'everything': 1405, 'evidence': 1406, 'evil': 1407, 'ex': 1408, 'exacting': 1409, 'exactly': 1410, 'examine': 1411, 'examined': 1412, 'examining': 1413, 'example': 1414, 'exasperated': 1415, 'excellent': 1416, 'except': 1417, 'exception': 1418, 'exchange': 1419, 'excited': 1420, 'excitement': 1421, 'excuse': 1422, 'execute': 1423, 'exercise': 1424, 'exertion': 1425, 'exhales': 1426, 'exhausted': 1427, 'exile': 1428, 'existence': 1429, 'exit': 1430, 'exiting': 1431, 'exits': 1432, 'expand': 1433, 'expanding': 1434, 'expands': 1435, 'expect': 1436, 'expecting': 1437, 'expects': 1438, 'expensive': 1439, 'experience': 1440, 'expert': 1441, 'expertly': 1442, 'explain': 1443, 'explaining': 1444, 'expletive': 1445, 'explodes': 1446, 'exploit': 1447, 'explosion': 1448, 'express': 1449, 'expressing': 1450, 'expression': 1451, 'ext': 1452, 'extra': 1453, 'extraordinary': 1454, 'extremely': 1455, 'eye': 1456, 'eyed': 1457, 'eyeing': 1458, 'eyes': 1459, 'f': 1460, 'face': 1461, 'faced': 1462, 'faceless': 1463, 'facial': 1464, 'facilities': 1465, 'facing': 1466, 'fact': 1467, 'facts': 1468, 'fade': 1469, 'faded': 1470, 'fades': 1471, 'failing': 1472, 'failure': 1473, 'fair': 1474, 'fairvale': 1475, 'fake': 1476, 'falcon': 1477, 'fall': 1478, 'falling': 1479, 'falls': 1480, 'false': 1481, 'familiar': 1482, 'family': 1483, 'famished': 1484, 'fanfare': 1485, 'fantastic': 1486, 'far': 1487, 'faraway': 1488, 'farm': 1489, 'fascinated': 1490, 'fascinating': 1491, 'fashioned': 1492, 'fast': 1493, 'fastened': 1494, 'fate': 1495, 'father': 1496, 'favorite': 1497, 'favors': 1498, 'fax': 1499, 'faxing': 1500, 'fbi': 1501, 'fear': 1502, 'fears': 1503, 'february': 1504, 'federal': 1505, 'feed': 1506, 'feel': 1507, 'feeling': 1508, 'feelings': 1509, 'feels': 1510, 'feet': 1511, 'feigned': 1512, 'fell': 1513, 'fella': 1514, 'fellas': 1515, 'fellow': 1516, 'felonies': 1517, 'felt': 1518, 'female': 1519, 'ferguson': 1520, 'fervor': 1521, 'few': 1522, 'fi': 1523, 'fibers': 1524, 'fielding': 1525, 'fifteen': 1526, 'fifth': 1527, 'fifties': 1528, 'fifty': 1529, 'fight': 1530, 'fighting': 1531, 'fights': 1532, 'figure': 1533, 'figured': 1534, 'figures': 1535, 'figuring': 1536, 'file': 1537, 'filed': 1538, 'files': 1539, 'filing': 1540, 'fill': 1541, 'filled': 1542, 'filling': 1543, 'filters': 1544, 'filthy': 1545, 'final': 1546, 'finally': 1547, 'find': 1548, 'finds': 1549, 'fine': 1550, 'finger': 1551, 'fingers': 1552, 'finish': 1553, 'finished': 1554, 'finishes': 1555, 'finishing': 1556, 'finley': 1557, 'fir': 1558, 'fire': 1559, 'firearms': 1560, 'fireplace': 1561, 'fires': 1562, 'firing': 1563, 'firm': 1564, 'firmly': 1565, 'firmness': 1566, 'first': 1567, 'fish': 1568, 'fishin': 1569, 'fishing': 1570, 'fists': 1571, 'fitfully': 1572, 'fitting': 1573, 'fitzgerald': 1574, 'five': 1575, 'fix': 1576, 'flak': 1577, 'flannel': 1578, 'flapping': 1579, 'flash': 1580, 'flashing': 1581, 'flashlight': 1582, 'flashy': 1583, 'flask': 1584, 'flat': 1585, 'flee': 1586, 'flies': 1587, 'flight': 1588, 'flings': 1589, 'flips': 1590, 'floor': 1591, 'floss': 1592, 'flourish': 1593, 'flower': 1594, 'flowered': 1595, 'flowers': 1596, 'flowing': 1597, 'flu': 1598, 'fluffy': 1599, 'fluorescents': 1600, 'flurry': 1601, 'flushed': 1602, 'flustered': 1603, 'fly': 1604, 'flying': 1605, 'focuses': 1606, 'focusing': 1607, 'folded': 1608, 'folder': 1609, 'folding': 1610, 'folds': 1611, 'follow': 1612, 'followed': 1613, 'following': 1614, 'follows': 1615, 'food': 1616, 'fool': 1617, 'fooling': 1618, 'fools': 1619, 'foot': 1620, 'football': 1621, 'footsteps': 1622, 'for': 1623, 'force': 1624, 'forced': 1625, 'forebrain': 1626, 'forehead': 1627, 'forensics': 1628, 'forest': 1629, 'forever': 1630, 'forget': 1631, 'forgive': 1632, 'forgot': 1633, 'forgotten': 1634, 'fork': 1635, 'forlorn': 1636, 'form': 1637, 'formaldehyde': 1638, 'forms': 1639, 'forth': 1640, 'fortress': 1641, 'fortune': 1642, 'forty': 1643, 'forward': 1644, 'forwards': 1645, 'found': 1646, 'four': 1647, 'foursome': 1648, 'fragment': 1649, 'frailty': 1650, 'frame': 1651, 'framed': 1652, 'francisco': 1653, 'frantically': 1654, 'frappe': 1655, 'fraud': 1656, 'fray': 1657, 'free': 1658, 'freeze': 1659, 'french': 1660, 'fresh': 1661, 'freshly': 1662, 'friday': 1663, 'fried': 1664, 'friend': 1665, 'friendly': 1666, 'friends': 1667, 'frightened': 1668, 'frightening': 1669, 'frog': 1670, 'from': 1671, 'front': 1672, 'frowns': 1673, 'frozen': 1674, 'fruit': 1675, 'fruits': 1676, 'frustrated': 1677, 'fulfill': 1678, 'full': 1679, 'fully': 1680, 'fumbles': 1681, 'fun': 1682, 'fund': 1683, 'funeral': 1684, 'funerals': 1685, 'funny': 1686, 'furious': 1687, 'furiously': 1688, 'furnished': 1689, 'further': 1690, 'fury': 1691, 'future': 1692, 'futures': 1693, 'gag': 1694, 'gained': 1695, 'gaining': 1696, 'gal': 1697, 'gallon': 1698, 'galvanized': 1699, 'games': 1700, 'gamut': 1701, 'gang': 1702, 'garage': 1703, 'gas': 1704, 'gasps': 1705, 'gassed': 1706, 'gate': 1707, 'gates': 1708, 'gather': 1709, 'gathered': 1710, 'gathering': 1711, 'gave': 1712, 'gaze': 1713, 'gazing': 1714, 'gear': 1715, 'gee': 1716, 'geez': 1717, 'generic': 1718, 'genial': 1719, 'genius': 1720, 'gent': 1721, 'gentle': 1722, 'gentlemen': 1723, 'gently': 1724, 'genus': 1725, 'gerard': 1726, 'gesture': 1727, 'gestures': 1728, 'get': 1729, 'gets': 1730, 'getting': 1731, 'ghostwood': 1732, 'giant': 1733, 'gifted': 1734, 'giggle': 1735, 'giggles': 1736, 'gimme': 1737, 'ginger': 1738, 'girl': 1739, 'girlfriend': 1740, 'girls': 1741, 'give': 1742, 'gives': 1743, 'givin': 1744, 'giving': 1745, 'glad': 1746, 'gladly': 1747, 'glance': 1748, 'glances': 1749, 'glancing': 1750, 'glare': 1751, 'glares': 1752, 'glass': 1753, 'glasses': 1754, 'glassine': 1755, 'gleaming': 1756, 'gleefully': 1757, 'glides': 1758, 'glimpse': 1759, 'glistening': 1760, 'gloats': 1761, 'globe': 1762, 'glove': 1763, 'gloved': 1764, 'gloves': 1765, 'glowers': 1766, 'glowing': 1767, 'glue': 1768, 'glued': 1769, 'glumly': 1770, 'go': 1771, 'god': 1772, 'god--': 1773, 'godforsaken': 1774, 'godspeed': 1775, 'goes': 1776, \"goin'\": 1777, 'going': 1778, 'gold': 1779, 'golden': 1780, 'golf': 1781, 'gon': 1782, 'gone': 1783, 'good': 1784, 'goodbye': 1785, 'goodnight': 1786, 'gordon': 1787, 'gorgeous': 1788, 'gossip': 1789, 'got': 1790, 'gotten': 1791, 'gown': 1792, 'grab': 1793, 'grabbing': 1794, 'grabs': 1795, 'grade': 1796, 'gradually': 1797, 'graduate': 1798, 'grand': 1799, 'granite': 1800, 'granted': 1801, 'grapefruit': 1802, 'grapefruits': 1803, 'grasp': 1804, 'grass': 1805, 'grave': 1806, 'graves': 1807, 'gravesite': 1808, 'grease': 1809, 'greased': 1810, 'greasy': 1811, 'great': 1812, 'greater': 1813, 'greatly': 1814, 'green': 1815, 'greets': 1816, 'grew': 1817, 'grey': 1818, 'griddlecake': 1819, 'griddlecakes': 1820, 'grief': 1821, 'grime': 1822, 'grin': 1823, 'grinds': 1824, 'grins': 1825, 'grips': 1826, 'groaning': 1827, 'groceries': 1828, 'grocery': 1829, 'ground': 1830, 'grounds': 1831, 'group': 1832, \"grow'st\": 1833, 'growing': 1834, 'growl': 1835, 'grows': 1836, 'guaranteed': 1837, 'guarantees': 1838, 'guard': 1839, 'guess': 1840, 'guesses': 1841, 'guests': 1842, 'gum': 1843, 'gums': 1844, 'gun': 1845, 'gunplay': 1846, 'guns': 1847, 'gust': 1848, 'gutter': 1849, 'guy': 1850, 'guys': 1851, 'h': 1852, 'ha': 1853, 'habit': 1854, 'habits': 1855, 'had': 1856, 'hail': 1857, 'hair': 1858, 'haircut': 1859, 'hairdo': 1860, 'haired': 1861, 'half': 1862, 'hall': 1863, 'hallway': 1864, 'ham': 1865, 'hammer': 1866, 'hand': 1867, 'handcarved': 1868, 'handed': 1869, 'handfull': 1870, 'handgun': 1871, 'handicapped': 1872, 'handle': 1873, 'hands': 1874, 'handsome': 1875, 'handwriting': 1876, 'hang': 1877, 'hanging': 1878, 'hangs': 1879, 'hank': 1880, 'happen': 1881, 'happened': 1882, 'happens': 1883, 'happily': 1884, 'happy': 1885, 'hard': 1886, 'harder': 1887, 'hardly': 1888, 'hardware': 1889, 'harley': 1890, 'harp': 1891, 'harpie': 1892, 'harry': 1893, 'has': 1894, 'hath': 1895, 'hats': 1896, 'haunting': 1897, 'have': 1898, 'having': 1899, 'hawaii': 1900, 'hawaiian': 1901, 'hawaiians': 1902, 'hawk': 1903, 'hay': 1904, 'hayseed': 1905, 'hayward': 1906, 'hazy': 1907, 'he': 1908, 'head': 1909, 'headdress': 1910, 'headed': 1911, 'headlights': 1912, 'headline': 1913, 'headphone': 1914, 'headphones': 1915, 'headress': 1916, 'heads': 1917, 'headset': 1918, 'headstone': 1919, 'headstones': 1920, 'headstrong': 1921, 'healing': 1922, 'health': 1923, 'hear': 1924, 'heard': 1925, 'hearing': 1926, 'hears': 1927, 'hearse': 1928, 'heart': 1929, 'hearted': 1930, 'hearty': 1931, 'heat': 1932, 'heated': 1933, 'heaven': 1934, 'heavenly': 1935, 'heavily': 1936, 'heaviness': 1937, 'heavy': 1938, 'heck': 1939, 'hefts': 1940, 'heidi': 1941, 'held': 1942, 'hell': 1943, 'hello': 1944, 'help': 1945, 'helped': 1946, 'helpful': 1947, 'helpless': 1948, 'helps': 1949, 'henry': 1950, 'her': 1951, 'here': 1952, 'herself': 1953, 'hesitation': 1954, 'hey': 1955, 'hi': 1956, 'hidden': 1957, 'hide': 1958, 'hides': 1959, 'hiding': 1960, 'hifi': 1961, 'high': 1962, 'highball': 1963, 'highlights': 1964, 'highway': 1965, 'hilarious': 1966, 'hill': 1967, 'hillbilly': 1968, 'hills': 1969, 'him': 1970, 'himself': 1971, 'hinge': 1972, 'hint': 1973, 'hired': 1974, 'hiring': 1975, 'hirsute': 1976, 'his': 1977, 'hit': 1978, 'hither': 1979, 'hits': 1980, 'hitting': 1981, 'hmm': 1982, 'hold': 1983, 'holding': 1984, 'holds': 1985, 'hole': 1986, 'hollow': 1987, 'holy': 1988, 'home': 1989, 'homecoming': 1990, 'homicidally': 1991, 'honest': 1992, 'honestly': 1993, 'honey': 1994, 'hong': 1995, 'honking': 1996, 'honky': 1997, 'honor': 1998, 'hood': 1999, 'hook': 2000, 'hoots': 2001, 'hop': 2002, 'hope': 2003, 'hopeless': 2004, 'hoping': 2005, 'hops': 2006, 'horn': 2007, 'horne': 2008, 'horns': 2009, 'horrible': 2010, 'horror': 2011, 'horse': 2012, 'hose': 2013, 'hospital': 2014, 'hot': 2015, 'hotel': 2016, 'houlies': 2017, 'hour': 2018, 'hours': 2019, 'house': 2020, 'household': 2021, 'houses': 2022, 'housetrailer': 2023, 'hovering': 2024, 'how': 2025, \"how'm\": 2026, 'however': 2027, 'howl': 2028, 'howling': 2029, 'howls': 2030, 'howsoever': 2031, 'hubcap': 2032, 'huckleberry': 2033, 'hug': 2034, 'huge': 2035, 'hugs': 2036, 'huh': 2037, 'hula': 2038, 'hulking': 2039, 'human': 2040, 'humdrum': 2041, 'humidity': 2042, 'humongous': 2043, 'hums': 2044, 'hundred': 2045, 'hungry': 2046, 'hunting': 2047, 'hurley': 2048, 'hurling': 2049, 'hurls': 2050, 'hurries': 2051, 'hurry': 2052, 'hurt': 2053, 'hurts': 2054, 'husband': 2055, 'hushed': 2056, 'hustle': 2057, 'hydraulic': 2058, 'hydraulics': 2059, 'hymnals': 2060, 'hypnotically': 2061, 'hypocrites': 2062, 'i': 2063, \"i'll\": 2064, \"i'm\": 2065, \"i've\": 2066, 'i-(loses': 2067, 'ice': 2068, 'iceland': 2069, 'icelander': 2070, 'icelandic': 2071, 'icelandics': 2072, 'id': 2073, 'idea': 2074, 'ideal': 2075, 'identical': 2076, 'identified': 2077, 'identity': 2078, 'idiots': 2079, 'idly': 2080, 'if': 2081, 'ignores': 2082, 'ill': 2083, 'illegal': 2084, 'illuminates': 2085, 'illumined': 2086, 'imaginable': 2087, 'imbroglio': 2088, 'immediately': 2089, 'impact': 2090, 'impatient': 2091, 'impatiently': 2092, 'implication': 2093, 'important': 2094, 'impossibly': 2095, 'impression': 2096, 'improve': 2097, 'impulses': 2098, 'in': 2099, 'incarcerated': 2100, 'incarnate': 2101, 'inches': 2102, 'included': 2103, 'including': 2104, 'incorrectly': 2105, 'incredible': 2106, 'indeed': 2107, 'india': 2108, 'indian': 2109, 'indicated': 2110, 'indicates': 2111, 'indifference': 2112, 'indifferently': 2113, 'indoctrination': 2114, 'industrial': 2115, 'ineffectual': 2116, 'infant': 2117, 'infirm': 2118, 'inflicted': 2119, 'influence': 2120, 'information': 2121, 'infra': 2122, 'inhales': 2123, 'inhand': 2124, 'inherent': 2125, 'initials': 2126, 'injured': 2127, 'inmate': 2128, 'inn': 2129, 'innocence': 2130, 'inquest': 2131, 'inquiry': 2132, 'ins': 2133, 'insane': 2134, 'insert': 2135, 'inserted': 2136, 'inserts': 2137, 'inside': 2138, 'insincerely': 2139, 'insincerity': 2140, 'insist': 2141, 'insolent': 2142, 'inspects': 2143, 'inspiration': 2144, 'installed': 2145, 'instance': 2146, 'instantly': 2147, 'instead': 2148, 'instinct': 2149, 'instinctively': 2150, 'institutional': 2151, 'instructed': 2152, 'insult': 2153, 'insults': 2154, 'insurance': 2155, 'int': 2156, 'intelligence': 2157, 'intelligent': 2158, 'intelligible': 2159, 'intends': 2160, 'intensive': 2161, 'intently': 2162, 'interact': 2163, 'intercepts': 2164, 'intercom': 2165, 'intercut': 2166, 'interest': 2167, 'interested': 2168, 'interesting': 2169, 'interests': 2170, 'international': 2171, 'interrogate': 2172, 'interrogation': 2173, 'interrupted': 2174, 'interrupts': 2175, 'intervenes': 2176, 'interview': 2177, 'intimately': 2178, 'into': 2179, 'intoning': 2180, 'intrigue': 2181, 'intrigued': 2182, 'intuition': 2183, 'invaded': 2184, 'inventing': 2185, 'invention': 2186, 'investigation': 2187, 'investigative': 2188, 'investigator': 2189, 'investment': 2190, 'invisible': 2191, 'invitation': 2192, 'involuntarily': 2193, 'involving': 2194, 'iron': 2195, 'irritating': 2196, 'is': 2197, 'issue': 2198, 'it': 2199, 'it\"-cole': 2200, 'it--': 2201, 'itch': 2202, 'item': 2203, 'its': 2204, 'j': 2205, 'j.': 2206, 'jack': 2207, 'jack\\'s\"--': 2208, 'jacket': 2209, 'jacks': 2210, 'jacoby': 2211, 'jacques': 2212, 'jade': 2213, 'jaguar': 2214, 'jail': 2215, 'james': 2216, 'james--': 2217, 'jamin': 2218, 'janek': 2219, 'janitor': 2220, 'jar': 2221, 'jared': 2222, 'jaundiced': 2223, 'jaw': 2224, 'jazz': 2225, 'jealous': 2226, 'jeans': 2227, 'jelly': 2228, 'jennings': 2229, 'jer': 2230, 'jerry': 2231, 'jesus': 2232, 'jet': 2233, 'jfk': 2234, 'jim': 2235, 'jimmies': 2236, 'jimmy': 2237, 'jitterbugging': 2238, 'job': 2239, 'jocelyn-(covering': 2240, 'joe': 2241, 'joey': 2242, 'jogged': 2243, 'jogging': 2244, 'johnnie': 2245, 'johnny': 2246, 'johnnys': 2247, 'johnson': 2248, 'join': 2249, 'joining': 2250, 'joke': 2251, 'joker': 2252, 'josie': 2253, 'jostled': 2254, 'jots': 2255, 'joy': 2256, 'juice': 2257, 'juke': 2258, 'jukebox': 2259, 'julie': 2260, 'jump': 2261, 'jumped': 2262, 'jumps': 2263, 'jurisdiction': 2264, 'just': 2265, 'k': 2266, 'kashmir': 2267, 'keep': 2268, 'keeping': 2269, 'keeps': 2270, 'kennedys': 2271, 'key': 2272, 'keyhole': 2273, 'keys': 2274, 'kicked': 2275, 'kicks': 2276, 'kid': 2277, 'kidding': 2278, 'kiddo': 2279, 'kids': 2280, 'kill': 2281, 'killed': 2282, 'killer': 2283, 'killing': 2284, 'kills': 2285, 'kind': 2286, \"kind'a\": 2287, 'kinda': 2288, 'kinds': 2289, 'kingdom': 2290, 'kingly': 2291, 'kiss': 2292, 'kissed': 2293, 'kisses': 2294, 'kit': 2295, 'kitchen': 2296, 'knee': 2297, 'kneel': 2298, 'kneels': 2299, 'knees': 2300, 'knew': 2301, 'knife': 2302, 'knob': 2303, 'knock': 2304, 'knocked': 2305, 'knockouts': 2306, 'knocks': 2307, 'knot': 2308, 'knothead': 2309, 'knots': 2310, 'know': 2311, 'knowing': 2312, 'knowledge': 2313, 'known': 2314, 'knows': 2315, 'knuckle': 2316, 'knuckled': 2317, 'knuckles': 2318, 'kong': 2319, 'koro': 2320, 'l': 2321, 'l.j.': 2322, 'lab': 2323, 'labeled': 2324, 'labels': 2325, 'labors': 2326, 'lack': 2327, 'lacking': 2328, 'lacquer': 2329, 'lady': 2330, 'lagging': 2331, 'laid': 2332, 'lair': 2333, 'lake': 2334, 'lama': 2335, 'lamplighter': 2336, 'land': 2337, 'language': 2338, 'lapels': 2339, 'large': 2340, 'larger': 2341, 'lariat': 2342, 'lasagne': 2343, 'lash': 2344, 'last': 2345, 'lastly': 2346, 'latch': 2347, 'late': 2348, 'later': 2349, 'lathers': 2350, 'laugh': 2351, 'laughed': 2352, 'laughing': 2353, 'laughs': 2354, 'laundry': 2355, 'laura': 2356, 'lawman': 2357, 'lawrence': 2358, 'lawyer': 2359, 'lay': 2360, 'lead': 2361, 'leader': 2362, 'leading': 2363, 'leads': 2364, 'leak': 2365, 'lean': 2366, 'leaned': 2367, 'leaning': 2368, 'leans': 2369, 'leap': 2370, 'leaping': 2371, 'leaps': 2372, 'learn': 2373, 'learned': 2374, 'learning': 2375, 'lease': 2376, 'least': 2377, 'leather': 2378, 'leave': 2379, 'leaves': 2380, 'leaving': 2381, 'led': 2382, 'ledger': 2383, 'lee': 2384, 'leering': 2385, 'left': 2386, 'legal': 2387, 'legend': 2388, 'leland': 2389, 'length': 2390, 'lengthy': 2391, 'lens': 2392, 'leo': 2393, 'leonard': 2394, 'lesions': 2395, 'less': 2396, 'lesson': 2397, 'let': 2398, 'lets': 2399, 'letter': 2400, 'letters': 2401, 'level': 2402, 'levels': 2403, 'lewis': 2404, 'licence': 2405, 'license': 2406, 'lie': 2407, 'lied': 2408, 'lies': 2409, 'life': 2410, 'lift': 2411, 'lifts': 2412, 'light': 2413, 'lighter': 2414, 'lightheaded': 2415, 'lightly': 2416, 'lights': 2417, 'like': 2418, 'liked': 2419, 'likin': 2420, 'liking': 2421, 'limit': 2422, 'limits': 2423, 'limousine': 2424, 'linchpin': 2425, 'lindy': 2426, 'line': 2427, 'lined': 2428, 'linen': 2429, 'lines': 2430, 'lion': 2431, 'lip': 2432, 'lips': 2433, 'liquor': 2434, 'list': 2435, 'listen': 2436, 'listening': 2437, 'listens': 2438, 'listlessly': 2439, 'lit': 2440, 'little': 2441, 'live': 2442, 'lived': 2443, 'lives': 2444, 'liveth': 2445, 'living': 2446, 'load': 2447, 'loaded': 2448, 'loaf': 2449, 'lobby': 2450, 'local': 2451, 'locates': 2452, 'locations': 2453, 'lockbox': 2454, 'locked': 2455, 'locker': 2456, 'lodge': 2457, 'log': 2458, 'logic': 2459, 'logtown': 2460, 'lone': 2461, 'loneliness': 2462, 'lonely': 2463, 'long': 2464, 'longer': 2465, 'longneck': 2466, 'longs': 2467, 'look': 2468, 'looked': 2469, 'lookin': 2470, 'looking': 2471, 'looks': 2472, 'loomer': 2473, 'loose': 2474, 'loosely': 2475, 'lord': 2476, 'lore': 2477, 'lose': 2478, 'losing': 2479, 'loss': 2480, 'losses': 2481, 'lost': 2482, 'lot': 2483, 'lotown': 2484, 'lots': 2485, 'lotus': 2486, 'loud': 2487, 'louis': 2488, 'lounge': 2489, 'lounging': 2490, 'love': 2491, 'loved': 2492, 'lovely': 2493, 'loves': 2494, 'loving': 2495, 'low': 2496, 'lower': 2497, 'lowering': 2498, 'lowers': 2499, 'lowtown': 2500, 'luck': 2501, 'lucky': 2502, 'lucy': 2503, 'lug': 2504, 'lumber': 2505, 'lummox': 2506, 'lumps': 2507, 'lunch': 2508, 'lurches': 2509, 'luscious': 2510, 'lush': 2511, 'lust': 2512, 'luxuriously': 2513, 'lydecker': 2514, 'lydeker': 2515, 'lying': 2516, 'm': 2517, 'ma': 2518, \"ma'am\": 2519, 'machine': 2520, 'mad': 2521, 'madam': 2522, 'maddy': 2523, 'made': 2524, 'madelaine': 2525, 'madeleine': 2526, 'madeline': 2527, 'madly': 2528, 'magic': 2529, 'magician': 2530, 'mahogany': 2531, 'maid': 2532, 'mail': 2533, 'maintain': 2534, 'major': 2535, 'make': 2536, 'maker': 2537, 'makes': 2538, 'makeup': 2539, 'making': 2540, 'male': 2541, 'malted': 2542, 'man': 2543, 'manages': 2544, 'mandarin': 2545, 'manic': 2546, 'manicure': 2547, 'manilla': 2548, 'manipulates': 2549, 'mann': 2550, 'manner': 2551, 'mannered': 2552, 'mannerless': 2553, 'mano': 2554, 'manslaughter': 2555, 'manufacture': 2556, 'many': 2557, 'map': 2558, 'maple': 2559, 'mapped': 2560, 'marches': 2561, 'marilyn': 2562, 'marked': 2563, 'marks': 2564, 'marlon': 2565, 'maroon': 2566, 'married': 2567, 'martell': 2568, 'mashes': 2569, 'masse': 2570, 'mast': 2571, 'match': 2572, 'matter': 2573, 'mattress': 2574, 'may': 2575, 'maybe': 2576, 'mayday': 2577, 'mayo': 2578, 'me': 2579, 'meals': 2580, 'mean': 2581, 'meaning': 2582, 'meaningful': 2583, 'means': 2584, 'meant': 2585, 'meantime': 2586, 'measure': 2587, 'meatloaf': 2588, 'medication': 2589, 'medicine': 2590, 'meet': 2591, 'meeting': 2592, 'meets': 2593, 'megaphone': 2594, 'melange': 2595, 'melanie': 2596, 'melody': 2597, 'melts': 2598, 'member': 2599, 'members': 2600, 'memorial': 2601, 'memories': 2602, 'memory': 2603, 'memphis': 2604, 'men': 2605, 'mention': 2606, 'menu': 2607, 'merciful': 2608, 'merry': 2609, 'mess': 2610, 'message': 2611, 'messages': 2612, 'messed': 2613, 'messy': 2614, 'met': 2615, 'metabolism': 2616, 'metal': 2617, 'method': 2618, 'michael': 2619, 'mid': 2620, 'mid-': 2621, 'middle': 2622, 'middleman': 2623, 'midge': 2624, 'midget': 2625, 'midnight': 2626, 'midst': 2627, 'midthirties': 2628, 'might': 2629, 'mike': 2630, \"mike're\": 2631, 'mild': 2632, 'mildew': 2633, 'mile': 2634, 'miles': 2635, 'milford': 2636, 'milkballs': 2637, 'mill': 2638, 'million': 2639, 'mind': 2640, 'mindbody': 2641, 'mindedly': 2642, 'mindless': 2643, 'mine': 2644, 'mingle': 2645, 'miniature': 2646, 'minimum': 2647, 'mink': 2648, 'minor': 2649, 'minute': 2650, 'minutes': 2651, 'mirror': 2652, 'mischief': 2653, 'misguided': 2654, 'mismatched': 2655, 'miss': 2656, 'missed': 2657, 'misses': 2658, 'missoula': 2659, 'mistake': 2660, 'mister': 2661, 'mittens': 2662, 'mixed': 2663, 'moan': 2664, 'moaning': 2665, 'moans': 2666, 'mocking': 2667, 'model': 2668, 'modern': 2669, 'modifications': 2670, 'mom': 2671, 'moment': 2672, 'moments': 2673, 'momentum': 2674, 'monday': 2675, 'money': 2676, 'monitor': 2677, 'monroe': 2678, 'monstrous': 2679, 'montana': 2680, 'month': 2681, 'months': 2682, 'mood': 2683, 'moon': 2684, 'mooney': 2685, 'moonless': 2686, 'moran': 2687, 'more': 2688, 'morgue': 2689, 'morn': 2690, 'morning': 2691, 'morons': 2692, 'mortally': 2693, 'mortician': 2694, 'most': 2695, 'mostly': 2696, 'motel': 2697, 'mother': 2698, 'motherly': 2699, 'motions': 2700, 'motorcycle': 2701, 'mound': 2702, 'mountain': 2703, 'mountains': 2704, 'mourners': 2705, 'mouse': 2706, 'mouth': 2707, 'mouthful': 2708, 'mouthing': 2709, 'mouthpiece': 2710, 'mouths': 2711, 'move': 2712, 'moved': 2713, 'moves': 2714, 'moving': 2715, 'mr.': 2716, 'mrs': 2717, 'mrs.': 2718, 'ms.': 2719, 'much': 2720, 'mucho': 2721, 'mug': 2722, 'mugs': 2723, 'mule': 2724, 'murder': 2725, 'murdered': 2726, 'muse': 2727, 'mushrooms': 2728, 'music': 2729, 'must': 2730, 'mute': 2731, 'muted': 2732, 'mutters': 2733, 'my': 2734, 'mynah': 2735, 'myself': 2736, 'mystery': 2737, 'mystified': 2738, 'n': 2739, \"n't\": 2740, 'na': 2741, 'nadine': 2742, \"nadine'd\": 2743, 'nail': 2744, 'name': 2745, 'named': 2746, 'names': 2747, 'napkin': 2748, 'narrows': 2749, 'native': 2750, 'naturally': 2751, 'nature': 2752, 'naturedly': 2753, 'naughty': 2754, 'nautically': 2755, 'near': 2756, 'nearby': 2757, 'nearly': 2758, 'neatnik': 2759, 'necessarily': 2760, 'necessary': 2761, 'neck': 2762, 'necklace': 2763, 'neckline': 2764, 'need': 2765, 'needed': 2766, 'needle': 2767, 'needlepoint': 2768, 'needles': 2769, 'needless': 2770, 'needs': 2771, 'negative': 2772, 'negligee': 2773, 'neither': 2774, 'nelson': 2775, 'neon': 2776, 'nepal': 2777, 'nephew': 2778, 'nervous': 2779, 'nervously': 2780, 'neurons': 2781, 'never': 2782, 'new': 2783, 'newborn': 2784, 'newly': 2785, 'news': 2786, 'newspaper': 2787, 'next': 2788, 'nice': 2789, 'niceties': 2790, 'niece': 2791, 'night': 2792, 'nighter': 2793, 'nightfall': 2794, 'nightgown': 2795, 'nightmare': 2796, 'nights': 2797, 'nine': 2798, 'no': 2799, 'nobody': 2800, 'nod': 2801, 'nods': 2802, 'noise': 2803, 'noiseless': 2804, 'noises': 2805, 'nominally': 2806, 'none': 2807, 'nonsense': 2808, 'nonsmoking': 2809, 'noon': 2810, 'nope': 2811, 'nor': 2812, 'norma': 2813, \"norma'll\": 2814, 'normal': 2815, 'normally': 2816, 'north': 2817, 'northern': 2818, 'northwest': 2819, 'norwegian': 2820, 'norweigans': 2821, 'nose': 2822, 'nostalgic': 2823, 'nostril': 2824, 'nostrils': 2825, 'not': 2826, 'note': 2827, 'notebook': 2828, 'notes': 2829, \"nothin'\": 2830, 'nothing': 2831, 'notice': 2832, 'noticed': 2833, 'notices': 2834, 'notion': 2835, 'now': 2836, 'nowhere': 2837, 'number': 2838, 'numbers': 2839, 'numerous': 2840, 'nurse': 2841, 'nursing': 2842, 'nuts': 2843, 'o': 2844, \"o'clock\": 2845, \"o'reilly\": 2846, 'o.': 2847, 'o.o.j.': 2848, 'oars': 2849, 'object': 2850, 'obligated': 2851, 'oblivious': 2852, 'obscured': 2853, 'obsessively': 2854, 'obstructing': 2855, 'obvious': 2856, 'obviously': 2857, 'occasion': 2858, 'occupy': 2859, 'odd': 2860, 'oddly': 2861, 'odysseus': 2862, 'of': 2863, 'off': 2864, 'offend': 2865, 'offer': 2866, 'offering': 2867, 'offers': 2868, 'office': 2869, 'officer': 2870, 'offscreen': 2871, 'often': 2872, 'og': 2873, 'oh': 2874, 'ohio': 2875, 'oil': 2876, 'okay': 2877, 'old': 2878, 'older': 2879, 'oldest': 2880, 'on': 2881, 'once': 2882, 'one': 2883, 'onearmed': 2884, 'ones': 2885, 'ongoing': 2886, 'only': 2887, 'onto': 2888, 'ooper': 2889, 'open': 2890, 'opened': 2891, 'opening': 2892, 'opens': 2893, 'opera': 2894, 'operatic': 2895, 'operating': 2896, 'operation': 2897, 'operator': 2898, 'opinion': 2899, 'opportunities': 2900, 'opportunity': 2901, 'opposite': 2902, 'options': 2903, 'or': 2904, 'oration': 2905, 'order': 2906, 'ordering': 2907, 'orders': 2908, 'ordinance': 2909, 'ordinary': 2910, 'organize': 2911, 'organized': 2912, 'orgasmic': 2913, 'originating': 2914, 'ornament': 2915, 'orthopedic': 2916, 'other': 2917, 'others': 2918, 'otter': 2919, 'ought': 2920, \"ought'a\": 2921, 'ounce': 2922, 'our': 2923, 'ourselves': 2924, 'out': 2925, 'outfit': 2926, 'outside': 2927, 'outstanding': 2928, 'over': 2929, 'overall': 2930, 'overcome': 2931, 'overhead': 2932, 'overheads': 2933, \"ow'st\": 2934, 'owe': 2935, 'owl': 2936, 'own': 2937, 'owned': 2938, 'owner': 2939, 'owns': 2940, 'p': 2941, 'pace': 2942, 'paces': 2943, 'package': 2944, 'packard': 2945, 'packet': 2946, 'pact': 2947, 'pad': 2948, 'paddle': 2949, 'page': 2950, 'paid': 2951, 'pain': 2952, 'pains': 2953, 'paint': 2954, 'pair': 2955, 'pajamas': 2956, 'pal': 2957, 'pale': 2958, 'palm': 2959, 'palmer': 2960, 'palmers': 2961, 'palms': 2962, 'pan': 2963, 'pancakes': 2964, 'panel': 2965, 'panne': 2966, 'pans': 2967, 'pantomime': 2968, 'pants': 2969, 'paper': 2970, 'papers': 2971, 'paradox': 2972, 'parakeet': 2973, 'parent': 2974, 'parents': 2975, 'paris': 2976, 'park': 2977, 'parked': 2978, 'parking': 2979, 'parole': 2980, 'parrot': 2981, 'part': 2982, 'partially': 2983, 'particles': 2984, 'particular': 2985, 'parting': 2986, 'partition': 2987, 'partner': 2988, 'parts': 2989, 'paso': 2990, 'pass': 2991, 'passed': 2992, 'passes': 2993, 'passing': 2994, 'passionate': 2995, 'passionately': 2996, 'past': 2997, 'pasting': 2998, 'patch': 2999, 'patches': 3000, 'patent': 3001, 'path': 3002, 'pathologist': 3003, 'patience': 3004, 'patient': 3005, 'patio': 3006, 'patrol': 3007, 'pats': 3008, 'patterns': 3009, 'paulson': 3010, 'pause': 3011, 'pauses': 3012, 'pay': 3013, 'peabrain': 3014, 'peace': 3015, 'peak': 3016, 'peaks': 3017, 'peanuts': 3018, 'pebble': 3019, 'pecking': 3020, 'peek': 3021, 'peeks': 3022, 'peering': 3023, 'peers': 3024, 'pen': 3025, 'pencil': 3026, 'penetrate': 3027, 'peninsula': 3028, 'pension': 3029, 'pensively': 3030, 'people': 3031, 'percent': 3032, 'perched': 3033, 'percolator': 3034, 'perfect': 3035, 'perfectly': 3036, 'perform': 3037, 'performance': 3038, 'performed': 3039, 'perfume': 3040, 'perhaps': 3041, 'perish': 3042, 'permission': 3043, 'perpetrator': 3044, 'perpetrators': 3045, 'perplexed': 3046, 'person': 3047, 'personal': 3048, 'persuaded': 3049, 'pertain': 3050, 'pertaining': 3051, 'pet': 3052, 'pete': 3053, 'pharmaceuticals': 3054, 'phd': 3055, 'philadelphia': 3056, 'phillip': 3057, 'phone': 3058, 'phones': 3059, 'photo': 3060, 'photograph': 3061, 'physical': 3062, 'physically': 3063, 'pick': 3064, 'picked': 3065, 'picking': 3066, 'picks': 3067, 'picnic': 3068, 'picture': 3069, 'pictures': 3070, 'pie': 3071, 'piece': 3072, 'pieces': 3073, 'pierce': 3074, 'pies': 3075, 'pile': 3076, 'pilgrimage': 3077, 'piling': 3078, 'pill': 3079, 'pilot': 3080, 'pimentos': 3081, 'pinches': 3082, 'pine': 3083, 'pines': 3084, 'piping': 3085, 'pistol': 3086, 'pitch': 3087, 'pitcher': 3088, 'place': 3089, 'placed': 3090, 'places': 3091, 'placid': 3092, 'placing': 3093, 'plain': 3094, 'plan': 3095, 'plane': 3096, 'planning': 3097, 'plans': 3098, 'plant': 3099, 'plastic': 3100, 'plate': 3101, 'plates': 3102, 'platform': 3103, 'platter': 3104, 'play': 3105, 'player': 3106, 'playfully': 3107, 'playing': 3108, 'plays': 3109, 'pleading': 3110, 'pleadings': 3111, 'pleads': 3112, 'pleasant': 3113, 'pleasantly': 3114, 'please': 3115, 'pleased': 3116, 'pleasure': 3117, 'plenty': 3118, 'plight': 3119, 'plops': 3120, 'plowing': 3121, 'plugs': 3122, 'plum': 3123, 'plunging': 3124, 'plus': 3125, 'plush': 3126, 'pm': 3127, 'pocket': 3128, 'pockets': 3129, 'poem': 3130, 'poet': 3131, 'point': 3132, 'pointed': 3133, 'pointer': 3134, 'points': 3135, 'poker': 3136, 'pole': 3137, 'police': 3138, 'polishes': 3139, 'polishing': 3140, 'pond': 3141, 'ponder': 3142, 'ponderosa': 3143, 'ponders': 3144, 'pony': 3145, 'poodle': 3146, 'pool': 3147, 'poor': 3148, 'pop': 3149, 'popped': 3150, 'pops': 3151, 'porch': 3152, 'porcupine': 3153, 'portable': 3154, 'portrait': 3155, 'position': 3156, 'positive': 3157, 'possession': 3158, 'possible': 3159, 'possibly': 3160, 'posssess': 3161, 'post': 3162, 'postgraduate': 3163, 'postmortem': 3164, 'pot': 3165, 'potato': 3166, 'pound': 3167, 'pour': 3168, 'poured': 3169, 'pouring': 3170, 'pours': 3171, 'pouts': 3172, 'pov': 3173, 'powder': 3174, 'power': 3175, 'practically': 3176, 'practice': 3177, 'practicing': 3178, 'pray': 3179, 'prayer': 3180, 'prayers': 3181, 'preaching': 3182, 'precipitated': 3183, 'prefer': 3184, 'prejudice': 3185, 'preoccupied': 3186, 'preparation': 3187, 'prepare': 3188, 'prepares': 3189, 'presence': 3190, 'present': 3191, 'press': 3192, 'pressed': 3193, 'presses': 3194, 'pressure': 3195, 'presume': 3196, 'pretend': 3197, 'pretty': 3198, 'previous': 3199, 'price': 3200, 'priced': 3201, 'pride': 3202, 'primal': 3203, 'primitive': 3204, 'prison': 3205, 'prisoner': 3206, 'private': 3207, 'probably': 3208, 'problem': 3209, 'problems': 3210, 'proceeds': 3211, 'produces': 3212, 'productive': 3213, 'profound': 3214, 'program': 3215, 'prom': 3216, 'promise': 3217, 'promised': 3218, 'proof': 3219, 'proper': 3220, 'property': 3221, 'proposes': 3222, 'protect': 3223, 'protecting': 3224, 'protection': 3225, 'protectors': 3226, 'proud': 3227, 'prove': 3228, 'psychiatrist': 3229, 'psychological': 3230, 'public': 3231, 'pulaski': 3232, 'pull': 3233, 'pulled': 3234, 'pulls': 3235, 'pulsating': 3236, 'pumice': 3237, 'pumped': 3238, 'punch': 3239, 'punches': 3240, 'punk': 3241, 'puppets': 3242, 'purchasing': 3243, 'purse': 3244, 'pursue': 3245, 'pursued': 3246, 'pursuit': 3247, 'pushes': 3248, 'pussycat': 3249, 'put': 3250, 'puts': 3251, 'putting': 3252, 'puzzled': 3253, 'quaint': 3254, 'qualification': 3255, 'quality': 3256, \"quarter'll\": 3257, 'quarterback': 3258, 'quarters': 3259, 'quartet': 3260, 'queen': 3261, 'question': 3262, 'questions': 3263, 'quick': 3264, 'quickly': 3265, 'quiet': 3266, 'quieter': 3267, 'quietly': 3268, 'quietness': 3269, 'quit': 3270, 'quite': 3271, 'quittin': 3272, 'qvath': 3273, 'r': 3274, 'r.': 3275, 'raccoons': 3276, 'racing': 3277, 'racket': 3278, 'radio': 3279, 'rage': 3280, 'rail': 3281, 'railing': 3282, 'railroad': 3283, 'rails': 3284, 'rain': 3285, 'rainbow': 3286, 'rainbows': 3287, 'raises': 3288, 'raising': 3289, 'ramp': 3290, 'ran': 3291, 'range': 3292, 'ranges': 3293, 'ransacked': 3294, 'rap': 3295, 'rapid': 3296, 'rapidly': 3297, 'rare': 3298, 'raspberry': 3299, 'rather': 3300, 're': 3301, 'reach': 3302, 'reaches': 3303, 'react': 3304, 'reacting': 3305, 'reaction': 3306, 'reactions': 3307, 'reacts': 3308, 'read': 3309, 'reads': 3310, 'ready': 3311, 'real': 3312, 'realize': 3313, 'realized': 3314, 'realizes': 3315, 'realizing': 3316, 'really': 3317, 'rear': 3318, 'reason': 3319, 'reasonable': 3320, 'reasonably': 3321, 'reasons': 3322, 'reassured': 3323, 'reassuring': 3324, 'rebellion': 3325, 'rebellious': 3326, 'rec': 3327, 'recall': 3328, 'recedes': 3329, 'receipts': 3330, 'receive': 3331, 'received': 3332, 'receiver': 3333, 'receives': 3334, 'recently': 3335, 'reception': 3336, 'receptionist': 3337, 'recipe': 3338, 'recite': 3339, 'recognize': 3340, 'recommend': 3341, 'recommendation': 3342, 'reconstruction': 3343, 'record': 3344, 'recorder': 3345, 'recording': 3346, 'rectangular': 3347, 'red': 3348, 'redhead': 3349, 'redlight': 3350, 'redoes': 3351, 'reeling': 3352, 'reenacts': 3353, 'reenters': 3354, 'reference': 3355, 'refill': 3356, 'refills': 3357, 'reflected': 3358, 'refuses': 3359, 'regains': 3360, 'regard': 3361, 'regarding': 3362, 'register': 3363, 'registers': 3364, 'regular': 3365, 'regulars': 3366, 'reigneth': 3367, 'reinforcement': 3368, 'relation': 3369, 'relations': 3370, 'relationship': 3371, 'release': 3372, 'released': 3373, 'releases': 3374, 'relief': 3375, 'relieving': 3376, 'relishing': 3377, 'reluctance': 3378, 'remain': 3379, 'remainder': 3380, 'remains': 3381, 'remember': 3382, 'remembered': 3383, 'remembering': 3384, 'remembers': 3385, 'remind': 3386, 'reminder': 3387, 'remote': 3388, 'remove': 3389, 'removes': 3390, 'removing': 3391, 'renault': 3392, 'rend': 3393, 'rending': 3394, 'rep.': 3395, 'repeats': 3396, 'repetition': 3397, 'replaces': 3398, 'replayed': 3399, 'replied': 3400, 'report': 3401, 'representative': 3402, 'repulses': 3403, 'requires': 3404, 'rescue': 3405, 'research': 3406, 'resemblance': 3407, 'reserve': 3408, 'residence': 3409, 'resist': 3410, 'resort': 3411, 'respect': 3412, 'respond': 3413, 'response': 3414, 'responsibilities': 3415, 'responsibility': 3416, 'responsible': 3417, 'rest': 3418, 'restlessly': 3419, 'rests': 3420, 'results': 3421, 'resume': 3422, 'resumes': 3423, 'resurrection': 3424, 'rethinks': 3425, 'retirement': 3426, 'retrieve': 3427, 'retrieves': 3428, 'return': 3429, 'returning': 3430, 'returns': 3431, 'reveal': 3432, 'revealing': 3433, 'reveals': 3434, 'reverse': 3435, 'reversed': 3436, 'reverses': 3437, 'rhizome': 3438, 'rhythm': 3439, 'rhythmic': 3440, 'ribbon': 3441, 'rich': 3442, 'ricochets': 3443, 'ride': 3444, 'ridiculous': 3445, 'riding': 3446, 'rig': 3447, 'right': 3448, 'right--': 3449, 'rights': 3450, 'rigorously': 3451, 'rime': 3452, 'ring': 3453, 'ringing': 3454, 'rings': 3455, 'rip': 3456, 'rise': 3457, 'rises': 3458, 'rising': 3459, 'risks': 3460, 'river': 3461, 'riverbank': 3462, 'ro': 3463, 'road': 3464, 'roadhouse': 3465, 'roadside': 3466, 'roaring': 3467, 'roars': 3468, 'roast': 3469, 'robe': 3470, 'robert': 3471, 'robin': 3472, 'rock': 3473, 'rockers': 3474, 'rocket': 3475, 'rocks': 3476, 'rodeo': 3477, 'roll': 3478, 'rolled': 3479, 'rolling': 3480, 'rolls': 3481, 'romantic': 3482, 'romeo': 3483, 'romeos': 3484, 'ronette': 3485, 'room': 3486, 'rooms': 3487, 'rooster': 3488, 'root': 3489, 'rope': 3490, 'ropes': 3491, 'rose': 3492, 'rosenferd': 3493, 'rosenfield': 3494, 'rotweiler': 3495, 'rough': 3496, 'roughing': 3497, 'round': 3498, 'rounds': 3499, 'route': 3500, 'routine': 3501, 'roving': 3502, 'row': 3503, 'rower': 3504, 'rowing': 3505, 'rub': 3506, 'rubber': 3507, 'rubbing': 3508, 'rubs': 3509, 'rude': 3510, 'ruin': 3511, 'ruled': 3512, 'rummaging': 3513, 'rumple': 3514, 'run': 3515, 'runabout': 3516, 'rundown': 3517, 'runner': 3518, 'runners': 3519, 'running': 3520, 'runs': 3521, 'runway': 3522, 'rush': 3523, 'rushes': 3524, 'rustic': 3525, 'rusticated': 3526, 's.': 3527, 'sabine': 3528, 'sad': 3529, 'saddened': 3530, 'sadly': 3531, 'sadness': 3532, 'safe': 3533, 'safely': 3534, 'safety': 3535, 'said': 3536, 'sail': 3537, 'sailing': 3538, 'sailor': 3539, 'sails': 3540, 'saint': 3541, 'saith': 3542, 'sake': 3543, 'sales': 3544, 'salesman': 3545, 'salesperson': 3546, 'same': 3547, 'sample': 3548, 'samples': 3549, 'samsonite': 3550, 'san': 3551, 'sand': 3552, 'sandpaper': 3553, 'sandwich': 3554, 'sandwiches': 3555, 'sarah': 3556, 'sashays': 3557, 'sat': 3558, 'satisfactorily': 3559, 'satisfied': 3560, 'sauna': 3561, 'sausage': 3562, 'save': 3563, 'saw': 3564, 'sawmill': 3565, 'saxophone': 3566, 'say': 3567, 'saying': 3568, 'says': 3569, 'scaled': 3570, 'scallywag': 3571, 'scans': 3572, 'scantily': 3573, 'scare': 3574, 'scared': 3575, 'scaring': 3576, 'scarlet': 3577, 'scattered': 3578, 'scatters': 3579, 'scene': 3580, 'scheduled': 3581, 'school': 3582, 'schoolgirl': 3583, 'schoolwork': 3584, 'science': 3585, 'score': 3586, 'scores': 3587, 'scorn': 3588, 'scornfully': 3589, 'scotch': 3590, 'scotches': 3591, 'scowls': 3592, 'scraping': 3593, 'scream': 3594, 'screaming': 3595, 'screams': 3596, 'screech': 3597, 'screen': 3598, 'script': 3599, 'scrubbing': 3600, 'scrubs': 3601, 'sculpture': 3602, 'scurries': 3603, 'seal': 3604, 'searches': 3605, 'searching': 3606, 'seat': 3607, 'seated': 3608, 'seats': 3609, 'seattle': 3610, 'sec': 3611, 'second': 3612, 'secret': 3613, 'secretary': 3614, 'secretly': 3615, 'secrets': 3616, 'section': 3617, 'security': 3618, 'sedated': 3619, 'seduce': 3620, 'seductively': 3621, 'see': 3622, 'seedy': 3623, 'seeing': 3624, 'seeking': 3625, 'seem': 3626, 'seemed': 3627, 'seemingly': 3628, 'seems': 3629, 'seen': 3630, 'seepage': 3631, 'sees': 3632, 'seized': 3633, 'sejir': 3634, 'selects': 3635, 'self': 3636, 'sell': 3637, 'sellin': 3638, 'selling': 3639, 'semi': 3640, 'send': 3641, 'sender': 3642, 'senfield': 3643, 'sensational': 3644, 'sense': 3645, 'sensibility': 3646, 'sensually': 3647, 'sent': 3648, 'sentiment': 3649, 'sentimental': 3650, 'separate': 3651, 'separately': 3652, 'separates': 3653, 'september': 3654, 'serial': 3655, 'series': 3656, 'serious': 3657, 'serves': 3658, 'service': 3659, 'serving': 3660, 'session': 3661, 'set': 3662, 'sets': 3663, 'setting': 3664, 'settle': 3665, 'seven': 3666, 'several': 3667, 'severally': 3668, 'severe': 3669, 'sew': 3670, 'sewn': 3671, 'sex': 3672, 'sexes': 3673, 'sexual': 3674, 'shade': 3675, 'shades': 3676, 'shadow': 3677, 'shadows': 3678, 'shake': 3679, 'shakes': 3680, 'shaking': 3681, 'shall': 3682, 'shallow': 3683, 'shape': 3684, 'shapiro': 3685, 'share': 3686, 'sharp': 3687, 'she': 3688, 'sheer': 3689, 'sheet': 3690, 'shelf': 3691, 'shellshocked': 3692, 'shelly': 3693, 'shenanigans': 3694, 'sheriff': 3695, 'sherlock': 3696, \"sherrif's\": 3697, 'shh': 3698, 'shift': 3699, 'shifts': 3700, 'shimmers': 3701, 'shiner': 3702, 'shines': 3703, 'shirt': 3704, 'shirt--': 3705, 'shit': 3706, 'shivers': 3707, 'shivery': 3708, 'shock': 3709, 'shocked': 3710, 'shocking': 3711, 'shoe': 3712, 'shoes': 3713, 'shoot': 3714, 'shooting': 3715, 'shoots': 3716, 'shop': 3717, 'shopper': 3718, 'short': 3719, 'shortest': 3720, 'shortly': 3721, 'shorts': 3722, 'shortstack': 3723, 'shot': 3724, 'shotgun': 3725, 'shots': 3726, 'should': 3727, 'shoulder': 3728, 'shoulders': 3729, 'shouting': 3730, 'shouts': 3731, 'shoving': 3732, 'show': 3733, 'shower': 3734, 'shows': 3735, 'shrieks': 3736, 'shrug': 3737, 'shushes': 3738, 'shut': 3739, 'shuts': 3740, 'shutting': 3741, 'shy': 3742, 'shyly': 3743, 'sick': 3744, 'sickened': 3745, 'side': 3746, 'sidebenefit': 3747, 'sided': 3748, 'sifts': 3749, 'sighs': 3750, 'sight': 3751, 'sign': 3752, 'signal': 3753, 'signals': 3754, 'signature': 3755, 'signing': 3756, 'signs': 3757, 'silence': 3758, 'silences': 3759, 'silent': 3760, 'silently': 3761, 'silk': 3762, 'silver': 3763, 'simple': 3764, 'simply': 3765, 'simultaneously': 3766, 'since': 3767, 'sincerely': 3768, 'sing': 3769, 'singin': 3770, 'singing': 3771, 'single': 3772, 'singular': 3773, 'sink': 3774, 'sinkhole': 3775, 'sip': 3776, 'sipping': 3777, 'sips': 3778, 'sir': 3779, 'siren': 3780, 'sister': 3781, 'sisters': 3782, 'sit': 3783, 'site': 3784, 'sits': 3785, 'sitting': 3786, 'situation': 3787, 'six': 3788, 'sixty': 3789, 'skeptical': 3790, 'sketch': 3791, 'sketched': 3792, 'sketches': 3793, 'skimpy': 3794, 'skin': 3795, 'skip': 3796, 'skips': 3797, 'sky': 3798, 'slams': 3799, 'slant': 3800, 'slaps': 3801, 'slats': 3802, 'sleep': 3803, 'sleeping': 3804, 'sleeps': 3805, 'sleeve': 3806, 'sleight': 3807, 'slept': 3808, 'slice': 3809, 'slicker': 3810, 'slickly': 3811, 'slides': 3812, 'slight': 3813, 'slightly': 3814, 'slinks': 3815, 'slipped': 3816, 'slipping': 3817, 'slips': 3818, 'slipshod': 3819, 'slit': 3820, 'slow': 3821, 'slowly': 3822, 'slows': 3823, 'slumber': 3824, 'sly': 3825, 'small': 3826, 'smaller': 3827, 'smalltown': 3828, 'smell': 3829, 'smells': 3830, 'smile': 3831, 'smiles': 3832, 'smiling': 3833, 'smoke': 3834, 'smokes': 3835, 'smokey': 3836, 'smoking': 3837, 'smorgasbord': 3838, 'snacking': 3839, 'snake': 3840, 'snaps': 3841, 'snapshot': 3842, 'sneaks': 3843, 'sneers': 3844, 'sniffing': 3845, 'sniffs': 3846, 'snooping': 3847, 'snowballs': 3848, 'snuck': 3849, 'snuffling': 3850, 'so': 3851, 'soap': 3852, 'soar': 3853, 'soaring': 3854, 'sob': 3855, 'sociable': 3856, 'social': 3857, 'society': 3858, 'sock': 3859, 'sockets': 3860, 'socks': 3861, 'sofa': 3862, 'sofas': 3863, 'soft': 3864, 'softie': 3865, 'softly': 3866, 'soggy': 3867, 'soiled': 3868, 'solid': 3869, 'solo': 3870, 'solution': 3871, 'solutions': 3872, 'solve': 3873, 'some': 3874, 'somebody': 3875, 'someday': 3876, 'someone': 3877, 'someplace': 3878, 'something': 3879, 'sometime': 3880, 'sometimes': 3881, 'somewhere': 3882, 'son': 3883, 'song': 3884, 'sonny': 3885, 'soon': 3886, 'sooner': 3887, 'soothing': 3888, 'sophistication': 3889, 'sorry': 3890, 'sort': 3891, 'sorts': 3892, 'sos': 3893, 'sotto': 3894, 'sought': 3895, 'soul': 3896, 'souls': 3897, 'sound': 3898, 'soundly': 3899, 'sounds': 3900, 'sour': 3901, 'source': 3902, 'south': 3903, 'southeast': 3904, 'space': 3905, 'spacious': 3906, 'spanky': 3907, 'spark': 3908, 'sparkling': 3909, 'sparkwood': 3910, 'sparky': 3911, 'sparsely': 3912, 'speak': 3913, 'speaker': 3914, 'speaking': 3915, 'speaks': 3916, 'spears': 3917, 'special': 3918, 'species': 3919, 'specific': 3920, 'specifically': 3921, 'specifics': 3922, 'speech': 3923, 'speechless': 3924, 'speed': 3925, 'speeding': 3926, 'spell': 3927, 'spells': 3928, 'spend': 3929, 'spent': 3930, 'spilled': 3931, 'spills': 3932, 'spin': 3933, 'spins': 3934, 'spirit': 3935, 'spirits': 3936, 'spiritual': 3937, 'spite': 3938, 'spoke': 3939, 'spoken': 3940, 'sponges': 3941, 'spooky': 3942, 'spool': 3943, 'spooling': 3944, 'spoon': 3945, 'sports': 3946, 'spot': 3947, 'spots': 3948, 'spotting': 3949, 'spread': 3950, 'spring': 3951, 'squad': 3952, 'square': 3953, 'squarejaw': 3954, 'squares': 3955, 'squaw': 3956, 'squawks': 3957, 'squeal': 3958, 'squeezed': 3959, 'squeezing': 3960, 'squints': 3961, 'squish': 3962, 'stack': 3963, 'stainless': 3964, 'stairs': 3965, 'stairway': 3966, 'stairwell': 3967, 'staked': 3968, 'staking': 3969, 'stalemate': 3970, 'stand': 3971, 'standing': 3972, 'stands': 3973, 'stare': 3974, 'stares': 3975, 'staring': 3976, 'stars': 3977, 'start': 3978, 'started': 3979, 'starting': 3980, 'startled': 3981, 'startles': 3982, 'starts': 3983, 'stash': 3984, 'stashed': 3985, 'stashes': 3986, 'state': 3987, 'stately': 3988, 'statement': 3989, 'stating': 3990, 'station': 3991, 'statistic': 3992, 'stay': 3993, 'staying': 3994, 'steady': 3995, 'steak': 3996, 'stealing': 3997, 'steaming': 3998, 'steamy': 3999, 'steel': 4000, 'steers': 4001, 'stenciled': 4002, 'steno': 4003, 'step': 4004, 'stepped': 4005, 'steppin': 4006, 'stepping': 4007, 'steps': 4008, 'stereo': 4009, 'stern': 4010, 'steve': 4011, 'stick': 4012, 'sticking': 4013, 'sticks': 4014, 'stiff': 4015, 'stiffens': 4016, 'stifles': 4017, 'still': 4018, 'stilted': 4019, 'stitches': 4020, 'stomach': 4021, 'stood': 4022, 'stop': 4023, 'stopped': 4024, 'stops': 4025, 'store': 4026, 'stories': 4027, 'storm': 4028, 'storms': 4029, 'story': 4030, 'stove': 4031, 'straight': 4032, 'strains': 4033, 'strange': 4034, 'strangely': 4035, 'stranger': 4036, 'stray': 4037, 'streaming': 4038, 'streamliner': 4039, 'street': 4040, 'streetcars': 4041, 'streets': 4042, 'strength': 4043, 'stretches': 4044, 'strict': 4045, 'strides': 4046, 'strikes': 4047, 'strikingly': 4048, 'string': 4049, 'strokes': 4050, 'strong': 4051, 'struck': 4052, 'structure': 4053, 'structures': 4054, 'struggles': 4055, 'struggling': 4056, 'strung': 4057, 'stuck': 4058, 'studies': 4059, 'study': 4060, 'studying': 4061, 'stuff': 4062, 'stunned': 4063, 'stunt': 4064, 'stupid': 4065, 'stupidity': 4066, 'style': 4067, 'stylized': 4068, 'stylus': 4069, 'sub': 4070, 'subconsciously': 4071, 'subdued': 4072, 'subject': 4073, 'submission': 4074, 'submitted': 4075, 'substitute': 4076, 'subtle': 4077, 'suburbis': 4078, 'successful': 4079, 'such': 4080, 'sucker': 4081, 'suction': 4082, 'sudden': 4083, 'suddenly': 4084, 'sue': 4085, 'suffer': 4086, 'suffered': 4087, 'suffice': 4088, 'suggest': 4089, 'suggesting': 4090, 'suggests': 4091, 'suicide': 4092, 'suit': 4093, 'suitcase': 4094, 'suitcases': 4095, 'suite': 4096, 'suits': 4097, 'sultan': 4098, 'summer': 4099, 'summing': 4100, 'sun': 4101, 'sunday': 4102, 'sundays': 4103, 'sunglasses': 4104, 'sunlight': 4105, 'super': 4106, 'supervises': 4107, 'supervisor': 4108, 'supper': 4109, 'supply': 4110, 'support': 4111, 'suppose': 4112, 'supposed': 4113, 'sure': 4114, 'surface': 4115, 'surgical': 4116, 'surprise': 4117, 'surprised': 4118, 'surprisingly': 4119, 'surrounded': 4120, 'surrounding': 4121, 'surrounds': 4122, 'suspect': 4123, 'suspects': 4124, 'suspended': 4125, 'suspense': 4126, 'suspicion': 4127, 'suspicious': 4128, 'swabbie': 4129, 'swallows': 4130, 'swaying': 4131, 'sways': 4132, 'swear': 4133, 'sweating': 4134, 'sweats': 4135, 'swede': 4136, 'sweeper': 4137, 'sweepers': 4138, 'sweeps': 4139, 'sweet': 4140, 'sweetheart': 4141, 'sweetly': 4142, 'sweetness': 4143, 'swells': 4144, 'swim': 4145, 'swing': 4146, 'swinging': 4147, 'swings': 4148, 'switchblade': 4149, 'switchboard': 4150, 'switches': 4151, 'swivels': 4152, 'swizzle': 4153, 'swoops': 4154, 'sylvia': 4155, 'sympathetic': 4156, 'sympathy': 4157, 'syringe': 4158, 'syrup': 4159, 'syrupy': 4160, 'system': 4161, 't': 4162, 't.': 4163, 'ta': 4164, 'tab': 4165, 'table': 4166, 'tables': 4167, 'tacked': 4168, 'tackle': 4169, 'tad': 4170, 'tag': 4171, 'tail': 4172, 'tails': 4173, 'take': 4174, 'taken': 4175, 'takes': 4176, 'taking': 4177, 'talk': 4178, 'talked': 4179, 'talkie': 4180, 'talking': 4181, 'talks': 4182, 'talky': 4183, 'tall': 4184, 'tallied': 4185, 'tao': 4186, 'tape': 4187, 'taps': 4188, 'target': 4189, 'targeted': 4190, 'targets': 4191, 'tarp': 4192, 'taste': 4193, 'tattoo': 4194, 'taught': 4195, 'teach': 4196, 'teak': 4197, 'team': 4198, 'tear': 4199, 'tearful': 4200, 'tears': 4201, 'tearyeyed': 4202, 'technique': 4203, 'teenage': 4204, 'teeth': 4205, 'telephone': 4206, 'telescopic': 4207, 'telescoping': 4208, 'television': 4209, 'televison': 4210, 'tell': 4211, 'telling': 4212, 'tells': 4213, 'temperate': 4214, 'temperature': 4215, 'tempered': 4216, 'tempestuous': 4217, 'tempo': 4218, 'ten': 4219, 'tender': 4220, 'tenderly': 4221, 'tending': 4222, 'tennessee': 4223, 'tense': 4224, 'tentative': 4225, 'tentatively': 4226, 'term': 4227, 'terrible': 4228, 'terribly': 4229, 'terrified': 4230, 'test': 4231, 'testament': 4232, 'testing': 4233, 'tests': 4234, 'than': 4235, 'thank': 4236, 'thankful': 4237, 'thanks': 4238, 'that': 4239, 'the': 4240, 'the-(leaving': 4241, 'thee': 4242, 'their': 4243, 'them': 4244, 'themed': 4245, 'themselves': 4246, 'then': 4247, 'there': 4248, 'thereafter': 4249, 'therefore': 4250, 'therenadine': 4251, 'theresa': 4252, 'these': 4253, 'they': 4254, \"they're\": 4255, 'thin': 4256, 'thing': 4257, 'things': 4258, \"things'll\": 4259, \"things're\": 4260, 'think': 4261, 'thinking': 4262, 'thinks': 4263, 'third': 4264, 'thirsty': 4265, 'thirty': 4266, 'this': 4267, 'this--': 4268, 'thorn': 4269, 'those': 4270, 'thou': 4271, 'though': 4272, 'thought': 4273, 'thoughts': 4274, 'thousand': 4275, 'thousands': 4276, 'thread': 4277, 'threat': 4278, 'three': 4279, 'threesome': 4280, 'throat': 4281, 'throng': 4282, 'through': 4283, 'throughout': 4284, 'throwing': 4285, 'throws': 4286, 'thru': 4287, 'thuds': 4288, 'thumb': 4289, 'thumbnail': 4290, 'thumbs': 4291, 'thump': 4292, 'thursday': 4293, 'thy': 4294, 'tibet': 4295, 'tibetan': 4296, 'ticket': 4297, 'tie': 4298, 'tied': 4299, 'ties': 4300, 'tight': 4301, 'til': 4302, 'till': 4303, 'tilt': 4304, 'tilting': 4305, 'timber': 4306, 'time': 4307, 'timed': 4308, 'timer': 4309, 'times': 4310, 'tip': 4311, 'tips': 4312, 'tiptoe': 4313, 'tired': 4314, 'tires': 4315, 'title': 4316, 'titles': 4317, 'to': 4318, 'toad': 4319, 'toast': 4320, 'tobacco': 4321, 'today': 4322, 'tofive': 4323, 'together': 4324, 'told': 4325, 'tolerant': 4326, 'tomes': 4327, 'tommy': 4328, 'tomorrow': 4329, 'tongue': 4330, 'tonight': 4331, 'tonk': 4332, 'too': 4333, 'took': 4334, 'toot': 4335, 'tooth': 4336, 'top': 4337, 'topic': 4338, 'torch': 4339, 'torches': 4340, 'torment': 4341, 'toss': 4342, 'tosses': 4343, 'tossin': 4344, 'total': 4345, 'tottering': 4346, 'totters': 4347, 'touch': 4348, 'touched': 4349, 'touches': 4350, 'tough': 4351, 'tour': 4352, 'toward': 4353, 'towards': 4354, 'towel': 4355, 'towering': 4356, 'towers': 4357, 'town': 4358, 'townsfolk': 4359, 'toxicology': 4360, 'toxological': 4361, 'toys': 4362, 'traces': 4363, 'track': 4364, 'tracked': 4365, 'tracker': 4366, 'trading': 4367, 'traditional': 4368, 'traffic': 4369, 'tragedies': 4370, 'tragedy': 4371, 'train': 4372, 'trained': 4373, 'trains': 4374, 'trait': 4375, 'transactions': 4376, 'transfer': 4377, 'transformer': 4378, 'translator': 4379, 'transmission': 4380, 'trash': 4381, 'travel': 4382, 'traveled': 4383, 'traveler': 4384, 'traveling': 4385, 'travels': 4386, 'tray': 4387, 'treacherous': 4388, 'treat': 4389, 'treating': 4390, 'treats': 4391, 'tree': 4392, 'trees': 4393, 'trellis': 4394, 'tremble': 4395, 'trembles': 4396, 'trembling': 4397, 'tri': 4398, 'trick': 4399, 'tried': 4400, 'tries': 4401, 'trigger': 4402, 'trips': 4403, 'triumphant': 4404, 'trooper': 4405, 'trouble': 4406, 'troubled': 4407, 'troubling': 4408, 'trout': 4409, 'troy': 4410, 'truck': 4411, 'trucker': 4412, 'trudy': 4413, 'true': 4414, 'truly': 4415, 'truman': 4416, 'trumpet': 4417, 'trust': 4418, 'trusted': 4419, 'truth': 4420, 'try': 4421, 'trying': 4422, 'tube': 4423, 'tucked': 4424, 'tuesday': 4425, 'tugs': 4426, 'tumbler': 4427, 'tumbles': 4428, 'tumult': 4429, 'tundra': 4430, 'tune': 4431, 'tuned': 4432, 'turkey': 4433, 'turkeys': 4434, 'turn': 4435, 'turned': 4436, 'turning': 4437, 'turnover': 4438, 'turnpike': 4439, 'turns': 4440, 'turntable': 4441, 'tutor': 4442, 'tutored': 4443, 'tv': 4444, 'twelve': 4445, 'twenty': 4446, 'twentyfive': 4447, 'twice': 4448, 'twin': 4449, 'twine': 4450, 'twinkle': 4451, 'twist': 4452, 'twists': 4453, 'two': 4454, 'tycoons': 4455, 'u.s.': 4456, 'ugly': 4457, 'uh': 4458, 'unaffected': 4459, 'unafraid': 4460, 'unarmed': 4461, 'unbutton': 4462, 'unbuttoning': 4463, 'uncertain': 4464, 'uncle': 4465, 'uncomfortable': 4466, 'uncomfortably': 4467, 'uncomplicated': 4468, 'uncovers': 4469, 'under': 4470, 'undercover': 4471, 'understand': 4472, 'understanding': 4473, 'understands': 4474, 'undeterred': 4475, 'undone': 4476, 'undressing': 4477, 'uneasily': 4478, 'uneasy': 4479, 'unfocused': 4480, 'unforeseen': 4481, 'unhandcuffed': 4482, 'unidentified': 4483, 'uniform': 4484, 'unknown': 4485, 'unless': 4486, 'unloading': 4487, 'unnoticed': 4488, 'unprofessional': 4489, 'unseen': 4490, 'unsettled': 4491, 'unshaven': 4492, 'unsolvable': 4493, 'unsophisticated': 4494, 'unsteadily': 4495, 'unsure': 4496, 'untied': 4497, 'until': 4498, 'unto': 4499, \"untrimm'd\": 4500, 'untroubled': 4501, 'unusual': 4502, 'unwavering': 4503, 'unwraps': 4504, 'up': 4505, 'upon': 4506, 'upper': 4507, 'upright': 4508, 'uprising': 4509, 'ups': 4510, 'upset': 4511, 'upside': 4512, 'upstairs': 4513, 'upstream': 4514, 'urgent': 4515, 'urgently': 4516, 'urinate': 4517, 'us': 4518, 'use': 4519, 'used': 4520, 'uses': 4521, 'using': 4522, 'usual': 4523, 'usually': 4524, 'uturn': 4525, 'v': 4526, 'vacation': 4527, 'vagrant': 4528, 'vain': 4529, 'value': 4530, 'vampires': 4531, 'vanilla': 4532, 'vanished': 4533, 'variety': 4534, 'varsity': 4535, 'vase': 4536, 'vcr': 4537, 'vehicle': 4538, 'venality': 4539, 'verification': 4540, 'very': 4541, 'veterinarian': 4542, 'vets': 4543, 'vette': 4544, 'vicious': 4545, 'video': 4546, 'videotape': 4547, 'view': 4548, 'vigorously': 4549, 'vikings': 4550, 'violation': 4551, 'violently': 4552, 'violins': 4553, 'virgin': 4554, 'vis': 4555, 'visa': 4556, 'visible': 4557, 'vision': 4558, 'visions': 4559, 'visit': 4560, 'visiting': 4561, 'voce': 4562, 'voice': 4563, 'void': 4564, 'voltage': 4565, 'volume': 4566, 'vowed': 4567, 'vulnerable': 4568, 'wage': 4569, 'wagon': 4570, 'wailing': 4571, 'waist': 4572, 'wait': 4573, \"wait'll\": 4574, 'waitin': 4575, 'waiting': 4576, 'waitress': 4577, 'waits': 4578, 'wake': 4579, 'waken': 4580, 'waking': 4581, 'walk': 4582, 'walkie': 4583, 'walking': 4584, 'walks': 4585, 'walky': 4586, 'wall': 4587, 'wallet': 4588, 'walls': 4589, 'waltz': 4590, 'wan': 4591, 'wander': 4592, \"wander'st\": 4593, 'wanders': 4594, 'want': 4595, 'wanted': 4596, 'wants': 4597, 'war': 4598, 'ward': 4599, 'warm': 4600, 'warmly': 4601, 'warmup': 4602, 'warn': 4603, 'warning': 4604, 'warrants': 4605, 'was': 4606, 'was-(thinks': 4607, 'wash': 4608, 'washed': 4609, 'washer': 4610, 'washington': 4611, 'wasted': 4612, 'watch': 4613, 'watches': 4614, 'watching': 4615, 'water': 4616, 'waterfall': 4617, 'watson': 4618, 'waves': 4619, 'wax': 4620, 'waxing': 4621, 'way': 4622, 'ways': 4623, 'we': 4624, \"we're\": 4625, 'weak': 4626, 'weakly': 4627, 'weakness': 4628, 'weapon': 4629, 'weapons': 4630, 'wear': 4631, 'wearing': 4632, 'wearinghis-': 4633, 'wears': 4634, 'weathered': 4635, 'wednesday': 4636, 'week': 4637, 'weeks': 4638, 'weeps': 4639, 'weepy': 4640, 'weighs': 4641, 'weight': 4642, 'weird': 4643, 'weirdo': 4644, 'welcome': 4645, 'well': 4646, 'went': 4647, 'were': 4648, 'west': 4649, 'western': 4650, 'wet': 4651, 'what': 4652, 'whatever': 4653, 'wheat': 4654, 'wheel': 4655, 'wheelchair': 4656, 'wheels': 4657, 'when': 4658, 'where': 4659, 'whether': 4660, 'which': 4661, 'while': 4662, 'whine': 4663, 'whips': 4664, 'whisks': 4665, 'whisper': 4666, 'whispered': 4667, 'whispering': 4668, 'whispers': 4669, 'whistle': 4670, 'whistles': 4671, 'whistling': 4672, 'white': 4673, 'whittle': 4674, 'whittled': 4675, 'whittling': 4676, 'who': 4677, 'whole': 4678, 'whom': 4679, 'whosoever': 4680, 'why': 4681, 'wide': 4682, 'widen': 4683, 'widow': 4684, 'wienie': 4685, 'wife': 4686, 'wiggles': 4687, 'wild': 4688, 'wildly': 4689, 'will': 4690, 'willing': 4691, 'willow': 4692, 'wilson': 4693, 'wind': 4694, 'window': 4695, 'windowless': 4696, 'windows': 4697, 'winds': 4698, 'wine': 4699, 'wing': 4700, 'wings': 4701, 'wink': 4702, 'winks': 4703, 'wipe': 4704, 'wipes': 4705, 'wiry': 4706, 'wisdom': 4707, 'wise': 4708, 'wish': 4709, 'wished': 4710, 'wishes': 4711, 'with': 4712, 'withdraws': 4713, 'within': 4714, 'without': 4715, 'witnessed': 4716, 'witnessing': 4717, 'wits': 4718, 'wizened': 4719, 'wo': 4720, 'woke': 4721, 'woken': 4722, 'woman': 4723, 'women': 4724, 'women--': 4725, 'wonder': 4726, 'wonderful': 4727, 'wonders': 4728, 'wood': 4729, 'wooded': 4730, 'woods': 4731, 'word': 4732, 'words': 4733, 'work': 4734, 'workboots': 4735, 'worked': 4736, 'workers': 4737, 'working': 4738, 'workout': 4739, 'works': 4740, 'world': 4741, 'worldly': 4742, 'worlds': 4743, 'worn': 4744, 'worried': 4745, 'worry': 4746, 'worse': 4747, 'worth': 4748, 'would': 4749, 'wound': 4750, 'wounded': 4751, 'wounds': 4752, 'wrapped': 4753, 'wrench': 4754, 'wrenches': 4755, 'wrist': 4756, 'wrists': 4757, 'wristwatch': 4758, 'write': 4759, 'writes': 4760, 'writing': 4761, 'written': 4762, 'wrong': 4763, 'wrote': 4764, 'y': 4765, 'ya': 4766, 'yard': 4767, 'yeah': 4768, 'year': 4769, 'yearns': 4770, 'yearold': 4771, 'years': 4772, 'yelling': 4773, 'yellow': 4774, 'yells': 4775, 'yep': 4776, 'yes': 4777, 'yesterday': 4778, 'yesterday--': 4779, 'yet': 4780, 'yip': 4781, 'yoga': 4782, 'yokel': 4783, 'you': 4784, \"you're\": 4785, 'young': 4786, 'younger': 4787, 'your': 4788, 'yours': 4789, 'yourself': 4790, 'z.': 4791, 'zap': 4792, 'zip': 4793}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriCBCC9WOuO"
      },
      "source": [
        "## create sequences\n",
        "Now, we have to create the input data for our LSTM. We create two lists:\n",
        " - **sequences**: this list will contain the sequences of words used to train the model,\n",
        " - **next_words**: this list will contain the next words for each sequences of the **sequences** list.\n",
        " \n",
        "In this exercice, we assume we will train the network with sequences of 30 words (seq_length = 30).\n",
        "\n",
        "So, to create the first sequence of words, we take the 30th first words in the **wordlist** list. The word 31 is the next word of this first sequence, and is added to the **next_words** list.\n",
        "\n",
        "Then we jump by a step of 1 (sequences_step = 1 in our example) in the list of words, to create the second sequence of words and retrieve the second \"next word\".\n",
        "\n",
        "We iterate this task until the end of the list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAecFMYhWOuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b15e81f7-ffda-46d0-d352-5b55fe1ed2a8"
      },
      "source": [
        "#create sequences\n",
        "sequences = []\n",
        "next_words = []\n",
        "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
        "    sequences.append(wordlist[i: i + seq_length])\n",
        "    next_words.append(wordlist[i + seq_length])\n",
        "\n",
        "print('nb sequences:', len(sequences))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 51166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZZMPiXqWOuQ"
      },
      "source": [
        "When we iterate over the whole list of words, we create 172104 sequences of words, and retrieve, for each of them, the next word to be predicted.\n",
        "\n",
        "However, these lists cannot be used \"as is\". We have to transform them in order to ingest them in the LSTM. Text will not be understood by neural net, we have to use digits.\n",
        "However, we cannot only map a words to its index in the vocabulary, as it does not represent intrasinqly the word. It is better to reorganize a sequence of words as a matrix of booleans.\n",
        "\n",
        "So, we create the matrix X and y :\n",
        " - X : the matrix of the following dimensions:\n",
        "     - number of sequences,\n",
        "     - number of words in sequences,\n",
        "     - number of words in the vocabulary.\n",
        " - y : the matrix of the following dimensions:\n",
        "     - number of sequences,\n",
        "     - number of words in the vocabulary.\n",
        " \n",
        "For each word, we retrieve its index in the vocabulary, and we set to 1 its position in the matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB6Js0cAWOuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e11eb2-9cc7-4589-a963-90c434b92553"
      },
      "source": [
        "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
        "for i, sentence in enumerate(sequences):\n",
        "    for t, word in enumerate(sentence):\n",
        "        X[i, t, vocab[word]] = 1\n",
        "    y[i, vocab[next_words[i]]] = 1\n",
        "\n",
        "print(np.shape(X))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51166, 20, 4794)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9kAzHF1WOuR"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaYn6zl3WOuS"
      },
      "source": [
        "Now, here come the fun part. The creation of the neural network.\n",
        "As you will see, I am using Keras which provide very good abstraction to design an architecture.\n",
        "\n",
        "In this example, I create the following neural network:\n",
        " - bidirectional LSTM,\n",
        " - with size of 256 and using RELU as activation,\n",
        " - then a dropout layer of 0,6 (it's pretty high, but necesseray to avoid quick divergence)\n",
        " \n",
        "\n",
        "The net should provide me a probability for each word of the vocabulary to be the next one after a given sentence. So I end it with:\n",
        "\n",
        " - a simple dense layer of the size of the vocabulary,\n",
        " - a softmax activation.\n",
        " \n",
        "I use ADAM as otpimizer and the loss calculation is done on the categorical crossentropy.\n",
        "\n",
        "Here is the function to build the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_yzdAy6vWOuS"
      },
      "source": [
        "def bidirectional_lstm_model(seq_length, vocab_size, rnn_size, batch_size, learning_rate):\n",
        "    print('Build LSTM model.')\n",
        "    model = Sequential()\n",
        "    #model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
        "    model.add(Bidirectional(LSTM(rnn_size, activation=\"tanh\", recurrent_activation = \"sigmoid\", recurrent_dropout = 0, use_bias=True,unroll= False),input_shape=(seq_length, vocab_size)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iUluxwv5WOuT"
      },
      "source": [
        "# #Hiper-parametros\n",
        "\n",
        "# rnn_size = 256 # size of RNN\n",
        "# batch_size = 32 # minibatch size\n",
        "# seq_length = seq_length # sequence length\n",
        "# num_epochs = 3 # number of epochs\n",
        "# learning_rate = 0.001 #learning rate\n",
        "# sequences_step = 1 #step to create sequences"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpxNZxQeWOuT"
      },
      "source": [
        "# md = bidirectional_lstm_model(seq_length, vocab_size, rnn_size, batch_size, learning_rate)\n",
        "# print(md.summary())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blH8DiXZWOuT"
      },
      "source": [
        "If a print the summary of this model, you can see it has close to 61 millions of trainable parameters. It is huge, and the compute will take some time to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AFdvB03WOuU"
      },
      "source": [
        "## train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsALVBmdWOuU"
      },
      "source": [
        "Enough speech, we train the model now. We shuffle the training set and extract 10% of it as validation sample. We simply run :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vql6vZJQWOuU"
      },
      "source": [
        "def fit_model(num_epochs, batch):\n",
        "  #fit the model\n",
        "  callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
        "            ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences_lstm.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
        "                            monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
        "\n",
        "  history = md.fit(X, y,\n",
        "                  batch_size=batch,\n",
        "                  shuffle=True,\n",
        "                  epochs=num_epochs,\n",
        "                  callbacks=[callbacks],\n",
        "                  validation_split=0.01)\n",
        "\n",
        "  #save the model\n",
        "  md.save(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')\n",
        "  return history"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do6n9-T3WOuV"
      },
      "source": [
        "# Generate phrase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6OyIAnxWOuW"
      },
      "source": [
        "Great !\n",
        "We have now trained a model to predict the next word of a given sequence of words. In order to generate text, the task is pretty simple:\n",
        "\n",
        " - we define a \"seed\" sequence of 30 words (30 is the number of words required by the neural net for the sequences),\n",
        " - we ask the neural net to predict word number 31,\n",
        " - then we update the sequence by moving words by a step of 1, adding words number 31 at its end,\n",
        " - we ask the neural net to predict word number 32,\n",
        " - etc. For as long as we want.\n",
        " \n",
        "Doing this, we generate phrases, word by word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVht5nitWOuW"
      },
      "source": [
        "def load_vocabulary():\n",
        "  #load vocabulary\n",
        "  print(\"loading vocabulary...\")\n",
        "  vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
        "\n",
        "  with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
        "          words, vocab, vocabulary_inv = cPickle.load(f)\n",
        "\n",
        "  vocab_size = len(words)\n",
        "  return vocab_size"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uTwG2VeWOuW"
      },
      "source": [
        "def load_the_model():\n",
        "  # load the model\n",
        "  print(\"loading model...\")\n",
        "  model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')\n",
        "  return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUc3XgxPWOuX"
      },
      "source": [
        "To improve the word generation, and tune a bit the prediction, we introduce a specific function to pick-up words.\n",
        "\n",
        "We will not take the words with the highest prediction (or the generation of text will be boring), but would like to insert some uncertainties, and let the solution sometime pick-up words with less good prediction.\n",
        "\n",
        "That is the purpose of the function **sample**, that will draw radomly a word from the vocabulary.\n",
        "\n",
        "The probabilty for a word to be drawn will depends directly on its probability to be the next word. In order to tune this probability, we introduce a \"temperature\" to smooth or sharpen its value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Npi--Zf6WOuY"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qHC-i-HWOuY"
      },
      "source": [
        "#initiate sentences\n",
        "def initiate_sentences(sentence, generated):\n",
        "  seed_sentences = 'audrey and shelly sit down in a big room , a cup of coffee on their hands .'\n",
        "  # generated = ''\n",
        "  # sentence = []\n",
        "  for i in range (seq_length):\n",
        "      sentence.append(\"a\")\n",
        "\n",
        "  seed = seed_sentences.split()\n",
        "\n",
        "  for i in range(len(seed)):\n",
        "      sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
        "\n",
        "  generated += ' '.join(sentence)\n",
        "  #print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
        "  return sentence, generated"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2j6_mxuWOuZ"
      },
      "source": [
        "def generate_text(sentence, generated):\n",
        "  words_number = 150\n",
        "  #generate the text\n",
        "  for i in range(words_number):\n",
        "      #create the vector\n",
        "      x = np.zeros((1, seq_length, vocab_size))\n",
        "      for t, word in enumerate(sentence):\n",
        "          x[0, t, vocab[word]] = 1.\n",
        "      #print(x.shape)\n",
        "\n",
        "      #calculate next word\n",
        "      preds = model.predict(x, verbose=0)[0]\n",
        "      next_index = sample(preds, 0.34)\n",
        "      next_word = vocabulary_inv[next_index]\n",
        "\n",
        "      #add the next word to the text\n",
        "      generated += \" \" + next_word\n",
        "      # shift the sentence by one, and and the next word at its end\n",
        "      sentence = sentence[1:] + [next_word]\n",
        "\n",
        "  #print(generated)\n",
        "  return sentence, generated\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOyPBlQwqS8-"
      },
      "source": [
        "def post_process(generated):\n",
        "  generated = generated.replace(\" .\", \".\\n\")\n",
        "  generated = generated.replace(\" )\", \")\")\n",
        "  generated = generated.replace(\"( \", \"(\")\n",
        "  generated = generated.replace(\" '\", \"'\")\n",
        "  generated = generated.replace(\" , \", \", \")\n",
        "  generated = generated.replace(\" : \", \": \")\n",
        "  generated = generated.replace(\" ? \", \"? \")\n",
        "\n",
        "  print(generated)\n",
        "  return generated"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL5YFHqgM4pe"
      },
      "source": [
        "def save_metrics(rnn_size, batch_size, seq_length, num_epochs, learning_rate, sequences_step, history, generated, md):\n",
        "  # Stream the metrics to a file in JSON format.\n",
        "  json_log = open(save_dir + \"/\" +'log' + '_R' + str(rnn_size) + '_B' + str(batch_size) + '_L' + str(learning_rate) + '.json', mode='wt', buffering=1)\n",
        "  json_log.write(\n",
        "          json.dumps({'rnn_size': rnn_size, \n",
        "                      'batch_size': batch_size,\n",
        "                      'seq_length': seq_length, \n",
        "                      'num_epochs': num_epochs, \n",
        "                      'learning_rate': learning_rate, \n",
        "                      'sequences_step': sequences_step, \n",
        "\n",
        "                      'history': history.history,\n",
        "\n",
        "                      'predicted_text': generated\n",
        "          })\n",
        "  )\n",
        "  json_log.close()\n",
        "\n",
        "  # Open the file\n",
        "  with open(save_dir + \"/\" +'model_summary' + '_R' + str(rnn_size) + '_B' + str(batch_size) + '_L' + str(learning_rate) + '.txt', mode='w') as fh:\n",
        "      # Pass the file handle in as a lambda function to make it callable\n",
        "      model.summary(line_length=80,print_fn=lambda x: fh.write(x + '\\n')) "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQVj9yv8UoOC"
      },
      "source": [
        "Combinaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdeYI3Zr_HUs"
      },
      "source": [
        "#Hiper-parametros\n",
        "\n",
        "rnn_size = [128, 256, 512] # size of RNN\n",
        "batch_size = [8, 16, 32, 64] # minibatch size\n",
        "seq_length = seq_length # sequence length\n",
        "num_epochs = 50 # number of epochs\n",
        "learning_rate = [0.01, 0.001] #learning rate\n",
        "sequences_step = 1 #step to create sequences"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_WfnULTUnb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc418b5-09fa-415a-fba0-98c593e1618e"
      },
      "source": [
        "for learning in learning_rate:\n",
        "  for rnn in rnn_size:\n",
        "    for batch in batch_size:\n",
        "      md = bidirectional_lstm_model(seq_length, vocab_size, rnn, batch, learning)\n",
        "      print(md.summary())\n",
        "      history = fit_model(num_epochs, batch)\n",
        "      load_vocabulary()\n",
        "      model = load_the_model()\n",
        "      generated = ''\n",
        "      sentence = []\n",
        "      sentence, generated = initiate_sentences(sentence, generated)\n",
        "      sentence, generated = generate_text(sentence, generated)\n",
        "      generated = post_process(generated)\n",
        "      save_metrics(rnn, batch, seq_length, num_epochs, learning, sequences_step, history, generated, md) "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build LSTM model.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 256)               5041152   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4794)              1232058   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 6,273,210\n",
            "Trainable params: 6,273,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "6332/6332 [==============================] - 69s 8ms/step - loss: 6.3175 - categorical_accuracy: 0.1012 - val_loss: 5.5112 - val_categorical_accuracy: 0.1836\n",
            "Epoch 2/50\n",
            "6332/6332 [==============================] - 50s 8ms/step - loss: 5.3524 - categorical_accuracy: 0.1680 - val_loss: 5.5247 - val_categorical_accuracy: 0.2012\n",
            "Epoch 3/50\n",
            "6332/6332 [==============================] - 50s 8ms/step - loss: 4.9441 - categorical_accuracy: 0.1958 - val_loss: 5.5722 - val_categorical_accuracy: 0.2012\n",
            "Epoch 4/50\n",
            "6332/6332 [==============================] - 50s 8ms/step - loss: 4.5892 - categorical_accuracy: 0.2213 - val_loss: 5.6321 - val_categorical_accuracy: 0.2168\n",
            "Epoch 5/50\n",
            "6332/6332 [==============================] - 50s 8ms/step - loss: 4.3158 - categorical_accuracy: 0.2431 - val_loss: 5.6965 - val_categorical_accuracy: 0.1973\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " audrey (to the phone) i'm not going to get the.\n",
            " josie (his head).\n",
            " truman i do n't - it.\n",
            " i'll get the double r diner - cooper's voice (to the phone) we're going to have to you.\n",
            " truman (a beat) audrey i'm going to be in.\n",
            " truman i do n't know.\n",
            " cooper (,) i know what you do n't.\n",
            " truman you're going to be in the world, i'll be right.\n",
            " cooper (his eyes) sets the him.\n",
            " audrey (he was a little man.\n",
            " cooper i've got to get her.\n",
            " i've got to get a large.\n",
            " truman and the rest of the side.\n",
            " cooper you'll take the what's a little man.\n",
            " but i\n",
            "Build LSTM model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 256)               5041152   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4794)              1232058   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 6,273,210\n",
            "Trainable params: 6,273,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "3166/3166 [==============================] - 34s 10ms/step - loss: 6.3943 - categorical_accuracy: 0.0865 - val_loss: 5.4196 - val_categorical_accuracy: 0.1797\n",
            "Epoch 2/50\n",
            "3166/3166 [==============================] - 30s 10ms/step - loss: 5.7251 - categorical_accuracy: 0.1442 - val_loss: 5.4044 - val_categorical_accuracy: 0.1816\n",
            "Epoch 3/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 4.8716 - categorical_accuracy: 0.1949 - val_loss: 5.3862 - val_categorical_accuracy: 0.1895\n",
            "Epoch 4/50\n",
            "3166/3166 [==============================] - 29s 9ms/step - loss: 4.3038 - categorical_accuracy: 0.2416 - val_loss: 5.4524 - val_categorical_accuracy: 0.1895\n",
            "Epoch 5/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 3.8600 - categorical_accuracy: 0.2793 - val_loss: 5.5066 - val_categorical_accuracy: 0.1953\n",
            "Epoch 6/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 3.4375 - categorical_accuracy: 0.3290 - val_loss: 5.6410 - val_categorical_accuracy: 0.1914\n",
            "Epoch 7/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 3.1574 - categorical_accuracy: 0.3670 - val_loss: 5.6917 - val_categorical_accuracy: 0.1934\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " she's get out of the kitchen.\n",
            " cut to: int.\n",
            " blue pine lodge - night re - establish.\n",
            " a small table, front of the door.\n",
            " cut to: int.\n",
            " sheriff's station reception area - day josie on the house, the door, the funeral.\n",
            " cooper and truman and hawk, i'll be the best and the rest of the world.\n",
            " and truman and truman, sheriff truman and truman, he's here.\n",
            " truman that's a beat.\n",
            " cooper's voice (over) you'll be in the room.\n",
            " truman and truman's voice (the tape) that's a half - the door.\n",
            " cooper, hawk and truman call on the roadhouse.\n",
            " cooper and hawk, sheriff truman, who's the right.\n",
            " cooper i ca n't know.\n",
            "\n",
            "Build LSTM model.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_2 (Bidirection (None, 256)               5041152   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4794)              1232058   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 6,273,210\n",
            "Trainable params: 6,273,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1583/1583 [==============================] - 23s 13ms/step - loss: 6.3523 - categorical_accuracy: 0.0871 - val_loss: 5.2530 - val_categorical_accuracy: 0.1602\n",
            "Epoch 2/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 5.1576 - categorical_accuracy: 0.1747 - val_loss: 5.0816 - val_categorical_accuracy: 0.1895\n",
            "Epoch 3/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 4.6006 - categorical_accuracy: 0.2130 - val_loss: 5.1061 - val_categorical_accuracy: 0.2090\n",
            "Epoch 4/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 4.0931 - categorical_accuracy: 0.2538 - val_loss: 5.1139 - val_categorical_accuracy: 0.2051\n",
            "Epoch 5/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 3.5995 - categorical_accuracy: 0.3082 - val_loss: 5.2497 - val_categorical_accuracy: 0.2051\n",
            "Epoch 6/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 3.1489 - categorical_accuracy: 0.3615 - val_loss: 5.3470 - val_categorical_accuracy: 0.1973\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " a looks up at the back.\n",
            " ed (a little).\n",
            ".. i've got a lot.\n",
            " i think i'll get the funeral.\n",
            " cooper (she closes the door) i'm trying to talk to the funeral.\n",
            " cooper nods, the same object of a dream.\n",
            " truman (he's voice yes, he can, the sheriff is on the speaker phone.\n",
            " cooper albert, i've got a little man.\n",
            " i'm not a place up, the were n't of the world.\n",
            " i'm here.\n",
            " i'm a way of laura.\n",
            " i've been been doing care.\n",
            " i do n't know.\n",
            " i do n't remember.\n",
            " i can give you a new thought.\n",
            " i think it's your husband, i've got a lot of the great northern hotel ,.\n",
            "\n",
            "Build LSTM model.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_3 (Bidirection (None, 256)               5041152   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4794)              1232058   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 6,273,210\n",
            "Trainable params: 6,273,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "792/792 [==============================] - 18s 19ms/step - loss: 6.5254 - categorical_accuracy: 0.0701 - val_loss: 5.3798 - val_categorical_accuracy: 0.1504\n",
            "Epoch 2/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 5.3694 - categorical_accuracy: 0.1522 - val_loss: 5.1370 - val_categorical_accuracy: 0.1973\n",
            "Epoch 3/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 4.7961 - categorical_accuracy: 0.1908 - val_loss: 5.0831 - val_categorical_accuracy: 0.1953\n",
            "Epoch 4/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 4.2796 - categorical_accuracy: 0.2382 - val_loss: 5.0543 - val_categorical_accuracy: 0.2012\n",
            "Epoch 5/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 3.6935 - categorical_accuracy: 0.2962 - val_loss: 5.1332 - val_categorical_accuracy: 0.1816\n",
            "Epoch 6/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 3.1967 - categorical_accuracy: 0.3549 - val_loss: 5.1857 - val_categorical_accuracy: 0.1914\n",
            "Epoch 7/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 2.7531 - categorical_accuracy: 0.4099 - val_loss: 5.3069 - val_categorical_accuracy: 0.1973\n",
            "Epoch 8/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 2.3727 - categorical_accuracy: 0.4698 - val_loss: 5.4853 - val_categorical_accuracy: 0.1934\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " a new pot of coffee, when a hint of a coma.\n",
            " cooper and jerry exit.\n",
            " cut to: ext.\n",
            " sheriff's station - day a small best friend.\n",
            " cooper steps into the kitchen.\n",
            " a large resort, a young man who killed laura palmer, but i'm not going.\n",
            " i've got a man.\n",
            " i've got a going to be together.\n",
            " i want to change.\n",
            " i have to take it.\n",
            " i got a going to know what it's going to get her for a day.\n",
            " i had a couple of - day, i'm not going to be - truman and a couple of days.\n",
            " you're up for a casket, and andy, i'm going to get the day.\n",
            " cooper i'll get it.\n",
            " i had a couple\n",
            "Build LSTM model.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_4 (Bidirection (None, 512)               10344448  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 12,803,770\n",
            "Trainable params: 12,803,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "6332/6332 [==============================] - 77s 12ms/step - loss: 6.5578 - categorical_accuracy: 0.0959 - val_loss: 5.6162 - val_categorical_accuracy: 0.1797\n",
            "Epoch 2/50\n",
            "6332/6332 [==============================] - 73s 12ms/step - loss: 5.3385 - categorical_accuracy: 0.1794 - val_loss: 5.6370 - val_categorical_accuracy: 0.1816\n",
            "Epoch 3/50\n",
            "6332/6332 [==============================] - 74s 12ms/step - loss: 4.7548 - categorical_accuracy: 0.2149 - val_loss: 5.7349 - val_categorical_accuracy: 0.1836\n",
            "Epoch 4/50\n",
            "6332/6332 [==============================] - 74s 12ms/step - loss: 4.1744 - categorical_accuracy: 0.2638 - val_loss: 5.8221 - val_categorical_accuracy: 0.1934\n",
            "Epoch 5/50\n",
            "6332/6332 [==============================] - 74s 12ms/step - loss: 3.6795 - categorical_accuracy: 0.3137 - val_loss: 5.9241 - val_categorical_accuracy: 0.1816\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " cooper (to cooper) bob, it's a beat.\n",
            " cooper (he's voice) i see you can you, i'm gon na get her.\n",
            " cooper (she takes a little).\n",
            ".. she was a smile.\n",
            " james (beat) we're going to have to see you.\n",
            " leo johnson.\n",
            " cooper (to the same) i've got a eye.\n",
            " (beat) i can tell you have to get your on my life.\n",
            " benjamin (and the spirit, i'm not a little.\n",
            " cooper (he sees the phone) oh, i know what i'm not, i want to see you, but and i do n't want to talk about about to you, i have to you? (he finds a little).\n",
            ".. i'm so you\n",
            "Build LSTM model.\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_5 (Bidirection (None, 512)               10344448  \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 12,803,770\n",
            "Trainable params: 12,803,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "3166/3166 [==============================] - 48s 14ms/step - loss: 6.4444 - categorical_accuracy: 0.0935 - val_loss: 5.3207 - val_categorical_accuracy: 0.1992\n",
            "Epoch 2/50\n",
            "3166/3166 [==============================] - 44s 14ms/step - loss: 5.1687 - categorical_accuracy: 0.1814 - val_loss: 5.2149 - val_categorical_accuracy: 0.1855\n",
            "Epoch 3/50\n",
            "3166/3166 [==============================] - 44s 14ms/step - loss: 4.7725 - categorical_accuracy: 0.2113 - val_loss: 5.3093 - val_categorical_accuracy: 0.1953\n",
            "Epoch 4/50\n",
            "3166/3166 [==============================] - 44s 14ms/step - loss: 4.2884 - categorical_accuracy: 0.2559 - val_loss: 5.3864 - val_categorical_accuracy: 0.1973\n",
            "Epoch 5/50\n",
            "3166/3166 [==============================] - 45s 14ms/step - loss: 3.7257 - categorical_accuracy: 0.3061 - val_loss: 5.4197 - val_categorical_accuracy: 0.1797\n",
            "Epoch 6/50\n",
            "3166/3166 [==============================] - 44s 14ms/step - loss: 3.2364 - categorical_accuracy: 0.3611 - val_loss: 5.5545 - val_categorical_accuracy: 0.1895\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " cooper (thinks) i know, i have a funeral.\n",
            " cooper and truman, i'll be afraid.\n",
            " audrey i feel so, i think you can.\n",
            " truman i do n't know the door.\n",
            " truman (to truman) cooper i do n't know what? truman i was in a with the funeral.\n",
            " cooper and truman, hawk, i'm a a couple of a little man.\n",
            " cooper and truman, andy and hawk look at each other.\n",
            " cut to: int.\n",
            " great northern hotel room - day truman, cooper and truman move off, dale cooper, truman, cooper? hawk what do you get a long ?.\n",
            ".. truman, i'll get your, what's you? cooper i've been doing here, i'm not going to be a problem.\n",
            "\n",
            "Build LSTM model.\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_6 (Bidirection (None, 512)               10344448  \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 12,803,770\n",
            "Trainable params: 12,803,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1583/1583 [==============================] - 35s 20ms/step - loss: 6.3752 - categorical_accuracy: 0.0830 - val_loss: 5.2591 - val_categorical_accuracy: 0.1855\n",
            "Epoch 2/50\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 5.1756 - categorical_accuracy: 0.1750 - val_loss: 5.0847 - val_categorical_accuracy: 0.1934\n",
            "Epoch 3/50\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 4.4352 - categorical_accuracy: 0.2297 - val_loss: 5.1610 - val_categorical_accuracy: 0.2051\n",
            "Epoch 4/50\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 3.7502 - categorical_accuracy: 0.2993 - val_loss: 5.2007 - val_categorical_accuracy: 0.1855\n",
            "Epoch 5/50\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 3.0099 - categorical_accuracy: 0.3847 - val_loss: 5.3842 - val_categorical_accuracy: 0.1855\n",
            "Epoch 6/50\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 2.4682 - categorical_accuracy: 0.4686 - val_loss: 5.5050 - val_categorical_accuracy: 0.1914\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " truman and laura palmer, a little man and they look at each other.\n",
            " truman, he's not.\n",
            " i'm going to know it's so.\n",
            " i'm not so much.\n",
            " truman and i'm gon na hurt you - the night, i'm gon na say it, you're gon na hurt you, the tape on the phone.\n",
            " cooper (the word) i'll be right.\n",
            " (to truman) the sound of the funeral.\n",
            " cooper (all shift) i'll be so much.\n",
            " the sheriff's department store.\n",
            " cooper and truman look at each other.\n",
            " truman and cooper look at each other.\n",
            " ben and the rock, i'll be back.\n",
            " donna i have to dance with you, i'll have you know.\n",
            " it was a great northern\n",
            "Build LSTM model.\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_7 (Bidirection (None, 512)               10344448  \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 12,803,770\n",
            "Trainable params: 12,803,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "792/792 [==============================] - 28s 31ms/step - loss: 7.2485 - categorical_accuracy: 0.0634 - val_loss: 5.4089 - val_categorical_accuracy: 0.1641\n",
            "Epoch 2/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 5.4533 - categorical_accuracy: 0.1567 - val_loss: 5.1029 - val_categorical_accuracy: 0.2090\n",
            "Epoch 3/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 4.7432 - categorical_accuracy: 0.2004 - val_loss: 5.0219 - val_categorical_accuracy: 0.1934\n",
            "Epoch 4/50\n",
            "792/792 [==============================] - 23s 30ms/step - loss: 4.1944 - categorical_accuracy: 0.2448 - val_loss: 5.0239 - val_categorical_accuracy: 0.2031\n",
            "Epoch 5/50\n",
            "792/792 [==============================] - 23s 30ms/step - loss: 3.6259 - categorical_accuracy: 0.3012 - val_loss: 5.1047 - val_categorical_accuracy: 0.1816\n",
            "Epoch 6/50\n",
            "792/792 [==============================] - 23s 30ms/step - loss: 3.0261 - categorical_accuracy: 0.3778 - val_loss: 5.2415 - val_categorical_accuracy: 0.1895\n",
            "Epoch 7/50\n",
            "792/792 [==============================] - 23s 30ms/step - loss: 2.4716 - categorical_accuracy: 0.4639 - val_loss: 5.3487 - val_categorical_accuracy: 0.1875\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " a beat.\n",
            " cooper nods, the casket.\n",
            " he takes a gun.\n",
            " he's changed.\n",
            " albert and truman, it is that.\n",
            " cooper what's that? donna i do n't know laura.\n",
            " i'm gon na come up.\n",
            " cooper i'm gon na see it.\n",
            " cooper nods, then: cooper (off his face) yes, i do n't know what i'm going to do.\n",
            " i'm gon na need a small table.\n",
            " cooper you do n't know what you know what you've been here.\n",
            " bobby that's that.\n",
            " truman you're going to think.\n",
            " i've been thinking about it.\n",
            " cooper what's that? cooper how long? donna i'm going to do n't i'm not going to do you know.\n",
            " i'm gon na need to\n",
            "Build LSTM model.\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_8 (Bidirection (None, 1024)              21737472  \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 26,651,322\n",
            "Trainable params: 26,651,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "6332/6332 [==============================] - 136s 21ms/step - loss: 6.5766 - categorical_accuracy: 0.0913 - val_loss: 5.5591 - val_categorical_accuracy: 0.1797\n",
            "Epoch 2/50\n",
            "6332/6332 [==============================] - 132s 21ms/step - loss: 5.4820 - categorical_accuracy: 0.1767 - val_loss: 5.8068 - val_categorical_accuracy: 0.1758\n",
            "Epoch 3/50\n",
            "6332/6332 [==============================] - 132s 21ms/step - loss: 5.0832 - categorical_accuracy: 0.2101 - val_loss: 5.7321 - val_categorical_accuracy: 0.1895\n",
            "Epoch 4/50\n",
            "6332/6332 [==============================] - 132s 21ms/step - loss: 4.4034 - categorical_accuracy: 0.2519 - val_loss: 5.6957 - val_categorical_accuracy: 0.1953\n",
            "Epoch 5/50\n",
            "6332/6332 [==============================] - 132s 21ms/step - loss: 71.8232 - categorical_accuracy: 0.1024 - val_loss: 56.8820 - val_categorical_accuracy: 0.0605\n",
            "loading vocabulary...\n",
            "loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            ".\n",
            " ,.\n",
            "'s.\n",
            ".\n",
            " she a.\n",
            ".\n",
            ".\n",
            ".\n",
            " ,.\n",
            " a a the i.\n",
            ".\n",
            " a.\n",
            " a the.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            " a.\n",
            ".\n",
            ".\n",
            ".\n",
            " cooper the.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ", i.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            " cooper.\n",
            " a.\n",
            ".\n",
            ".\n",
            ".\n",
            " it it.\n",
            ".\n",
            " it.\n",
            ".\n",
            " a a a.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            " cooper.\n",
            " it.\n",
            ".\n",
            " ?.\n",
            ".\n",
            ".\n",
            ".\n",
            " you.\n",
            ".\n",
            ".\n",
            ".\n",
            " a.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            " i.\n",
            ".\n",
            ".\n",
            ".\n",
            " a.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            " i.\n",
            ".\n",
            ".\n",
            " i.\n",
            ".\n",
            ".\n",
            ".\n",
            ", a.\n",
            ".\n",
            " a.\n",
            ".\n",
            " ,.\n",
            " a a.\n",
            " a.\n",
            " a a.\n",
            " a.\n",
            " the\n",
            "Build LSTM model.\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_9 (Bidirection (None, 1024)              21737472  \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 26,651,322\n",
            "Trainable params: 26,651,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "3166/3166 [==============================] - 84s 26ms/step - loss: 8.1289 - categorical_accuracy: 0.0486 - val_loss: 5.4753 - val_categorical_accuracy: 0.1777\n",
            "Epoch 2/50\n",
            "3166/3166 [==============================] - 81s 26ms/step - loss: 5.3189 - categorical_accuracy: 0.1704 - val_loss: 5.3122 - val_categorical_accuracy: 0.1934\n",
            "Epoch 3/50\n",
            "3166/3166 [==============================] - 81s 26ms/step - loss: 4.3918 - categorical_accuracy: 0.2402 - val_loss: 5.3681 - val_categorical_accuracy: 0.1797\n",
            "Epoch 4/50\n",
            "3166/3166 [==============================] - 81s 26ms/step - loss: 3.6126 - categorical_accuracy: 0.3295 - val_loss: 5.5505 - val_categorical_accuracy: 0.1953\n",
            "Epoch 5/50\n",
            "3166/3166 [==============================] - 82s 26ms/step - loss: 2.8923 - categorical_accuracy: 0.4213 - val_loss: 5.7948 - val_categorical_accuracy: 0.1777\n",
            "Epoch 6/50\n",
            "3166/3166 [==============================] - 81s 26ms/step - loss: 2.4324 - categorical_accuracy: 0.4935 - val_loss: 5.8213 - val_categorical_accuracy: 0.1914\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " audrey (to his father) i'm sorry, i've got a lot of the pretty good.\n",
            " audrey (to the father, is that business.\n",
            " i've been in the world ?.\n",
            " i'm not going to see him.\n",
            " he's a beat.\n",
            " cooper (after a beat) i've got a word.\n",
            " cooper takes a long.\n",
            " he (to cooper) i've been here to the sheriff.\n",
            " cooper did n't know what? cooper you're not going to get them? cooper (beat) i did n't see you.\n",
            " bobby i've got a secret.\n",
            " i've got a not an idea of interrogation.\n",
            " cooper i've got a day.\n",
            " cooper got ta think.\n",
            " shelly (he looks in her.\n",
            " cooper (the diner in the diner)\n",
            "Build LSTM model.\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_10 (Bidirectio (None, 1024)              21737472  \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 26,651,322\n",
            "Trainable params: 26,651,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1583/1583 [==============================] - 62s 37ms/step - loss: 6.5593 - categorical_accuracy: 0.0800 - val_loss: 5.2683 - val_categorical_accuracy: 0.1895\n",
            "Epoch 2/50\n",
            "1583/1583 [==============================] - 57s 36ms/step - loss: 5.1186 - categorical_accuracy: 0.1828 - val_loss: 5.1765 - val_categorical_accuracy: 0.1973\n",
            "Epoch 3/50\n",
            "1583/1583 [==============================] - 57s 36ms/step - loss: 4.2324 - categorical_accuracy: 0.2578 - val_loss: 5.1416 - val_categorical_accuracy: 0.1875\n",
            "Epoch 4/50\n",
            "1583/1583 [==============================] - 58s 36ms/step - loss: 3.5631 - categorical_accuracy: 0.3281 - val_loss: 5.3338 - val_categorical_accuracy: 0.1797\n",
            "Epoch 5/50\n",
            "1583/1583 [==============================] - 58s 36ms/step - loss: 2.5620 - categorical_accuracy: 0.4614 - val_loss: 5.5615 - val_categorical_accuracy: 0.1562\n",
            "Epoch 6/50\n",
            "1583/1583 [==============================] - 58s 36ms/step - loss: 2.0275 - categorical_accuracy: 0.5503 - val_loss: 5.7591 - val_categorical_accuracy: 0.1797\n",
            "Epoch 7/50\n",
            "1583/1583 [==============================] - 57s 36ms/step - loss: 1.5615 - categorical_accuracy: 0.6317 - val_loss: 6.0370 - val_categorical_accuracy: 0.1699\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " audrey i'm not afraid of my aunt i do n't know.\n",
            " and i'll get that's your heart.\n",
            " we trembling in a listening to the face, what is the coffee.\n",
            " cooper (hands it) i'm this is doing, i'll check that i've got a friend of the night.\n",
            " james hurley.\n",
            " cooper (he's a voice).\n",
            ".. i can almost hear you.\n",
            " james' voice i just ca n't know what i'd like to do.\n",
            " i feel so bad for you.\n",
            " donna (another kiss) i've been of the sheriff, but we like a emotional sent from donna (andy and get out) i'll be right there, i know, james.\n",
            " james donna, i'll be an all.\n",
            " james you'll be right\n",
            "Build LSTM model.\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_11 (Bidirectio (None, 1024)              21737472  \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 26,651,322\n",
            "Trainable params: 26,651,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "792/792 [==============================] - 50s 59ms/step - loss: 6.5529 - categorical_accuracy: 0.0750 - val_loss: 5.2241 - val_categorical_accuracy: 0.1836\n",
            "Epoch 2/50\n",
            "792/792 [==============================] - 46s 58ms/step - loss: 5.4125 - categorical_accuracy: 0.1692 - val_loss: 5.2400 - val_categorical_accuracy: 0.1992\n",
            "Epoch 3/50\n",
            "792/792 [==============================] - 46s 58ms/step - loss: 4.7468 - categorical_accuracy: 0.2153 - val_loss: 5.0485 - val_categorical_accuracy: 0.2051\n",
            "Epoch 4/50\n",
            "792/792 [==============================] - 46s 58ms/step - loss: 4.1379 - categorical_accuracy: 0.2648 - val_loss: 4.9713 - val_categorical_accuracy: 0.1875\n",
            "Epoch 5/50\n",
            "792/792 [==============================] - 46s 58ms/step - loss: 3.6028 - categorical_accuracy: 0.3193 - val_loss: 5.0368 - val_categorical_accuracy: 0.1934\n",
            "Epoch 6/50\n",
            "792/792 [==============================] - 46s 58ms/step - loss: 3.1913 - categorical_accuracy: 0.3685 - val_loss: 5.0836 - val_categorical_accuracy: 0.2070\n",
            "Epoch 7/50\n",
            "792/792 [==============================] - 46s 58ms/step - loss: 2.8422 - categorical_accuracy: 0.4145 - val_loss: 5.0522 - val_categorical_accuracy: 0.2090\n",
            "Epoch 8/50\n",
            "792/792 [==============================] - 46s 58ms/step - loss: 2.4799 - categorical_accuracy: 0.4713 - val_loss: 5.2524 - val_categorical_accuracy: 0.1914\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " truman and i do n't know.\n",
            " but i think you'll kill him.\n",
            " cooper (after a beat) you know who? cooper how do you do n't know.\n",
            " cooper looks at the phone.\n",
            " josie turns to the door.\n",
            " cooper and truman rise.\n",
            " cooper, you're a big time.\n",
            " i did n't have any of the most, harry.\n",
            " truman (after a beat) i have seen before to late.\n",
            " audrey (a beat) i'm going to talk to you about.\n",
            " i knew it was in her daughter's room.\n",
            " cooper (a whisper) i do n't know.\n",
            " i think it's a problem.\n",
            " i think it's my best friend.\n",
            " i was in the world.\n",
            " i want to talk to you.\n",
            " norma i think it's not\n",
            "Build LSTM model.\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_12 (Bidirectio (None, 256)               5041152   \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 4794)              1232058   \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 6,273,210\n",
            "Trainable params: 6,273,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "6332/6332 [==============================] - 55s 8ms/step - loss: 6.3901 - categorical_accuracy: 0.0712 - val_loss: 5.7226 - val_categorical_accuracy: 0.1133\n",
            "Epoch 2/50\n",
            "6332/6332 [==============================] - 51s 8ms/step - loss: 5.6169 - categorical_accuracy: 0.1210 - val_loss: 5.4370 - val_categorical_accuracy: 0.1680\n",
            "Epoch 3/50\n",
            "6332/6332 [==============================] - 51s 8ms/step - loss: 5.2082 - categorical_accuracy: 0.1621 - val_loss: 5.3865 - val_categorical_accuracy: 0.2012\n",
            "Epoch 4/50\n",
            "6332/6332 [==============================] - 51s 8ms/step - loss: 4.8619 - categorical_accuracy: 0.1965 - val_loss: 5.4443 - val_categorical_accuracy: 0.1973\n",
            "Epoch 5/50\n",
            "6332/6332 [==============================] - 51s 8ms/step - loss: 4.5876 - categorical_accuracy: 0.2199 - val_loss: 5.3790 - val_categorical_accuracy: 0.2129\n",
            "Epoch 6/50\n",
            "6332/6332 [==============================] - 51s 8ms/step - loss: 4.3199 - categorical_accuracy: 0.2443 - val_loss: 5.3804 - val_categorical_accuracy: 0.2070\n",
            "Epoch 7/50\n",
            "6332/6332 [==============================] - 51s 8ms/step - loss: 4.0663 - categorical_accuracy: 0.2788 - val_loss: 5.4870 - val_categorical_accuracy: 0.1953\n",
            "Epoch 8/50\n",
            "6332/6332 [==============================] - 51s 8ms/step - loss: 3.8244 - categorical_accuracy: 0.3080 - val_loss: 5.5094 - val_categorical_accuracy: 0.1836\n",
            "Epoch 9/50\n",
            "6332/6332 [==============================] - 51s 8ms/step - loss: 3.6063 - categorical_accuracy: 0.3367 - val_loss: 5.5105 - val_categorical_accuracy: 0.1973\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " audrey (back to truman) (a beat) i want to see him.\n",
            " cooper (to her) i'm not going to see you.\n",
            " cooper how is this is? cooper you're going to have a long know.\n",
            " cooper's voice (to him) i'm going to be the last night, i'm going to see it.\n",
            " cooper (to her) i'm going to get the wife here, i'm going to see it.\n",
            " she was her.\n",
            " she turns to truman and cooper the last night, you, the wood? james i've been a little man and an okay.\n",
            " they're going to see the \" eileen, we'll be together.\n",
            " he looks at the counter, and james.\n",
            " james enters.\n",
            " donna and turns off the\n",
            "Build LSTM model.\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_13 (Bidirectio (None, 256)               5041152   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 4794)              1232058   \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 6,273,210\n",
            "Trainable params: 6,273,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "3166/3166 [==============================] - 34s 10ms/step - loss: 6.4438 - categorical_accuracy: 0.0663 - val_loss: 5.6522 - val_categorical_accuracy: 0.1035\n",
            "Epoch 2/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 5.6550 - categorical_accuracy: 0.1153 - val_loss: 5.3637 - val_categorical_accuracy: 0.1562\n",
            "Epoch 3/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 5.2744 - categorical_accuracy: 0.1526 - val_loss: 5.2970 - val_categorical_accuracy: 0.1875\n",
            "Epoch 4/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 4.9620 - categorical_accuracy: 0.1788 - val_loss: 5.2616 - val_categorical_accuracy: 0.1855\n",
            "Epoch 5/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 4.6574 - categorical_accuracy: 0.2081 - val_loss: 5.2630 - val_categorical_accuracy: 0.1875\n",
            "Epoch 6/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 4.3878 - categorical_accuracy: 0.2278 - val_loss: 5.3151 - val_categorical_accuracy: 0.2031\n",
            "Epoch 7/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 4.1091 - categorical_accuracy: 0.2596 - val_loss: 5.3128 - val_categorical_accuracy: 0.2012\n",
            "Epoch 8/50\n",
            "3166/3166 [==============================] - 30s 9ms/step - loss: 3.8317 - categorical_accuracy: 0.2907 - val_loss: 5.3877 - val_categorical_accuracy: 0.2070\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " audrey (to the top) a large man, when, the little man, hands a beat.\n",
            " cut to: int.\n",
            " blue pine lodge kitchen - night the room - day in the living room, a red red - moon - cut to: int.\n",
            " blue pine lodge room - day re - establish.\n",
            " cut to: int.\n",
            " blue pine lodge corridor - night josie packard enters the room, sees the phone, catherine.\n",
            ".. josie josie's voice i'm.\n",
            "...\n",
            ".. (to the key) she stops.\n",
            ".. (ben) i'm going to see you, the feel of laura's voice.\n",
            ".. (a little man) i have to be to her, i'm going to be the best of laura was in my life.\n",
            " i do n't know.\n",
            " i think\n",
            "Build LSTM model.\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_14 (Bidirectio (None, 256)               5041152   \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 4794)              1232058   \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 6,273,210\n",
            "Trainable params: 6,273,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1583/1583 [==============================] - 23s 13ms/step - loss: 6.4936 - categorical_accuracy: 0.0695 - val_loss: 5.9105 - val_categorical_accuracy: 0.0781\n",
            "Epoch 2/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 5.9120 - categorical_accuracy: 0.0837 - val_loss: 5.5694 - val_categorical_accuracy: 0.1406\n",
            "Epoch 3/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 5.4847 - categorical_accuracy: 0.1269 - val_loss: 5.3559 - val_categorical_accuracy: 0.1738\n",
            "Epoch 4/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 5.1522 - categorical_accuracy: 0.1602 - val_loss: 5.2378 - val_categorical_accuracy: 0.1797\n",
            "Epoch 5/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 4.8512 - categorical_accuracy: 0.1826 - val_loss: 5.1877 - val_categorical_accuracy: 0.2031\n",
            "Epoch 6/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 4.6201 - categorical_accuracy: 0.2025 - val_loss: 5.1591 - val_categorical_accuracy: 0.1992\n",
            "Epoch 7/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 4.3371 - categorical_accuracy: 0.2241 - val_loss: 5.1815 - val_categorical_accuracy: 0.1914\n",
            "Epoch 8/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 4.0932 - categorical_accuracy: 0.2483 - val_loss: 5.2274 - val_categorical_accuracy: 0.1816\n",
            "Epoch 9/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 3.8274 - categorical_accuracy: 0.2763 - val_loss: 5.2412 - val_categorical_accuracy: 0.2129\n",
            "Epoch 10/50\n",
            "1583/1583 [==============================] - 19s 12ms/step - loss: 3.6008 - categorical_accuracy: 0.3011 - val_loss: 5.3045 - val_categorical_accuracy: 0.1934\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " he starts to her, and searches to his hand, he's voice they hear them.\n",
            " he's voice he's voice (to the) that's good - night.\n",
            " shelly (trying to get) you're going to get to get here.\n",
            " truman i'm going to get the day.\n",
            " cooper i'm gon na get a time.\n",
            " cooper i'm going to see you, harry, it's been.\n",
            " he's a girl.\n",
            " cooper i'm going to get to get from a moment.\n",
            " cooper's voice i'm going to get a little man.\n",
            " cut to: int.\n",
            " sheriff's house - day re - establish.\n",
            " cut to: int.\n",
            " sheriff's station - day re - establish.\n",
            " cut to: int.\n",
            " sheriff's station reception area - day\n",
            "Build LSTM model.\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_15 (Bidirectio (None, 256)               5041152   \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 4794)              1232058   \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 6,273,210\n",
            "Trainable params: 6,273,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "792/792 [==============================] - 18s 19ms/step - loss: 6.5954 - categorical_accuracy: 0.0662 - val_loss: 6.0156 - val_categorical_accuracy: 0.0781\n",
            "Epoch 2/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 6.0583 - categorical_accuracy: 0.0733 - val_loss: 5.7645 - val_categorical_accuracy: 0.0859\n",
            "Epoch 3/50\n",
            "792/792 [==============================] - 15s 18ms/step - loss: 5.7462 - categorical_accuracy: 0.1001 - val_loss: 5.4804 - val_categorical_accuracy: 0.1445\n",
            "Epoch 4/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 5.4198 - categorical_accuracy: 0.1339 - val_loss: 5.2736 - val_categorical_accuracy: 0.1777\n",
            "Epoch 5/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 5.1271 - categorical_accuracy: 0.1600 - val_loss: 5.2162 - val_categorical_accuracy: 0.1875\n",
            "Epoch 6/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 4.8877 - categorical_accuracy: 0.1803 - val_loss: 5.1618 - val_categorical_accuracy: 0.1953\n",
            "Epoch 7/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 4.6668 - categorical_accuracy: 0.1973 - val_loss: 5.1472 - val_categorical_accuracy: 0.2051\n",
            "Epoch 8/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 4.4572 - categorical_accuracy: 0.2126 - val_loss: 5.1741 - val_categorical_accuracy: 0.1973\n",
            "Epoch 9/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 4.2148 - categorical_accuracy: 0.2359 - val_loss: 5.2625 - val_categorical_accuracy: 0.1875\n",
            "Epoch 10/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 4.0222 - categorical_accuracy: 0.2522 - val_loss: 5.2366 - val_categorical_accuracy: 0.2051\n",
            "Epoch 11/50\n",
            "792/792 [==============================] - 14s 18ms/step - loss: 3.8055 - categorical_accuracy: 0.2752 - val_loss: 5.2550 - val_categorical_accuracy: 0.2129\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " a pair of the dalai lama is on a couple of of days? i've got to get a couple of a few days.\n",
            " james i do n't know.\n",
            " i was going to see you? i'm not going to see the funeral.\n",
            " truman and i do n't know.\n",
            " i've got to be a little of a lot.\n",
            " cooper (a drink) i'll have a little of a boat days.\n",
            " cooper (cry) i'm going to be fine.\n",
            " cooper is.\n",
            " cooper (back to truman) i thought about? cooper () you'll be your road.\n",
            " cooper () i think it's here.\n",
            " cooper i know, but i'm going to be a week of you're here, i'm going to talk to you? i want\n",
            "Build LSTM model.\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_16 (Bidirectio (None, 512)               10344448  \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 12,803,770\n",
            "Trainable params: 12,803,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "6332/6332 [==============================] - 78s 12ms/step - loss: 6.3341 - categorical_accuracy: 0.0762 - val_loss: 5.4643 - val_categorical_accuracy: 0.1523\n",
            "Epoch 2/50\n",
            "6332/6332 [==============================] - 75s 12ms/step - loss: 5.3966 - categorical_accuracy: 0.1533 - val_loss: 5.3184 - val_categorical_accuracy: 0.1934\n",
            "Epoch 3/50\n",
            "6332/6332 [==============================] - 74s 12ms/step - loss: 4.9301 - categorical_accuracy: 0.1926 - val_loss: 5.2965 - val_categorical_accuracy: 0.2109\n",
            "Epoch 4/50\n",
            "6332/6332 [==============================] - 74s 12ms/step - loss: 4.5355 - categorical_accuracy: 0.2308 - val_loss: 5.3192 - val_categorical_accuracy: 0.2129\n",
            "Epoch 5/50\n",
            "6332/6332 [==============================] - 75s 12ms/step - loss: 4.1631 - categorical_accuracy: 0.2646 - val_loss: 5.3857 - val_categorical_accuracy: 0.2051\n",
            "Epoch 6/50\n",
            "6332/6332 [==============================] - 75s 12ms/step - loss: 3.7621 - categorical_accuracy: 0.3175 - val_loss: 5.4459 - val_categorical_accuracy: 0.1953\n",
            "Epoch 7/50\n",
            "6332/6332 [==============================] - 75s 12ms/step - loss: 3.4101 - categorical_accuracy: 0.3660 - val_loss: 5.5566 - val_categorical_accuracy: 0.2051\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " they look at each other, then sees a little man, but there's a little man.\n",
            " truman what's the car, harry.\n",
            " cooper (from the phone).\n",
            ".. (rises) yeah ,.\n",
            ".. (to the envelope) yes, the morning, i'm going to be a friend.\n",
            " lucy (from the phone).\n",
            ".. (takes the phone) hawk, i'm going to know the andy, i'm going to be a little man.\n",
            " a little man, there's a beat.\n",
            " cut to: int.\n",
            " sheriff's office - night truman and truman look at each other.\n",
            " cooper looks at the room, but he's not at the window.\n",
            " cooper and truman look at each other.\n",
            " bobby moves off.\n",
            " cut to: ext.\n",
            " sheriff's station\n",
            "Build LSTM model.\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_17 (Bidirectio (None, 512)               10344448  \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 12,803,770\n",
            "Trainable params: 12,803,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "3166/3166 [==============================] - 49s 15ms/step - loss: 6.3817 - categorical_accuracy: 0.0688 - val_loss: 5.5984 - val_categorical_accuracy: 0.1191\n",
            "Epoch 2/50\n",
            "3166/3166 [==============================] - 45s 14ms/step - loss: 5.5339 - categorical_accuracy: 0.1309 - val_loss: 5.2230 - val_categorical_accuracy: 0.1875\n",
            "Epoch 3/50\n",
            "3166/3166 [==============================] - 45s 14ms/step - loss: 5.0509 - categorical_accuracy: 0.1757 - val_loss: 5.1998 - val_categorical_accuracy: 0.1875\n",
            "Epoch 4/50\n",
            "3166/3166 [==============================] - 45s 14ms/step - loss: 4.6375 - categorical_accuracy: 0.2079 - val_loss: 5.1569 - val_categorical_accuracy: 0.2168\n",
            "Epoch 5/50\n",
            "3166/3166 [==============================] - 45s 14ms/step - loss: 4.2474 - categorical_accuracy: 0.2465 - val_loss: 5.1974 - val_categorical_accuracy: 0.2070\n",
            "Epoch 6/50\n",
            "3166/3166 [==============================] - 45s 14ms/step - loss: 3.8491 - categorical_accuracy: 0.2895 - val_loss: 5.2627 - val_categorical_accuracy: 0.2031\n",
            "Epoch 7/50\n",
            "3166/3166 [==============================] - 45s 14ms/step - loss: 3.4494 - categorical_accuracy: 0.3446 - val_loss: 5.3174 - val_categorical_accuracy: 0.1816\n",
            "Epoch 8/50\n",
            "3166/3166 [==============================] - 45s 14ms/step - loss: 3.0724 - categorical_accuracy: 0.3918 - val_loss: 5.4282 - val_categorical_accuracy: 0.1660\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " audrey nods, takes out a small hand.\n",
            " i'm a lot of remembers in the phone.\n",
            " audrey (getting on) i've got a once.\n",
            ".. (to the sketch) i'm gon na get the phone and we have an old - armed man with the phone.\n",
            " he looks up at the room.\n",
            " cooper takes the pill from his pocket.\n",
            " mike it's a little man.\n",
            " cooper moves to the door.\n",
            " cooper did it's the last night? cooper i'm not going to get one of your father.\n",
            " cooper i'm gon na get the right of the last night.\n",
            " cooper i've got a little man, i'm going to see him.\n",
            " truman and it's car, the casket and and wants to see you a couple of the dead.\n",
            "\n",
            "Build LSTM model.\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_18 (Bidirectio (None, 512)               10344448  \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 12,803,770\n",
            "Trainable params: 12,803,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1583/1583 [==============================] - 35s 20ms/step - loss: 6.4391 - categorical_accuracy: 0.0680 - val_loss: 5.8063 - val_categorical_accuracy: 0.0859\n",
            "Epoch 2/50\n",
            "1583/1583 [==============================] - 31s 19ms/step - loss: 5.7009 - categorical_accuracy: 0.1100 - val_loss: 5.3252 - val_categorical_accuracy: 0.1699\n",
            "Epoch 3/50\n",
            "1583/1583 [==============================] - 30s 19ms/step - loss: 5.1679 - categorical_accuracy: 0.1611 - val_loss: 5.1657 - val_categorical_accuracy: 0.1973\n",
            "Epoch 4/50\n",
            "1583/1583 [==============================] - 31s 19ms/step - loss: 4.8219 - categorical_accuracy: 0.1917 - val_loss: 5.1088 - val_categorical_accuracy: 0.1934\n",
            "Epoch 5/50\n",
            "1583/1583 [==============================] - 31s 19ms/step - loss: 4.4218 - categorical_accuracy: 0.2228 - val_loss: 5.0598 - val_categorical_accuracy: 0.1973\n",
            "Epoch 6/50\n",
            "1583/1583 [==============================] - 31s 19ms/step - loss: 4.0561 - categorical_accuracy: 0.2567 - val_loss: 5.0871 - val_categorical_accuracy: 0.2227\n",
            "Epoch 7/50\n",
            "1583/1583 [==============================] - 31s 19ms/step - loss: 3.6288 - categorical_accuracy: 0.3058 - val_loss: 5.2293 - val_categorical_accuracy: 0.2070\n",
            "Epoch 8/50\n",
            "1583/1583 [==============================] - 31s 19ms/step - loss: 3.1769 - categorical_accuracy: 0.3656 - val_loss: 5.4046 - val_categorical_accuracy: 0.1836\n",
            "Epoch 9/50\n",
            "1583/1583 [==============================] - 31s 20ms/step - loss: 2.7629 - categorical_accuracy: 0.4264 - val_loss: 5.6486 - val_categorical_accuracy: 0.1777\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " a small piece of my bed, pulls out the counter, then it's a sitting.\n",
            " cooper it's the night.\n",
            " (rises) i've got a couple of you.\n",
            " cut to: int.\n",
            " sheriff's station - night re - establish.\n",
            " cut to: int.\n",
            " blue pine lodge living room - day truman josie packard, josie packard, josie, josie packard, i'm not going to be a word.\n",
            " josie i'm not not.\n",
            " josie i was not.\n",
            " but pete's voice i'm not a little man.\n",
            " i said the mill was together.\n",
            " laura palmer's body.\n",
            " he was on the hospital.\n",
            " cooper was a man, then: cooper was a little man was murdered.\n",
            " and the great northern? mike that's a new man, the\n",
            "Build LSTM model.\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_19 (Bidirectio (None, 512)               10344448  \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 12,803,770\n",
            "Trainable params: 12,803,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "792/792 [==============================] - 27s 31ms/step - loss: 6.5079 - categorical_accuracy: 0.0669 - val_loss: 5.9475 - val_categorical_accuracy: 0.0781\n",
            "Epoch 2/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 5.8828 - categorical_accuracy: 0.0855 - val_loss: 5.4573 - val_categorical_accuracy: 0.1465\n",
            "Epoch 3/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 5.3667 - categorical_accuracy: 0.1408 - val_loss: 5.1862 - val_categorical_accuracy: 0.1816\n",
            "Epoch 4/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 5.0117 - categorical_accuracy: 0.1738 - val_loss: 5.0984 - val_categorical_accuracy: 0.2051\n",
            "Epoch 5/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 4.6747 - categorical_accuracy: 0.1991 - val_loss: 5.0635 - val_categorical_accuracy: 0.2090\n",
            "Epoch 6/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 4.3534 - categorical_accuracy: 0.2251 - val_loss: 5.0796 - val_categorical_accuracy: 0.2031\n",
            "Epoch 7/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 3.9133 - categorical_accuracy: 0.2675 - val_loss: 5.0716 - val_categorical_accuracy: 0.2090\n",
            "Epoch 8/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 3.4706 - categorical_accuracy: 0.3172 - val_loss: 5.1780 - val_categorical_accuracy: 0.1953\n",
            "Epoch 9/50\n",
            "792/792 [==============================] - 24s 30ms/step - loss: 3.0371 - categorical_accuracy: 0.3735 - val_loss: 5.2987 - val_categorical_accuracy: 0.2051\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " he stops to the nurse.\n",
            " leland palmer, then: it's a certain man, baby, we hear the dance dance.\n",
            " leland leland palmer's arms.\n",
            " leland palmer's voice leland palmer.\n",
            ".. leland palmer palmer dance, leland palmer's arms.\n",
            " leland and leland palmer leland leland palmer, leland palmer's hand.\n",
            " leland palmer's grave.\n",
            " leland palmer's favorite.\n",
            " leland leland's hand.\n",
            " leland leland palmer's dance.\n",
            " leland palmer's death.\n",
            " leland leland palmer's dance.\n",
            " leland leland palmer's dance.\n",
            " leland leland palmer's dance.\n",
            " leland leland's dance, and dance dance.\n",
            " leland dance at him.\n",
            " leland dance leland.\n",
            " leland leland palmer? leland dance dance, dance dance.\n",
            " leland dance with him.\n",
            " leland leland palmer, leland leland palmer, dance dance ,\n",
            "Build LSTM model.\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_20 (Bidirectio (None, 1024)              21737472  \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 26,651,322\n",
            "Trainable params: 26,651,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "6332/6332 [==============================] - 138s 21ms/step - loss: 6.3215 - categorical_accuracy: 0.0754 - val_loss: 5.4800 - val_categorical_accuracy: 0.1602\n",
            "Epoch 2/50\n",
            "6332/6332 [==============================] - 133s 21ms/step - loss: 5.3627 - categorical_accuracy: 0.1569 - val_loss: 5.2761 - val_categorical_accuracy: 0.1973\n",
            "Epoch 3/50\n",
            "6332/6332 [==============================] - 134s 21ms/step - loss: 4.7983 - categorical_accuracy: 0.2068 - val_loss: 5.2855 - val_categorical_accuracy: 0.1934\n",
            "Epoch 4/50\n",
            "6332/6332 [==============================] - 134s 21ms/step - loss: 4.2629 - categorical_accuracy: 0.2627 - val_loss: 5.2754 - val_categorical_accuracy: 0.2207\n",
            "Epoch 5/50\n",
            "6332/6332 [==============================] - 134s 21ms/step - loss: 3.6728 - categorical_accuracy: 0.3352 - val_loss: 5.3164 - val_categorical_accuracy: 0.2129\n",
            "Epoch 6/50\n",
            "6332/6332 [==============================] - 136s 22ms/step - loss: 3.0588 - categorical_accuracy: 0.4206 - val_loss: 5.7515 - val_categorical_accuracy: 0.1738\n",
            "Epoch 7/50\n",
            "6332/6332 [==============================] - 137s 22ms/step - loss: 2.4864 - categorical_accuracy: 0.5094 - val_loss: 5.8679 - val_categorical_accuracy: 0.1719\n",
            "Epoch 8/50\n",
            "6332/6332 [==============================] - 135s 21ms/step - loss: 2.0085 - categorical_accuracy: 0.5869 - val_loss: 6.0678 - val_categorical_accuracy: 0.1641\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " truman and a small key.\n",
            " pete takes a long as.\n",
            " they all a small of the woman.\n",
            " the log lady one the sheriff truman and it's only.\n",
            " one of the tell you i have to have.\n",
            " cooper that's right.\n",
            " he's closer.\n",
            " cooper i'm going to have a funeral.\n",
            " cooper where's my brother,? cooper i want to see you - truman it's time.\n",
            " cooper harry, i'll get the find out.\n",
            " hawk what's the sheriff? truman they're going to find out.\n",
            " cut to: ext.\n",
            " renault's apartment - night truman and hawk look at each other, a lot of the car is off.\n",
            " a renault's apartment - night that's a stop.\n",
            " leland palmer.\n",
            " i'm the sorry.\n",
            " truman i\n",
            "Build LSTM model.\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_21 (Bidirectio (None, 1024)              21737472  \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 26,651,322\n",
            "Trainable params: 26,651,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "3166/3166 [==============================] - 87s 27ms/step - loss: 6.3996 - categorical_accuracy: 0.0696 - val_loss: 5.6234 - val_categorical_accuracy: 0.1133\n",
            "Epoch 2/50\n",
            "3166/3166 [==============================] - 84s 26ms/step - loss: 5.4995 - categorical_accuracy: 0.1345 - val_loss: 5.1759 - val_categorical_accuracy: 0.1758\n",
            "Epoch 3/50\n",
            "3166/3166 [==============================] - 84s 27ms/step - loss: 4.8302 - categorical_accuracy: 0.1957 - val_loss: 5.0809 - val_categorical_accuracy: 0.2051\n",
            "Epoch 4/50\n",
            "3166/3166 [==============================] - 84s 26ms/step - loss: 4.2640 - categorical_accuracy: 0.2456 - val_loss: 5.2022 - val_categorical_accuracy: 0.1953\n",
            "Epoch 5/50\n",
            "3166/3166 [==============================] - 85s 27ms/step - loss: 3.6378 - categorical_accuracy: 0.3121 - val_loss: 5.2937 - val_categorical_accuracy: 0.1973\n",
            "Epoch 6/50\n",
            "3166/3166 [==============================] - 84s 27ms/step - loss: 3.0211 - categorical_accuracy: 0.3912 - val_loss: 5.4880 - val_categorical_accuracy: 0.1875\n",
            "Epoch 7/50\n",
            "3166/3166 [==============================] - 85s 27ms/step - loss: 2.4894 - categorical_accuracy: 0.4677 - val_loss: 5.8611 - val_categorical_accuracy: 0.1582\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " he tries to her eyes, trying to sound on a television.\n",
            " \" invitation to love \" jared, it's a couple of days.\n",
            " i'm not going to have to the funeral of my own.\n",
            " audrey (after a beat) i know you know, i've been here, i know, i've got a lot of a couple of days ago, i'll get the hell out of my way.\n",
            " truman (taking it) not not a way of the door.\n",
            " cooper (after a beat) i'll get your life and you know? cooper i do n't know.\n",
            " i've got a couple of the best - thank you, norma.\n",
            " i'm not a way of the other half - bobby (shakes out) what did you mean? \"\n",
            "Build LSTM model.\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_22 (Bidirectio (None, 1024)              21737472  \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 26,651,322\n",
            "Trainable params: 26,651,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1583/1583 [==============================] - 63s 38ms/step - loss: 6.3816 - categorical_accuracy: 0.0734 - val_loss: 5.5849 - val_categorical_accuracy: 0.1348\n",
            "Epoch 2/50\n",
            "1583/1583 [==============================] - 58s 37ms/step - loss: 5.4683 - categorical_accuracy: 0.1384 - val_loss: 5.2394 - val_categorical_accuracy: 0.1738\n",
            "Epoch 3/50\n",
            "1583/1583 [==============================] - 59s 37ms/step - loss: 4.9034 - categorical_accuracy: 0.1845 - val_loss: 4.9703 - val_categorical_accuracy: 0.2129\n",
            "Epoch 4/50\n",
            "1583/1583 [==============================] - 59s 37ms/step - loss: 4.3959 - categorical_accuracy: 0.2300 - val_loss: 5.0128 - val_categorical_accuracy: 0.2051\n",
            "Epoch 5/50\n",
            "1583/1583 [==============================] - 59s 37ms/step - loss: 3.8226 - categorical_accuracy: 0.2894 - val_loss: 5.0419 - val_categorical_accuracy: 0.2070\n",
            "Epoch 6/50\n",
            "1583/1583 [==============================] - 59s 37ms/step - loss: 3.2254 - categorical_accuracy: 0.3630 - val_loss: 5.1474 - val_categorical_accuracy: 0.2227\n",
            "Epoch 7/50\n",
            "1583/1583 [==============================] - 59s 37ms/step - loss: 2.6231 - categorical_accuracy: 0.4486 - val_loss: 5.2697 - val_categorical_accuracy: 0.1953\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " truman and cooper look at each other.\n",
            " bobby i know it.\n",
            " i do n't know, i think it's like this is it.\n",
            " (a beat) i know who's up.\n",
            " donna (to the little) i've been one of the night? truman i thought you'd be a little man with the little man and his eyes.\n",
            " cooper and you look at each other.\n",
            " hawk takes a look at the other.\n",
            " he smiles.\n",
            " truman look at the other.\n",
            " leo is a look at the other.\n",
            " bobby i want to see you in the car? cut to: ext.\n",
            " sheriff's station - day re - establish.\n",
            " cut to: int.\n",
            " bobby's house - day re - establish.\n",
            " cut to: int.\n",
            " sheriff's station - day re\n",
            "Build LSTM model.\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_23 (Bidirectio (None, 1024)              21737472  \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 26,651,322\n",
            "Trainable params: 26,651,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "792/792 [==============================] - 52s 61ms/step - loss: 6.4720 - categorical_accuracy: 0.0676 - val_loss: 5.9290 - val_categorical_accuracy: 0.0820\n",
            "Epoch 2/50\n",
            "792/792 [==============================] - 46s 59ms/step - loss: 5.7107 - categorical_accuracy: 0.1084 - val_loss: 5.3023 - val_categorical_accuracy: 0.1582\n",
            "Epoch 3/50\n",
            "792/792 [==============================] - 47s 60ms/step - loss: 5.0907 - categorical_accuracy: 0.1686 - val_loss: 5.1096 - val_categorical_accuracy: 0.1992\n",
            "Epoch 4/50\n",
            "792/792 [==============================] - 47s 60ms/step - loss: 4.5957 - categorical_accuracy: 0.2118 - val_loss: 5.0047 - val_categorical_accuracy: 0.1895\n",
            "Epoch 5/50\n",
            "792/792 [==============================] - 47s 59ms/step - loss: 4.0234 - categorical_accuracy: 0.2616 - val_loss: 5.0876 - val_categorical_accuracy: 0.1973\n",
            "Epoch 6/50\n",
            "792/792 [==============================] - 47s 59ms/step - loss: 3.4384 - categorical_accuracy: 0.3255 - val_loss: 5.0511 - val_categorical_accuracy: 0.2070\n",
            "Epoch 7/50\n",
            "792/792 [==============================] - 47s 59ms/step - loss: 2.8165 - categorical_accuracy: 0.4105 - val_loss: 5.5018 - val_categorical_accuracy: 0.1777\n",
            "Epoch 8/50\n",
            "792/792 [==============================] - 47s 59ms/step - loss: 2.1645 - categorical_accuracy: 0.5173 - val_loss: 5.6947 - val_categorical_accuracy: 0.1582\n",
            "loading vocabulary...\n",
            "loading model...\n",
            "a a audrey and shelly sit down in a big room, a cup of coffee on their hands.\n",
            " every little man.\n",
            " he's clean, but there are a little man and runs off.\n",
            " benjamin lifts the rock and takes it into the air.\n",
            " any any man.\n",
            " he moves to the door.\n",
            " open it's a police of it, it.\n",
            " james it's good, you're n't.\n",
            " it's wrong.\n",
            " it was me.\n",
            " he was a little man in her eyes.\n",
            " major briggs and son.\n",
            " bobby briggs.\n",
            " mike what's the hell? leo that's here.\n",
            " bobby that's right.\n",
            " there was a little man.\n",
            " he was n't supposed to talk to us.\n",
            " he was a little man runs her hand and heads it.\n",
            " mike, bobby.\n",
            " bobby you know what? james i got to want to talk to you about it? bobby i'm\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}