{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "bidirectional-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuFernandez/twin-peaks-generator/blob/master/bi_lstm_refactored.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IZzJr-AWOuC"
      },
      "source": [
        "# Text Generation using Bidirectional LSTM and Doc2Vec models\n",
        "\n",
        "The purpose of [this article](https://medium.com/@david.campion/text-generation-using-bidirectional-lstm-and-doc2vec-models-1-3-8979eb65cb3a) is to discuss about text generation, using machine learning approaches, especially neural networks.\n",
        "\n",
        "It is not the first article about it, and probably not the last. Actually, there is a lot of litterature about text generation using \"AI\" techniques, and some codes are available to generate texts from existing novels, trying to create new chapters for **\"Game of Thrones\"**, **\"Harry Potter\"**, or a new piece in the style of **Shakespears**. Sometimes with interesting results.\n",
        "\n",
        "Mainly, these approaches are using classic LSTM networks, and the are pretty fun to be experimented.\n",
        "\n",
        "However, generated texts provide a taste of unachievement. Generated sentences seems quite right, whith correct grammar and syntax, as if the neural network was understanding correctly the structure of a sentence. But the whole new text does not have great sense. If it is not complete nosense. \n",
        "\n",
        "This problem could come from the approach itself, using only LSTM to generate text word by word. But how can we improve them ? In this article, I will try to investigate a new way to generate sentences.\n",
        "\n",
        "It does not mean that I will use something completely different from LTSM : I am not, I will use LTSM network to generate sequences of words. However I will try to go further than a classic LSTM neural network and I will use an additional neural network (LSTM again), to select the best phrases.\n",
        "\n",
        "Then, this article can be used as a tutorial. It describes :\n",
        " 1. **how to train a neural network to generate sentences** (i.e. sequences of words), based on existing novels. I will use a bidirectional LSTM Architecture to perform that.\n",
        " 2. **how to train a neural network to select the best next sentence for given paragraph** (i.e. a sequence of sentences). I will also use a bidirectional LSTM archicture, in addition to a Doc2Vec model of the target novels.\n",
        "\n",
        "\n",
        "### Note about Data inputs\n",
        "As data inputs, I will not use texts which are not free in term of intellectual properties. So I will not train the solution to create a new chapter for **\"Game of Throne\"** or **\"Harry Potter\"**.\n",
        "Sorry about that, there is plenty of \"free\" text to perform such texts generation exercices and we can dive into the [Gutemberg project](http://www.gutenberg.org), which provides huge amount of texts (from [William Shakespears](http://www.gutenberg.org/ebooks/author/65) to [H.P. Lovecraft](http://www.gutenberg.org/ebooks/author/34724), or other great authors).\n",
        "\n",
        "However, I am also a french author of fantasy and Science fiction. So I will use my personnal material to create a new chapter of my stories, hoping it can help me in my next work!\n",
        "\n",
        "So, I will base this exercice on **\"Artistes et Phalanges\"**, a french fantasy novel I wrote over the 10 past years, wich I hope will be fair enough in term of data inputs. It contains more than 830 000 charaters.\n",
        "\n",
        "By the way, if you're a french reader and found of fantasy, you can find it on iBook store and Amazon Kindle for free... Please note I provide also the data for free on my github repository. Enjoy it!\n",
        "\n",
        "## 1. a Neural Network for Generating Sentences\n",
        "\n",
        "The first step is to generate sentences in the style of a given author.\n",
        "\n",
        "There is huge litterature about it, espacially using LSTM to perform such task. As this kind of network are working well for this job, we will use them.\n",
        "\n",
        "The purpose of this note is not to deep dive into LSTM description, you can find very great article about them and I suggest you to read [this article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) from Andrej Karpathy.\n",
        "\n",
        "You can also find easily existing code to perform text generation using LSTM. On my github, you can find two tutorials, one using [Tensorflow](https://github.com/campdav/text-rnn-tensorflow), and another one using [Keras](https://github.com/campdav/text-rnn-keras) (over tensorflow), that is easier to understand.\n",
        "\n",
        "For this first part of these exercice, I will re-use these materials, but with few improvements :\n",
        " - Instead of a simple _LSTM_, I will use a _bidirectional LSTM_. This network configuration converge faster than a single LSTM (less epochs are required), and from empiric tests, seems better in term of accuracy. You can have a look at [this article](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/) from Jason Brownlee, for a good tutorial about bidirectional LSTM.\n",
        " - I will use Keras, which require less complexity to create the network of is more readible than conventional Tensorflow code.\n",
        "\n",
        "### 1.1. What is the neural network task in our case ?\n",
        "\n",
        "LSTM (Long Short Term Memory) are very good for analysing sequences of values and predicting the next values from them. For example, LSTM could be a very good choice if you want to predict the very next point of a given time serie (assuming a correlation exist in the sequence).\n",
        "\n",
        "Talking about sentences and texts ; phrases (sentences) are basically sequences of words. So, it is natural to assume LSTM could be usefull to generate the next word of a given sentence.\n",
        "\n",
        "In summary, the objective of a LSTM neural network in this situation is to guess the next word of a given sentence.\n",
        "\n",
        "For example:\n",
        "What is the next word of this following sentence : \"he is walking down the\"\n",
        "\n",
        "Our neural net will take the sequence of words as input : \"he\", \"is\", \"walking\", ...\n",
        "Its ouput will be a matrix providing the probability for each word from the dictionnary to be the next one of the given sentence.\n",
        "\n",
        "Then, how will we build the complete text ? Simply iterating the process, by switching the setence by one word, including the new guessed word at its end. Then, we guess a new word for this new sentence. ad vitam aeternam.\n",
        "\n",
        "### 1.1.1. Process\n",
        "\n",
        "In order to do that, first, we build a dictionary containing all words from the novels we want to use.\n",
        "\n",
        " 1. read the data (the novels we want to use),\n",
        " 1. create the dictionnary of words,\n",
        " 2. create the list of sentences,\n",
        " 3. create the neural network,\n",
        " 4. train the neural network,\n",
        " 5. generate new sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzDrR6o24gX1",
        "outputId": "3e379f67-5544-44a2-b48f-ab85b32f86b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74f1O2AYWOuH"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, Input, Flatten, Bidirectional\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
        "from keras.metrics import categorical_accuracy\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "from six.moves import cPickle\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWZS9VRdTs0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed6297f-6d2e-464f-ae58-497d249d91e7"
      },
      "source": [
        "#chequeo estar usando la gpu y me fijo cuál\n",
        "import tensorflow as tf \n",
        "print(tf.test.gpu_device_name())\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11987701422839952186\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14509932544\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 16623545501371189505\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQlhpYLWOuI"
      },
      "source": [
        "We have raw text and a lot of things have to be done to use them: split them in words list, etc.\n",
        "In order to do that, I use the spacy library which is incredible to deal with texts. For this exercice, I will only use very few options from spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tBCLWPcZWOuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad88fbb-c123-4e52-a602-dd992a58aa77"
      },
      "source": [
        "#import spacy, and english model\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A66-e91WOuJ"
      },
      "source": [
        "# parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "608Hi9WdwsUj"
      },
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "tz_bsas = pytz.timezone('America/Buenos_Aires') \n",
        "datetime_bsas = datetime.now(tz_bsas)\n",
        "date = datetime_bsas.strftime(\"%Y-%m-%d-%Hh_%Mm\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "V6S-FjXjWOuJ"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/twin-peaks-generator/data'# data directory containing input.txt\n",
        "save_dir = '/content/drive/MyDrive/twin-peaks-generator/save/' + date # directory to store models\n",
        "os.mkdir(save_dir)\n",
        "seq_length = 30 # sequence length\n",
        "sequences_step = 1 #step to create sequences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GSGnk4ayWOuK"
      },
      "source": [
        "# file_list = [\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\"]\n",
        "file_list = [\"101\",\"102\",\"103\",\"104\"]\n",
        "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLi_ZvgkWOuK"
      },
      "source": [
        "# read data\n",
        "\n",
        "I create a specific function to create a list of words from raw text. I use spacy library, with a specific function to retrieve only lower character of the words and remove carriage returns (\\n).\n",
        "\n",
        "I am doing that because I want to reduce the number of potential words in my dictionnary, and I assume we do not have to avoid capital letters. Indeed, they are only part of the syntax of the text, it's shape, and do not deals with its sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CLpVmxTGWOuK"
      },
      "source": [
        "def create_wordlist(doc):\n",
        "    wl = []\n",
        "    for word in doc:\n",
        "        if word.text not in (\"\\n\",\"\\n\\n\", '\\n\\n\\n', '\\n\\n\\n\\n', '\\n\\n\\n\\n\\n','\\u2009','\\xa0'):\n",
        "            wl.append(word.text.lower())\n",
        "    return wl"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwp-SqxEWOuL"
      },
      "source": [
        "Create the list of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7tSqMUNfWOuL"
      },
      "source": [
        "wordlist = []\n",
        "for file_name in file_list:\n",
        "    input_file = os.path.join(data_dir, file_name + \".txt\")\n",
        "    #read data\n",
        "    with codecs.open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "    #create sentences\n",
        "    doc = nlp(data)\n",
        "    wl = create_wordlist(doc)\n",
        "    wordlist = wordlist + wl"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiSXxyky7HYB",
        "outputId": "68a1e362-c35e-45a5-abce-8c061e4ccaed"
      },
      "source": [
        "len(wordlist)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1CobyAiWOuM"
      },
      "source": [
        "## Create dictionary\n",
        "\n",
        "The first step is to create the dictionnary, it means, the list of all words contained in texts. For each word, we will assign an index to it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_g7kc5RWOuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5e6be1-d911-46cf-98ab-327d37f75d5b"
      },
      "source": [
        "# count the number of words\n",
        "word_counts = collections.Counter(wordlist)\n",
        "\n",
        "# Mapping from index to word : that's the vocabulary\n",
        "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "\n",
        "# Mapping from word to index\n",
        "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "words = [x[0] for x in word_counts.most_common()]\n",
        "\n",
        "#size of the vocabulary\n",
        "vocab_size = len(words)\n",
        "print(\"vocab size: \", vocab_size)\n",
        "\n",
        "#save the words and vocabulary\n",
        "with open(os.path.join(vocab_file), 'wb') as f:\n",
        "    cPickle.dump((words, vocab, vocabulary_inv), f)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  4794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNDKcN_ZndB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d1ddad-4bbb-40d7-a530-403e975cf466"
      },
      "source": [
        "print(vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, '!': 1, '\"': 2, '#': 3, '%': 4, \"'\": 5, \"'bout\": 6, \"'d\": 7, \"'em\": 8, \"'ll\": 9, \"'m\": 10, \"'re\": 11, \"'s\": 12, \"'ve\": 13, '(': 14, ')': 15, ',': 16, '-': 17, '-(stay': 18, '--': 19, '--and': 20, '--i': 21, '--jerry': 22, '--no': 23, '--sun': 24, '-james': 25, '-norma': 26, '-sweetie': 27, '.': 28, '..': 29, '...': 30, '.32': 31, '/': 32, '1': 33, '100': 34, '12': 35, '12:27': 36, '12:30': 37, '18': 38, '19': 39, '1942': 40, '1950': 41, '1959': 42, '1988': 43, '1989': 44, '1:18': 45, '22': 46, '2:24': 47, '3': 48, '315': 49, '36-footer': 50, '52': 51, '5th': 52, '64': 53, '6:18': 54, '8:17': 55, '9': 56, '9/4': 57, ':': 58, ';': 59, '?': 60, 'a': 61, 'a--': 62, 'a.f.o.': 63, 'a.m.': 64, 'a.s.a.p.': 65, 'abandoned': 66, 'abiding': 67, 'abject': 68, 'able': 69, 'about': 70, 'above': 71, 'abruptly': 72, 'absent': 73, 'absentmindedly': 74, 'absolute': 75, 'absolutely': 76, 'accept': 77, 'access': 78, 'accident': 79, 'accidental': 80, 'accommodations': 81, 'accompanied': 82, 'account': 83, 'accounts': 84, 'accurate': 85, 'acetylcholine': 86, 'achieving': 87, 'aching': 88, 'acids': 89, 'acknowledge': 90, 'acquaintance': 91, 'acquainted': 92, 'across': 93, 'acrylic': 94, 'act': 95, 'acting': 96, 'action': 97, 'actions': 98, 'activated': 99, 'actually': 100, 'add': 101, 'addicted': 102, 'addictive': 103, 'adding': 104, 'addition': 105, 'addressed': 106, 'adds': 107, 'admire': 108, 'advance': 109, 'advancing': 110, 'adversity': 111, 'advice': 112, 'advised': 113, 'affairs': 114, 'affected': 115, 'affection': 116, 'afford': 117, 'afraid': 118, 'after': 119, 'afternoon': 120, 'afternoons': 121, 'afterwards': 122, 'again': 123, 'against': 124, 'age': 125, 'aged': 126, 'agent': 127, 'aggressiveness': 128, 'agnostic': 129, 'ago': 130, 'agonizing': 131, 'agreed': 132, 'ah': 133, 'ahead': 134, 'ahh': 135, 'ahoooooooooooh': 136, 'aid': 137, 'aim': 138, 'air': 139, 'airplane': 140, 'alarm': 141, 'alarmed': 142, 'alarmingly': 143, 'albert': 144, 'alert': 145, 'algebra': 146, 'alibi': 147, 'alike': 148, 'all': 149, 'allows': 150, 'almost': 151, 'alone': 152, 'along': 153, 'aloud': 154, 'alphabetically': 155, 'already': 156, 'alright': 157, 'also': 158, 'altercation': 159, 'aluminum': 160, 'always': 161, 'always-': 162, 'am': 163, 'amateur': 164, 'amazed': 165, 'amazement': 166, 'amazing': 167, 'ambient': 168, 'ambiguously': 169, 'ambitions': 170, 'amen': 171, 'amidst': 172, 'ammo': 173, 'ammunition': 174, 'among': 175, 'amount': 176, 'amphetamines': 177, 'amused': 178, 'amusement': 179, 'an': 180, 'ancient': 181, 'and': 182, 'andrew': 183, 'andwarrants': 184, 'andy': 185, 'angel': 186, 'anger': 187, 'angle': 188, 'angrily': 189, 'angry': 190, 'animal': 191, 'ankles': 192, 'annette': 193, 'announcer': 194, 'annual': 195, 'another': 196, 'answer': 197, 'answered': 198, 'answers': 199, 'anticipate': 200, 'antlers': 201, 'anxiously': 202, 'any': 203, 'anybody': 204, 'anymore': 205, 'anyone': 206, 'anything': 207, 'anyway': 208, 'anywhere': 209, 'apart': 210, 'apartment': 211, 'apb': 212, 'apiece': 213, 'apparently': 214, 'appear': 215, 'appears': 216, 'appointment': 217, 'appreciate': 218, 'appreciates': 219, 'appreciation': 220, 'appreciatively': 221, 'approach': 222, 'approaching': 223, 'appropriate': 224, 'approval': 225, 'approximating': 226, 'april': 227, 'apron': 228, 'arcs': 229, 'ardent': 230, 'ardently': 231, 'are': 232, 'area': 233, 'argument': 234, 'aria': 235, 'arky': 236, 'arm': 237, 'armed': 238, 'arms': 239, 'armstrong': 240, 'around': 241, 'arrange': 242, 'arranged': 243, 'arrangements': 244, 'arrest': 245, 'arrive': 246, 'arrived': 247, 'arrives': 248, 'arriving': 249, 'arrow': 250, 'arson': 251, 'art': 252, 'arteries': 253, 'artificial': 254, 'artist': 255, 'as': 256, 'ascot': 257, 'ashamed': 258, 'ashore': 259, 'ashtray': 260, 'aside': 261, 'ask': 262, 'asked': 263, 'asleep': 264, 'aspiration': 265, 'ass': 266, 'assault': 267, 'assaulted': 268, 'assembly': 269, 'assent': 270, 'assist': 271, 'assistance': 272, 'assistant': 273, 'assistants': 274, 'assisted': 275, 'associates': 276, 'association': 277, 'assume': 278, 'assuming': 279, 'astonished': 280, 'at': 281, 'atavistic': 282, 'ate': 283, 'athlete': 284, 'atmosphere': 285, 'attached': 286, 'attaches': 287, 'attacked': 288, 'attempt': 289, 'attended': 290, 'attention': 291, 'attired': 292, 'attitude': 293, 'attorney': 294, 'attractive': 295, 'audio': 296, 'audrey': 297, 'aunt': 298, 'autodialer': 299, 'automobiles': 300, 'autopsy': 301, 'available': 302, 'awaiting': 303, 'away': 304, 'awe': 305, 'awful': 306, 'awhile': 307, 'awkward': 308, 'awoke': 309, 'axe': 310, 'b': 311, 'b.g': 312, 'baby': 313, 'back': 314, 'background': 315, 'backhands': 316, 'backing': 317, 'backlit': 318, 'backup': 319, 'backward': 320, 'backwater': 321, 'bacon': 322, 'bad': 323, 'badge': 324, 'badges': 325, 'badly': 326, 'bag': 327, 'bags': 328, 'baguette': 329, 'bail': 330, 'ball': 331, 'ballpark': 332, 'balls': 333, 'band': 334, 'bandage': 335, 'bang': 336, 'bank': 337, 'bankrupt': 338, 'bankruptcy': 339, 'banks': 340, 'banner': 341, 'baptized': 342, 'bar': 343, 'bare': 344, 'barely': 345, 'bark': 346, 'barks': 347, 'baron': 348, 'barring': 349, 'bars': 350, 'bartender': 351, 'base': 352, 'basement': 353, 'basket': 354, 'bastard': 355, 'bath': 356, 'bathrobe': 357, 'bathroom': 358, 'be': 359, 'beach': 360, 'beam': 361, 'beaming': 362, 'beams': 363, 'bear': 364, 'bearclaw': 365, 'bearclaws': 366, 'beast': 367, 'beat': 368, 'beats': 369, 'beautiful': 370, 'beauty': 371, 'beaver': 372, 'because': 373, 'beclouded': 374, 'become': 375, 'bed': 376, 'bedroom': 377, 'beds': 378, 'bedside': 379, 'bedspread': 380, 'been': 381, 'beep': 382, 'beer': 383, 'beers': 384, 'beestung': 385, 'before': 386, 'begging': 387, 'begin': 388, 'beginning': 389, 'begins': 390, 'beguiling': 391, 'begun': 392, 'behalf': 393, 'behave': 394, 'behavior': 395, 'behind': 396, 'beige': 397, 'being': 398, 'believe': 399, 'believed': 400, 'believes': 401, 'believeth': 402, 'bell': 403, 'bellhops': 404, 'belong': 405, 'belongs': 406, 'below': 407, 'ben': 408, 'bench': 409, 'bend': 410, 'bends': 411, 'beneath': 412, 'benefit': 413, 'benefits': 414, 'benignly': 415, 'benjamin': 416, 'bent': 417, 'berg': 418, 'bernard': 419, 'bernie': 420, 'beside': 421, 'besides': 422, 'best': 423, 'bestsmelling': 424, 'bet': 425, \"bet'cha\": 426, 'betrayed': 427, 'better': 428, 'betty': 429, 'between': 430, 'beyond': 431, 'bicker': 432, 'bidets': 433, 'big': 434, 'biggest': 435, 'bike': 436, 'billy': 437, 'bind': 438, 'binds': 439, 'binocular': 440, 'binoculars': 441, 'bird': 442, 'birdcall': 443, 'birds': 444, 'bit': 445, 'bite': 446, 'bites': 447, 'biting': 448, 'bitter': 449, 'black': 450, 'blackboard': 451, 'blackfeet': 452, 'blackfoot': 453, 'blackie': 454, 'blah': 455, 'blanches': 456, 'blank': 457, 'blanket': 458, 'bless': 459, 'blessed': 460, 'blessings': 461, 'blew': 462, 'blindly': 463, 'blinds': 464, 'blinking': 465, 'blinks': 466, 'blithering': 467, 'block': 468, 'blocks': 469, 'blood': 470, 'bloodstain': 471, 'bloodstained': 472, 'bloody': 473, 'bloom': 474, 'blow': 475, 'blowing': 476, 'blows': 477, 'blue': 478, 'blueprints': 479, 'blues': 480, 'blushes': 481, 'blushing': 482, 'board': 483, 'boat': 484, 'bob': 485, 'bobby': 486, 'bobcat': 487, 'body': 488, 'bogus': 489, 'boiler': 490, 'bold': 491, 'bolt': 492, 'bombshell': 493, 'boob': 494, 'book': 495, 'bookhouse': 496, 'books': 497, 'bookshelf': 498, 'bookshelves': 499, 'booth': 500, 'boots': 501, 'bop': 502, 'bopper': 503, 'border': 504, 'bordered': 505, 'bored': 506, 'born': 507, 'boss': 508, 'both': 509, 'bother': 510, 'bothering': 511, 'bottle': 512, 'bottom': 513, 'bouffant': 514, 'bought': 515, 'bound': 516, 'bounds': 517, 'bouquet': 518, 'bourbon': 519, 'bow': 520, 'bowl': 521, 'box': 522, 'boxer': 523, 'boxes': 524, 'boy': 525, 'boyfriend': 526, 'boys': 527, 'bracelet': 528, 'brag': 529, 'brains': 530, 'brakelights': 531, 'brakes': 532, 'branch': 533, 'brand': 534, 'brandeis': 535, 'brando': 536, 'brass': 537, 'brassy': 538, 'brave': 539, 'brawl': 540, 'brays': 541, 'break': 542, 'breakfast': 543, 'breaking': 544, 'breaks': 545, 'breast': 546, 'breath': 547, 'breathe': 548, 'breathless': 549, 'breed': 550, 'breeze': 551, 'breezes': 552, 'breezily': 553, 'brennan': 554, 'brewing': 555, 'brewskis': 556, 'brie': 557, 'brief': 558, 'briefly': 559, 'briefs': 560, 'brigade': 561, 'briggs': 562, 'bright': 563, 'brightening': 564, 'brightens': 565, 'brightly': 566, 'brilliant': 567, 'bring': 568, 'bringing': 569, 'brings': 570, 'broke': 571, 'broken': 572, 'brother': 573, 'brothers': 574, 'brought': 575, 'brown': 576, 'bruised': 577, 'bruises': 578, 'brushing': 579, 'brutally': 580, 'buck': 581, 'bucket': 582, 'bucks': 583, 'buddhism': 584, 'buds': 585, 'buffalo': 586, 'bugging': 587, 'building': 588, 'built': 589, 'bulbs': 590, 'bulgarian': 591, 'bulky': 592, 'bulldogged': 593, 'bullets': 594, 'bumblebee': 595, 'bumpkin': 596, 'bureau': 597, 'burgs': 598, 'burial': 599, 'buried': 600, 'burly': 601, 'burma': 602, 'burned': 603, 'burning': 604, 'burnished': 605, 'burns': 606, 'burrows': 607, 'burst': 608, 'bursts': 609, 'bury': 610, 'burying': 611, 'business': 612, 'businessman': 613, 'bust': 614, 'bustling': 615, 'busy': 616, 'but': 617, 'butt': 618, 'butte': 619, 'butter': 620, 'buttkicked': 621, 'button': 622, 'buttons': 623, 'buy': 624, 'buzz': 625, 'buzzes': 626, 'buzzing': 627, 'by': 628, 'bzzzzz': 629, 'c': 630, 'ca': 631, 'cab': 632, 'cabinet': 633, 'cable': 634, 'cajole': 635, 'cake': 636, 'calculate': 637, 'calculated': 638, 'calhoun': 639, 'call': 640, 'called': 641, 'calling': 642, 'calls': 643, 'calm': 644, 'calmly': 645, 'came': 646, 'camera': 647, 'can': 648, 'canada': 649, 'canadian': 650, 'candidly': 651, 'candies': 652, 'candlelight': 653, 'candles': 654, 'candy': 655, 'canuck': 656, 'capable': 657, 'captain': 658, 'captured': 659, 'car': 660, 'card': 661, 'cards': 662, 'care': 663, 'cared': 664, 'career': 665, 'careful': 666, 'carefully': 667, 'caressing': 668, 'caretaker': 669, 'carpet': 670, 'carry': 671, 'carrying': 672, 'cars': 673, 'cartridge': 674, 'carving': 675, 'case': 676, 'cases': 677, 'cash': 678, 'cashews': 679, 'casino': 680, 'casket': 681, 'caskets': 682, 'cassette': 683, 'catch': 684, 'catches': 685, 'catching': 686, 'catherine': 687, 'catty': 688, 'caught': 689, 'cause': 690, 'cautiously': 691, 'ceases': 692, 'ceiling': 693, 'cell': 694, 'cemetery': 695, 'center': 696, 'centuries': 697, 'ceremony': 698, 'certain': 699, 'certainly': 700, 'chain': 701, 'chair': 702, 'chairing': 703, 'chairs': 704, 'chalk': 705, 'chambermaids': 706, 'champion-(takes': 707, 'chance': 708, 'chances': 709, 'change': 710, 'changed': 711, 'changing': 712, 'chaos': 713, 'character': 714, 'characterize': 715, 'characters': 716, 'charge': 717, 'charged': 718, 'charging': 719, 'charity': 720, 'charm': 721, 'charming': 722, 'chasing': 723, 'chat': 724, 'check': 725, 'checkbook': 726, 'checked': 727, 'checking': 728, 'cheek': 729, 'cheer': 730, 'cheerleader': 731, 'cherry': 732, 'chest': 733, 'chet': 734, 'chew': 735, 'chewing': 736, 'chicken': 737, 'chickenfeed': 738, 'child': 739, 'childhood': 740, 'children': 741, 'chin': 742, 'china': 743, 'chinese': 744, 'chip': 745, 'chips': 746, 'chocolate': 747, 'choice': 748, 'choices': 749, 'chomps': 750, 'choose': 751, 'chop': 752, 'chopping': 753, 'chowderhead': 754, 'christ': 755, 'chuckle': 756, 'church': 757, 'cider': 758, 'cigar': 759, 'cigarette': 760, 'cigarettes': 761, 'circle': 762, 'circles': 763, 'circumstance': 764, 'circus': 765, 'citizens': 766, 'city': 767, 'clad': 768, 'clang': 769, 'clanks': 770, 'claps': 771, 'clarence': 772, 'class': 773, 'classroom': 774, 'classy': 775, 'claw': 776, 'clay': 777, 'clean': 778, 'cleaner': 779, 'cleaning': 780, 'cleans': 781, 'clear': 782, 'cleared': 783, 'clearing': 784, 'clears': 785, 'clench': 786, 'cliches': 787, 'cliches\"-cooper': 788, 'client': 789, 'climbs': 790, 'clinic': 791, 'clink': 792, 'clip': 793, 'clock': 794, 'clocking': 795, 'close': 796, 'closed': 797, 'closer': 798, 'closes': 799, 'closest': 800, 'closet': 801, 'closing': 802, 'closure': 803, 'clothes': 804, 'clouds': 805, 'club': 806, 'clubhouse': 807, 'clucking': 808, 'clues': 809, 'clutch': 810, 'clutches': 811, 'clutching': 812, 'coast': 813, 'coat': 814, 'coated': 815, 'cocaine': 816, 'cocks': 817, 'coconut': 818, 'coconuts': 819, 'code': 820, 'coexisting': 821, 'coffee': 822, 'coffees': 823, 'coffin': 824, 'coin': 825, 'coincidence': 826, 'coital': 827, 'cold': 828, 'cole': 829, 'collapses': 830, 'collar': 831, 'colleague': 832, 'collectively': 833, 'collects': 834, 'colleen': 835, 'college': 836, 'collides': 837, 'colonel': 838, 'colt': 839, 'coma': 840, 'combat': 841, 'come': 842, 'comes': 843, 'comfort': 844, 'comfortable': 845, 'comforted': 846, 'comforting': 847, 'comforts': 848, 'comin': 849, 'coming': 850, 'comments': 851, 'commercial': 852, 'commitment': 853, 'common': 854, 'commotion': 855, 'communist': 856, 'community': 857, 'company': 858, 'compare': 859, 'compartment': 860, 'compassion': 861, 'compassionate': 862, 'completely': 863, 'completing': 864, 'complexion': 865, 'composing': 866, 'composure': 867, 'compulsively': 868, 'computer': 869, 'concentrate': 870, 'concern': 871, 'concerning': 872, 'conclude': 873, 'concluded': 874, 'conclusion': 875, 'concussion': 876, 'condition': 877, 'conducting': 878, 'conference': 879, 'confide': 880, 'confident': 881, 'confidential': 882, 'confidentiality': 883, 'confidentially': 884, 'confirm': 885, 'confiscate': 886, 'confounds': 887, 'confronts': 888, 'confused': 889, 'confusion': 890, 'congenital': 891, 'connected': 892, 'connection': 893, 'connections': 894, 'consider': 895, 'considerably': 896, 'consideration': 897, 'considers': 898, 'consistent': 899, 'console': 900, 'consoles': 901, 'constitute': 902, 'construction': 903, 'consulting': 904, 'consults': 905, 'contact': 906, 'contain': 907, 'containing': 908, 'contemplative': 909, 'contents': 910, 'continue': 911, 'continues': 912, 'continuing': 913, 'contract': 914, 'contrariness': 915, 'contribute': 916, 'contributing': 917, 'control': 918, 'controls': 919, 'convenience': 920, 'convent': 921, 'conversation': 922, 'convert': 923, 'convince': 924, 'convinced': 925, 'cook': 926, 'cookie': 927, 'cooking': 928, 'cool': 929, 'coop': 930, 'cooper': 931, 'coordination': 932, 'coots': 933, 'corn': 934, 'corner': 935, 'correct': 936, 'correctly': 937, 'corridor': 938, 'corridors': 939, 'corvette': 940, 'cosmetics': 941, 'cost': 942, 'costs': 943, 'cot': 944, 'cots': 945, 'cottages': 946, 'cotton': 947, 'couch': 948, 'could': 949, 'counsel': 950, 'count': 951, 'counter': 952, 'counties': 953, 'countries': 954, 'country': 955, 'couple': 956, 'couples': 957, 'course': 958, 'cousin': 959, 'covered': 960, 'covers': 961, 'cowboy': 962, 'cozy': 963, 'cracker': 964, 'cracking': 965, 'cradling': 966, 'crap': 967, 'crash': 968, 'crazy': 969, 'cream': 970, 'creams': 971, 'credit': 972, 'credits': 973, 'creepy': 974, 'cremate': 975, 'cretins': 976, 'cries': 977, 'crime': 978, 'criminal': 979, 'crisis': 980, 'crisp': 981, 'crispy': 982, 'crop': 983, 'cross': 984, 'crosses': 985, 'crossing': 986, 'crouched': 987, 'crouches': 988, 'crowd': 989, 'crows': 990, 'cruching': 991, 'cruel': 992, 'cruise': 993, 'cruiser': 994, 'crusty': 995, 'cry': 996, 'crying': 997, 'cryptically': 998, 'cube': 999, 'cuff': 1000, 'culture': 1001, 'cup': 1002, 'cupped': 1003, 'cups': 1004, 'curious': 1005, 'current': 1006, 'curtain': 1007, 'curtains': 1008, 'custody': 1009, 'custom': 1010, 'customer': 1011, 'customers': 1012, 'customized': 1013, 'customs': 1014, 'cut': 1015, 'cute': 1016, 'cutest': 1017, 'cuts': 1018, 'cutting': 1019, 'cycle': 1020, 'dad': 1021, 'daddy': 1022, 'dalai': 1023, 'dale': 1024, 'damaged': 1025, 'damn': 1026, 'dance': 1027, 'dancer': 1028, 'dances': 1029, 'dancing': 1030, 'dandy': 1031, 'danger': 1032, 'dangerous': 1033, 'dark': 1034, 'darkens': 1035, 'darkness': 1036, 'darlene': 1037, 'darling': 1038, 'darn': 1039, 'dart': 1040, 'date': 1041, 'daughter': 1042, 'daughters': 1043, 'dawn': 1044, 'day': 1045, 'days': 1046, 'de': 1047, 'dead': 1048, 'deafening': 1049, 'deal': 1050, 'dealing': 1051, 'deals': 1052, 'dealt': 1053, 'dear': 1054, 'death': 1055, 'debonair': 1056, 'debt': 1057, 'decade': 1058, 'deceitful': 1059, 'decency': 1060, 'decent': 1061, 'decide': 1062, 'decided': 1063, 'decides': 1064, 'decision': 1065, 'decked': 1066, 'deckhand': 1067, 'declines': 1068, 'dedicated': 1069, 'deductive': 1070, 'deep': 1071, 'deepest': 1072, 'deeply': 1073, 'defending': 1074, 'defense': 1075, 'defensive': 1076, 'definitely': 1077, 'definition': 1078, 'degree': 1079, 'deleted': 1080, 'delicate': 1081, 'delight': 1082, 'delivered': 1083, 'delivering': 1084, 'delivery': 1085, 'demonstrate': 1086, 'demonstrates': 1087, 'demonstrating': 1088, 'demoralized': 1089, 'dental': 1090, 'department': 1091, 'departmental': 1092, 'departs': 1093, 'depend': 1094, 'depends': 1095, 'deposit': 1096, 'deputies': 1097, 'deputy': 1098, 'der': 1099, 'descend': 1100, 'descends': 1101, 'describe': 1102, 'describing': 1103, 'description': 1104, 'design': 1105, 'desire': 1106, 'desk': 1107, 'despair': 1108, 'desperate': 1109, 'desperately': 1110, 'despite': 1111, 'destination': 1112, 'destroy': 1113, 'destroyed': 1114, 'detached': 1115, 'detailed': 1116, 'determined': 1117, 'devilish': 1118, 'devotion': 1119, 'dialogue': 1120, 'dials': 1121, 'diane': 1122, 'diary': 1123, 'did': 1124, 'die': 1125, 'died': 1126, 'dies': 1127, 'diesel': 1128, 'diet': 1129, 'dieth': 1130, 'difference': 1131, 'different': 1132, 'difficult': 1133, 'difficulty': 1134, 'dig': 1135, 'digestive': 1136, 'digging': 1137, 'dignity': 1138, 'digs': 1139, \"dimm'd\": 1140, 'diner': 1141, 'dining': 1142, 'dinner': 1143, 'dinners': 1144, 'dips': 1145, 'direct': 1146, 'directed': 1147, 'direction': 1148, 'directions': 1149, 'directly': 1150, 'directs': 1151, 'dirt': 1152, 'dirty': 1153, 'disappear': 1154, 'disappoint': 1155, 'disappointed': 1156, 'disaster': 1157, 'discards': 1158, 'discernable': 1159, 'discharges': 1160, 'discourages': 1161, 'discover': 1162, 'discovered': 1163, 'discovers': 1164, 'discovery': 1165, 'discreet': 1166, 'discreetly': 1167, 'discretely': 1168, 'discuss': 1169, 'discussed': 1170, 'discussion': 1171, 'diseased': 1172, 'disguised': 1173, 'dishes': 1174, 'dismay': 1175, 'disperse': 1176, 'displayed': 1177, 'displays': 1178, 'displeased': 1179, 'dissolve': 1180, 'dissolved': 1181, 'distance': 1182, 'distant': 1183, 'distinctive': 1184, 'distinguished': 1185, 'distorted': 1186, 'distress': 1187, 'distressed': 1188, 'distributes': 1189, 'disturb': 1190, 'disturbed': 1191, 'disturbing': 1192, 'division': 1193, 'divorce': 1194, 'do': 1195, 'doc': 1196, 'dock': 1197, 'doctor': 1198, 'document': 1199, 'does': 1200, 'does--': 1201, 'dog': 1202, 'doggone': 1203, 'doing': 1204, 'dollar': 1205, 'dollars': 1206, 'dolts': 1207, 'domino': 1208, \"don't\": 1209, \"don't-(screams\": 1210, 'donald': 1211, 'done': 1212, 'donna': 1213, 'donut': 1214, 'donuts': 1215, 'door': 1216, 'doorbell': 1217, 'doors': 1218, 'doorway': 1219, 'doozy': 1220, 'dots': 1221, 'double': 1222, 'doublecross': 1223, 'doubles': 1224, 'doubt': 1225, 'doughnut': 1226, 'douglas': 1227, 'douses': 1228, 'down': 1229, 'downcast': 1230, 'downstairs': 1231, 'downtown': 1232, 'dr': 1233, 'dr.': 1234, 'drag': 1235, 'drags': 1236, 'drain': 1237, 'drape': 1238, 'draped': 1239, 'drapes': 1240, 'draw': 1241, 'drawer': 1242, 'drawers': 1243, 'drawing': 1244, 'drawn': 1245, 'draws': 1246, 'dream': 1247, 'dreamed': 1248, 'dreams': 1249, 'dress': 1250, 'dressed': 1251, 'dresser': 1252, 'dressing': 1253, 'drifts': 1254, 'drink': 1255, 'drinking': 1256, 'drinks': 1257, 'drips': 1258, 'drive': 1259, 'driver': 1260, 'drivers': 1261, 'drives': 1262, 'driveway': 1263, 'drivin': 1264, 'driving': 1265, 'drop': 1266, 'dropped': 1267, 'drops': 1268, 'drove': 1269, 'drug': 1270, 'drugged': 1271, 'drugs': 1272, 'drunk': 1273, 'dry': 1274, 'dubious': 1275, 'duck': 1276, 'ducks': 1277, 'dug': 1278, 'dull': 1279, 'dullards': 1280, 'dumb': 1281, 'dumbbells': 1282, 'dumps': 1283, 'dumpster': 1284, 'dunces': 1285, 'duration': 1286, 'during': 1287, 'dusk': 1288, 'dustbuster': 1289, 'dusters': 1290, 'dusty': 1291, 'duty': 1292, 'dwells': 1293, 'dying': 1294, 'e': 1295, 'each': 1296, 'eager': 1297, 'eagle': 1298, 'ear': 1299, 'earlier': 1300, 'early': 1301, 'ears': 1302, 'earth': 1303, 'easier': 1304, 'east': 1305, 'easy': 1306, 'eat': 1307, 'eating': 1308, 'econo': 1309, 'ecstatic': 1310, 'ed': 1311, 'edge': 1312, 'edges': 1313, 'edsker': 1314, 'education': 1315, 'effect': 1316, 'efficiency': 1317, 'effortlessly': 1318, 'egg': 1319, 'eggnog': 1320, 'eggs': 1321, 'eight': 1322, 'eighteen': 1323, 'eighty': 1324, 'eileen': 1325, 'either': 1326, 'eject': 1327, 'el': 1328, 'elaborate': 1329, 'elbow': 1330, 'elderly': 1331, 'electronic': 1332, 'elegant': 1333, 'elevator': 1334, 'elk': 1335, 'else': 1336, 'elvis': 1337, 'embedded': 1338, 'embrace': 1339, 'embraces': 1340, 'emerald': 1341, 'emerge': 1342, 'emerges': 1343, 'emotional': 1344, 'emotions': 1345, 'empire': 1346, 'empties': 1347, 'empty': 1348, 'en': 1349, 'encounter': 1350, 'encourage': 1351, 'encouraging': 1352, 'end': 1353, 'endeavors': 1354, 'ends': 1355, 'endures': 1356, 'engage': 1357, 'engine': 1358, 'english': 1359, 'engrossed': 1360, 'enigma': 1361, 'enigmatically': 1362, 'enjoy': 1363, 'enjoys': 1364, 'enlisting': 1365, 'enormous': 1366, 'enough': 1367, 'enter': 1368, 'entering': 1369, 'enters': 1370, 'entertaining': 1371, 'enthusiastic': 1372, 'entire': 1373, 'entirely': 1374, 'entrance': 1375, 'entrust': 1376, 'entry': 1377, 'envelope': 1378, 'episode': 1379, 'erase': 1380, 'erupting': 1381, 'escape': 1382, 'escort': 1383, 'escorted': 1384, 'escorts': 1385, 'especially': 1386, 'establish': 1387, 'established': 1388, 'establishing': 1389, 'establishment': 1390, 'estate': 1391, 'estates': 1392, 'estimated': 1393, 'eternal': 1394, 'evacuation': 1395, 'even': 1396, 'evening': 1397, 'event': 1398, 'events': 1399, 'eventually': 1400, 'ever': 1401, 'every': 1402, 'everybody': 1403, 'everyone': 1404, 'everything': 1405, 'evidence': 1406, 'evil': 1407, 'ex': 1408, 'exacting': 1409, 'exactly': 1410, 'examine': 1411, 'examined': 1412, 'examining': 1413, 'example': 1414, 'exasperated': 1415, 'excellent': 1416, 'except': 1417, 'exception': 1418, 'exchange': 1419, 'excited': 1420, 'excitement': 1421, 'excuse': 1422, 'execute': 1423, 'exercise': 1424, 'exertion': 1425, 'exhales': 1426, 'exhausted': 1427, 'exile': 1428, 'existence': 1429, 'exit': 1430, 'exiting': 1431, 'exits': 1432, 'expand': 1433, 'expanding': 1434, 'expands': 1435, 'expect': 1436, 'expecting': 1437, 'expects': 1438, 'expensive': 1439, 'experience': 1440, 'expert': 1441, 'expertly': 1442, 'explain': 1443, 'explaining': 1444, 'expletive': 1445, 'explodes': 1446, 'exploit': 1447, 'explosion': 1448, 'express': 1449, 'expressing': 1450, 'expression': 1451, 'ext': 1452, 'extra': 1453, 'extraordinary': 1454, 'extremely': 1455, 'eye': 1456, 'eyed': 1457, 'eyeing': 1458, 'eyes': 1459, 'f': 1460, 'face': 1461, 'faced': 1462, 'faceless': 1463, 'facial': 1464, 'facilities': 1465, 'facing': 1466, 'fact': 1467, 'facts': 1468, 'fade': 1469, 'faded': 1470, 'fades': 1471, 'failing': 1472, 'failure': 1473, 'fair': 1474, 'fairvale': 1475, 'fake': 1476, 'falcon': 1477, 'fall': 1478, 'falling': 1479, 'falls': 1480, 'false': 1481, 'familiar': 1482, 'family': 1483, 'famished': 1484, 'fanfare': 1485, 'fantastic': 1486, 'far': 1487, 'faraway': 1488, 'farm': 1489, 'fascinated': 1490, 'fascinating': 1491, 'fashioned': 1492, 'fast': 1493, 'fastened': 1494, 'fate': 1495, 'father': 1496, 'favorite': 1497, 'favors': 1498, 'fax': 1499, 'faxing': 1500, 'fbi': 1501, 'fear': 1502, 'fears': 1503, 'february': 1504, 'federal': 1505, 'feed': 1506, 'feel': 1507, 'feeling': 1508, 'feelings': 1509, 'feels': 1510, 'feet': 1511, 'feigned': 1512, 'fell': 1513, 'fella': 1514, 'fellas': 1515, 'fellow': 1516, 'felonies': 1517, 'felt': 1518, 'female': 1519, 'ferguson': 1520, 'fervor': 1521, 'few': 1522, 'fi': 1523, 'fibers': 1524, 'fielding': 1525, 'fifteen': 1526, 'fifth': 1527, 'fifties': 1528, 'fifty': 1529, 'fight': 1530, 'fighting': 1531, 'fights': 1532, 'figure': 1533, 'figured': 1534, 'figures': 1535, 'figuring': 1536, 'file': 1537, 'filed': 1538, 'files': 1539, 'filing': 1540, 'fill': 1541, 'filled': 1542, 'filling': 1543, 'filters': 1544, 'filthy': 1545, 'final': 1546, 'finally': 1547, 'find': 1548, 'finds': 1549, 'fine': 1550, 'finger': 1551, 'fingers': 1552, 'finish': 1553, 'finished': 1554, 'finishes': 1555, 'finishing': 1556, 'finley': 1557, 'fir': 1558, 'fire': 1559, 'firearms': 1560, 'fireplace': 1561, 'fires': 1562, 'firing': 1563, 'firm': 1564, 'firmly': 1565, 'firmness': 1566, 'first': 1567, 'fish': 1568, 'fishin': 1569, 'fishing': 1570, 'fists': 1571, 'fitfully': 1572, 'fitting': 1573, 'fitzgerald': 1574, 'five': 1575, 'fix': 1576, 'flak': 1577, 'flannel': 1578, 'flapping': 1579, 'flash': 1580, 'flashing': 1581, 'flashlight': 1582, 'flashy': 1583, 'flask': 1584, 'flat': 1585, 'flee': 1586, 'flies': 1587, 'flight': 1588, 'flings': 1589, 'flips': 1590, 'floor': 1591, 'floss': 1592, 'flourish': 1593, 'flower': 1594, 'flowered': 1595, 'flowers': 1596, 'flowing': 1597, 'flu': 1598, 'fluffy': 1599, 'fluorescents': 1600, 'flurry': 1601, 'flushed': 1602, 'flustered': 1603, 'fly': 1604, 'flying': 1605, 'focuses': 1606, 'focusing': 1607, 'folded': 1608, 'folder': 1609, 'folding': 1610, 'folds': 1611, 'follow': 1612, 'followed': 1613, 'following': 1614, 'follows': 1615, 'food': 1616, 'fool': 1617, 'fooling': 1618, 'fools': 1619, 'foot': 1620, 'football': 1621, 'footsteps': 1622, 'for': 1623, 'force': 1624, 'forced': 1625, 'forebrain': 1626, 'forehead': 1627, 'forensics': 1628, 'forest': 1629, 'forever': 1630, 'forget': 1631, 'forgive': 1632, 'forgot': 1633, 'forgotten': 1634, 'fork': 1635, 'forlorn': 1636, 'form': 1637, 'formaldehyde': 1638, 'forms': 1639, 'forth': 1640, 'fortress': 1641, 'fortune': 1642, 'forty': 1643, 'forward': 1644, 'forwards': 1645, 'found': 1646, 'four': 1647, 'foursome': 1648, 'fragment': 1649, 'frailty': 1650, 'frame': 1651, 'framed': 1652, 'francisco': 1653, 'frantically': 1654, 'frappe': 1655, 'fraud': 1656, 'fray': 1657, 'free': 1658, 'freeze': 1659, 'french': 1660, 'fresh': 1661, 'freshly': 1662, 'friday': 1663, 'fried': 1664, 'friend': 1665, 'friendly': 1666, 'friends': 1667, 'frightened': 1668, 'frightening': 1669, 'frog': 1670, 'from': 1671, 'front': 1672, 'frowns': 1673, 'frozen': 1674, 'fruit': 1675, 'fruits': 1676, 'frustrated': 1677, 'fulfill': 1678, 'full': 1679, 'fully': 1680, 'fumbles': 1681, 'fun': 1682, 'fund': 1683, 'funeral': 1684, 'funerals': 1685, 'funny': 1686, 'furious': 1687, 'furiously': 1688, 'furnished': 1689, 'further': 1690, 'fury': 1691, 'future': 1692, 'futures': 1693, 'gag': 1694, 'gained': 1695, 'gaining': 1696, 'gal': 1697, 'gallon': 1698, 'galvanized': 1699, 'games': 1700, 'gamut': 1701, 'gang': 1702, 'garage': 1703, 'gas': 1704, 'gasps': 1705, 'gassed': 1706, 'gate': 1707, 'gates': 1708, 'gather': 1709, 'gathered': 1710, 'gathering': 1711, 'gave': 1712, 'gaze': 1713, 'gazing': 1714, 'gear': 1715, 'gee': 1716, 'geez': 1717, 'generic': 1718, 'genial': 1719, 'genius': 1720, 'gent': 1721, 'gentle': 1722, 'gentlemen': 1723, 'gently': 1724, 'genus': 1725, 'gerard': 1726, 'gesture': 1727, 'gestures': 1728, 'get': 1729, 'gets': 1730, 'getting': 1731, 'ghostwood': 1732, 'giant': 1733, 'gifted': 1734, 'giggle': 1735, 'giggles': 1736, 'gimme': 1737, 'ginger': 1738, 'girl': 1739, 'girlfriend': 1740, 'girls': 1741, 'give': 1742, 'gives': 1743, 'givin': 1744, 'giving': 1745, 'glad': 1746, 'gladly': 1747, 'glance': 1748, 'glances': 1749, 'glancing': 1750, 'glare': 1751, 'glares': 1752, 'glass': 1753, 'glasses': 1754, 'glassine': 1755, 'gleaming': 1756, 'gleefully': 1757, 'glides': 1758, 'glimpse': 1759, 'glistening': 1760, 'gloats': 1761, 'globe': 1762, 'glove': 1763, 'gloved': 1764, 'gloves': 1765, 'glowers': 1766, 'glowing': 1767, 'glue': 1768, 'glued': 1769, 'glumly': 1770, 'go': 1771, 'god': 1772, 'god--': 1773, 'godforsaken': 1774, 'godspeed': 1775, 'goes': 1776, \"goin'\": 1777, 'going': 1778, 'gold': 1779, 'golden': 1780, 'golf': 1781, 'gon': 1782, 'gone': 1783, 'good': 1784, 'goodbye': 1785, 'goodnight': 1786, 'gordon': 1787, 'gorgeous': 1788, 'gossip': 1789, 'got': 1790, 'gotten': 1791, 'gown': 1792, 'grab': 1793, 'grabbing': 1794, 'grabs': 1795, 'grade': 1796, 'gradually': 1797, 'graduate': 1798, 'grand': 1799, 'granite': 1800, 'granted': 1801, 'grapefruit': 1802, 'grapefruits': 1803, 'grasp': 1804, 'grass': 1805, 'grave': 1806, 'graves': 1807, 'gravesite': 1808, 'grease': 1809, 'greased': 1810, 'greasy': 1811, 'great': 1812, 'greater': 1813, 'greatly': 1814, 'green': 1815, 'greets': 1816, 'grew': 1817, 'grey': 1818, 'griddlecake': 1819, 'griddlecakes': 1820, 'grief': 1821, 'grime': 1822, 'grin': 1823, 'grinds': 1824, 'grins': 1825, 'grips': 1826, 'groaning': 1827, 'groceries': 1828, 'grocery': 1829, 'ground': 1830, 'grounds': 1831, 'group': 1832, \"grow'st\": 1833, 'growing': 1834, 'growl': 1835, 'grows': 1836, 'guaranteed': 1837, 'guarantees': 1838, 'guard': 1839, 'guess': 1840, 'guesses': 1841, 'guests': 1842, 'gum': 1843, 'gums': 1844, 'gun': 1845, 'gunplay': 1846, 'guns': 1847, 'gust': 1848, 'gutter': 1849, 'guy': 1850, 'guys': 1851, 'h': 1852, 'ha': 1853, 'habit': 1854, 'habits': 1855, 'had': 1856, 'hail': 1857, 'hair': 1858, 'haircut': 1859, 'hairdo': 1860, 'haired': 1861, 'half': 1862, 'hall': 1863, 'hallway': 1864, 'ham': 1865, 'hammer': 1866, 'hand': 1867, 'handcarved': 1868, 'handed': 1869, 'handfull': 1870, 'handgun': 1871, 'handicapped': 1872, 'handle': 1873, 'hands': 1874, 'handsome': 1875, 'handwriting': 1876, 'hang': 1877, 'hanging': 1878, 'hangs': 1879, 'hank': 1880, 'happen': 1881, 'happened': 1882, 'happens': 1883, 'happily': 1884, 'happy': 1885, 'hard': 1886, 'harder': 1887, 'hardly': 1888, 'hardware': 1889, 'harley': 1890, 'harp': 1891, 'harpie': 1892, 'harry': 1893, 'has': 1894, 'hath': 1895, 'hats': 1896, 'haunting': 1897, 'have': 1898, 'having': 1899, 'hawaii': 1900, 'hawaiian': 1901, 'hawaiians': 1902, 'hawk': 1903, 'hay': 1904, 'hayseed': 1905, 'hayward': 1906, 'hazy': 1907, 'he': 1908, 'head': 1909, 'headdress': 1910, 'headed': 1911, 'headlights': 1912, 'headline': 1913, 'headphone': 1914, 'headphones': 1915, 'headress': 1916, 'heads': 1917, 'headset': 1918, 'headstone': 1919, 'headstones': 1920, 'headstrong': 1921, 'healing': 1922, 'health': 1923, 'hear': 1924, 'heard': 1925, 'hearing': 1926, 'hears': 1927, 'hearse': 1928, 'heart': 1929, 'hearted': 1930, 'hearty': 1931, 'heat': 1932, 'heated': 1933, 'heaven': 1934, 'heavenly': 1935, 'heavily': 1936, 'heaviness': 1937, 'heavy': 1938, 'heck': 1939, 'hefts': 1940, 'heidi': 1941, 'held': 1942, 'hell': 1943, 'hello': 1944, 'help': 1945, 'helped': 1946, 'helpful': 1947, 'helpless': 1948, 'helps': 1949, 'henry': 1950, 'her': 1951, 'here': 1952, 'herself': 1953, 'hesitation': 1954, 'hey': 1955, 'hi': 1956, 'hidden': 1957, 'hide': 1958, 'hides': 1959, 'hiding': 1960, 'hifi': 1961, 'high': 1962, 'highball': 1963, 'highlights': 1964, 'highway': 1965, 'hilarious': 1966, 'hill': 1967, 'hillbilly': 1968, 'hills': 1969, 'him': 1970, 'himself': 1971, 'hinge': 1972, 'hint': 1973, 'hired': 1974, 'hiring': 1975, 'hirsute': 1976, 'his': 1977, 'hit': 1978, 'hither': 1979, 'hits': 1980, 'hitting': 1981, 'hmm': 1982, 'hold': 1983, 'holding': 1984, 'holds': 1985, 'hole': 1986, 'hollow': 1987, 'holy': 1988, 'home': 1989, 'homecoming': 1990, 'homicidally': 1991, 'honest': 1992, 'honestly': 1993, 'honey': 1994, 'hong': 1995, 'honking': 1996, 'honky': 1997, 'honor': 1998, 'hood': 1999, 'hook': 2000, 'hoots': 2001, 'hop': 2002, 'hope': 2003, 'hopeless': 2004, 'hoping': 2005, 'hops': 2006, 'horn': 2007, 'horne': 2008, 'horns': 2009, 'horrible': 2010, 'horror': 2011, 'horse': 2012, 'hose': 2013, 'hospital': 2014, 'hot': 2015, 'hotel': 2016, 'houlies': 2017, 'hour': 2018, 'hours': 2019, 'house': 2020, 'household': 2021, 'houses': 2022, 'housetrailer': 2023, 'hovering': 2024, 'how': 2025, \"how'm\": 2026, 'however': 2027, 'howl': 2028, 'howling': 2029, 'howls': 2030, 'howsoever': 2031, 'hubcap': 2032, 'huckleberry': 2033, 'hug': 2034, 'huge': 2035, 'hugs': 2036, 'huh': 2037, 'hula': 2038, 'hulking': 2039, 'human': 2040, 'humdrum': 2041, 'humidity': 2042, 'humongous': 2043, 'hums': 2044, 'hundred': 2045, 'hungry': 2046, 'hunting': 2047, 'hurley': 2048, 'hurling': 2049, 'hurls': 2050, 'hurries': 2051, 'hurry': 2052, 'hurt': 2053, 'hurts': 2054, 'husband': 2055, 'hushed': 2056, 'hustle': 2057, 'hydraulic': 2058, 'hydraulics': 2059, 'hymnals': 2060, 'hypnotically': 2061, 'hypocrites': 2062, 'i': 2063, \"i'll\": 2064, \"i'm\": 2065, \"i've\": 2066, 'i-(loses': 2067, 'ice': 2068, 'iceland': 2069, 'icelander': 2070, 'icelandic': 2071, 'icelandics': 2072, 'id': 2073, 'idea': 2074, 'ideal': 2075, 'identical': 2076, 'identified': 2077, 'identity': 2078, 'idiots': 2079, 'idly': 2080, 'if': 2081, 'ignores': 2082, 'ill': 2083, 'illegal': 2084, 'illuminates': 2085, 'illumined': 2086, 'imaginable': 2087, 'imbroglio': 2088, 'immediately': 2089, 'impact': 2090, 'impatient': 2091, 'impatiently': 2092, 'implication': 2093, 'important': 2094, 'impossibly': 2095, 'impression': 2096, 'improve': 2097, 'impulses': 2098, 'in': 2099, 'incarcerated': 2100, 'incarnate': 2101, 'inches': 2102, 'included': 2103, 'including': 2104, 'incorrectly': 2105, 'incredible': 2106, 'indeed': 2107, 'india': 2108, 'indian': 2109, 'indicated': 2110, 'indicates': 2111, 'indifference': 2112, 'indifferently': 2113, 'indoctrination': 2114, 'industrial': 2115, 'ineffectual': 2116, 'infant': 2117, 'infirm': 2118, 'inflicted': 2119, 'influence': 2120, 'information': 2121, 'infra': 2122, 'inhales': 2123, 'inhand': 2124, 'inherent': 2125, 'initials': 2126, 'injured': 2127, 'inmate': 2128, 'inn': 2129, 'innocence': 2130, 'inquest': 2131, 'inquiry': 2132, 'ins': 2133, 'insane': 2134, 'insert': 2135, 'inserted': 2136, 'inserts': 2137, 'inside': 2138, 'insincerely': 2139, 'insincerity': 2140, 'insist': 2141, 'insolent': 2142, 'inspects': 2143, 'inspiration': 2144, 'installed': 2145, 'instance': 2146, 'instantly': 2147, 'instead': 2148, 'instinct': 2149, 'instinctively': 2150, 'institutional': 2151, 'instructed': 2152, 'insult': 2153, 'insults': 2154, 'insurance': 2155, 'int': 2156, 'intelligence': 2157, 'intelligent': 2158, 'intelligible': 2159, 'intends': 2160, 'intensive': 2161, 'intently': 2162, 'interact': 2163, 'intercepts': 2164, 'intercom': 2165, 'intercut': 2166, 'interest': 2167, 'interested': 2168, 'interesting': 2169, 'interests': 2170, 'international': 2171, 'interrogate': 2172, 'interrogation': 2173, 'interrupted': 2174, 'interrupts': 2175, 'intervenes': 2176, 'interview': 2177, 'intimately': 2178, 'into': 2179, 'intoning': 2180, 'intrigue': 2181, 'intrigued': 2182, 'intuition': 2183, 'invaded': 2184, 'inventing': 2185, 'invention': 2186, 'investigation': 2187, 'investigative': 2188, 'investigator': 2189, 'investment': 2190, 'invisible': 2191, 'invitation': 2192, 'involuntarily': 2193, 'involving': 2194, 'iron': 2195, 'irritating': 2196, 'is': 2197, 'issue': 2198, 'it': 2199, 'it\"-cole': 2200, 'it--': 2201, 'itch': 2202, 'item': 2203, 'its': 2204, 'j': 2205, 'j.': 2206, 'jack': 2207, 'jack\\'s\"--': 2208, 'jacket': 2209, 'jacks': 2210, 'jacoby': 2211, 'jacques': 2212, 'jade': 2213, 'jaguar': 2214, 'jail': 2215, 'james': 2216, 'james--': 2217, 'jamin': 2218, 'janek': 2219, 'janitor': 2220, 'jar': 2221, 'jared': 2222, 'jaundiced': 2223, 'jaw': 2224, 'jazz': 2225, 'jealous': 2226, 'jeans': 2227, 'jelly': 2228, 'jennings': 2229, 'jer': 2230, 'jerry': 2231, 'jesus': 2232, 'jet': 2233, 'jfk': 2234, 'jim': 2235, 'jimmies': 2236, 'jimmy': 2237, 'jitterbugging': 2238, 'job': 2239, 'jocelyn-(covering': 2240, 'joe': 2241, 'joey': 2242, 'jogged': 2243, 'jogging': 2244, 'johnnie': 2245, 'johnny': 2246, 'johnnys': 2247, 'johnson': 2248, 'join': 2249, 'joining': 2250, 'joke': 2251, 'joker': 2252, 'josie': 2253, 'jostled': 2254, 'jots': 2255, 'joy': 2256, 'juice': 2257, 'juke': 2258, 'jukebox': 2259, 'julie': 2260, 'jump': 2261, 'jumped': 2262, 'jumps': 2263, 'jurisdiction': 2264, 'just': 2265, 'k': 2266, 'kashmir': 2267, 'keep': 2268, 'keeping': 2269, 'keeps': 2270, 'kennedys': 2271, 'key': 2272, 'keyhole': 2273, 'keys': 2274, 'kicked': 2275, 'kicks': 2276, 'kid': 2277, 'kidding': 2278, 'kiddo': 2279, 'kids': 2280, 'kill': 2281, 'killed': 2282, 'killer': 2283, 'killing': 2284, 'kills': 2285, 'kind': 2286, \"kind'a\": 2287, 'kinda': 2288, 'kinds': 2289, 'kingdom': 2290, 'kingly': 2291, 'kiss': 2292, 'kissed': 2293, 'kisses': 2294, 'kit': 2295, 'kitchen': 2296, 'knee': 2297, 'kneel': 2298, 'kneels': 2299, 'knees': 2300, 'knew': 2301, 'knife': 2302, 'knob': 2303, 'knock': 2304, 'knocked': 2305, 'knockouts': 2306, 'knocks': 2307, 'knot': 2308, 'knothead': 2309, 'knots': 2310, 'know': 2311, 'knowing': 2312, 'knowledge': 2313, 'known': 2314, 'knows': 2315, 'knuckle': 2316, 'knuckled': 2317, 'knuckles': 2318, 'kong': 2319, 'koro': 2320, 'l': 2321, 'l.j.': 2322, 'lab': 2323, 'labeled': 2324, 'labels': 2325, 'labors': 2326, 'lack': 2327, 'lacking': 2328, 'lacquer': 2329, 'lady': 2330, 'lagging': 2331, 'laid': 2332, 'lair': 2333, 'lake': 2334, 'lama': 2335, 'lamplighter': 2336, 'land': 2337, 'language': 2338, 'lapels': 2339, 'large': 2340, 'larger': 2341, 'lariat': 2342, 'lasagne': 2343, 'lash': 2344, 'last': 2345, 'lastly': 2346, 'latch': 2347, 'late': 2348, 'later': 2349, 'lathers': 2350, 'laugh': 2351, 'laughed': 2352, 'laughing': 2353, 'laughs': 2354, 'laundry': 2355, 'laura': 2356, 'lawman': 2357, 'lawrence': 2358, 'lawyer': 2359, 'lay': 2360, 'lead': 2361, 'leader': 2362, 'leading': 2363, 'leads': 2364, 'leak': 2365, 'lean': 2366, 'leaned': 2367, 'leaning': 2368, 'leans': 2369, 'leap': 2370, 'leaping': 2371, 'leaps': 2372, 'learn': 2373, 'learned': 2374, 'learning': 2375, 'lease': 2376, 'least': 2377, 'leather': 2378, 'leave': 2379, 'leaves': 2380, 'leaving': 2381, 'led': 2382, 'ledger': 2383, 'lee': 2384, 'leering': 2385, 'left': 2386, 'legal': 2387, 'legend': 2388, 'leland': 2389, 'length': 2390, 'lengthy': 2391, 'lens': 2392, 'leo': 2393, 'leonard': 2394, 'lesions': 2395, 'less': 2396, 'lesson': 2397, 'let': 2398, 'lets': 2399, 'letter': 2400, 'letters': 2401, 'level': 2402, 'levels': 2403, 'lewis': 2404, 'licence': 2405, 'license': 2406, 'lie': 2407, 'lied': 2408, 'lies': 2409, 'life': 2410, 'lift': 2411, 'lifts': 2412, 'light': 2413, 'lighter': 2414, 'lightheaded': 2415, 'lightly': 2416, 'lights': 2417, 'like': 2418, 'liked': 2419, 'likin': 2420, 'liking': 2421, 'limit': 2422, 'limits': 2423, 'limousine': 2424, 'linchpin': 2425, 'lindy': 2426, 'line': 2427, 'lined': 2428, 'linen': 2429, 'lines': 2430, 'lion': 2431, 'lip': 2432, 'lips': 2433, 'liquor': 2434, 'list': 2435, 'listen': 2436, 'listening': 2437, 'listens': 2438, 'listlessly': 2439, 'lit': 2440, 'little': 2441, 'live': 2442, 'lived': 2443, 'lives': 2444, 'liveth': 2445, 'living': 2446, 'load': 2447, 'loaded': 2448, 'loaf': 2449, 'lobby': 2450, 'local': 2451, 'locates': 2452, 'locations': 2453, 'lockbox': 2454, 'locked': 2455, 'locker': 2456, 'lodge': 2457, 'log': 2458, 'logic': 2459, 'logtown': 2460, 'lone': 2461, 'loneliness': 2462, 'lonely': 2463, 'long': 2464, 'longer': 2465, 'longneck': 2466, 'longs': 2467, 'look': 2468, 'looked': 2469, 'lookin': 2470, 'looking': 2471, 'looks': 2472, 'loomer': 2473, 'loose': 2474, 'loosely': 2475, 'lord': 2476, 'lore': 2477, 'lose': 2478, 'losing': 2479, 'loss': 2480, 'losses': 2481, 'lost': 2482, 'lot': 2483, 'lotown': 2484, 'lots': 2485, 'lotus': 2486, 'loud': 2487, 'louis': 2488, 'lounge': 2489, 'lounging': 2490, 'love': 2491, 'loved': 2492, 'lovely': 2493, 'loves': 2494, 'loving': 2495, 'low': 2496, 'lower': 2497, 'lowering': 2498, 'lowers': 2499, 'lowtown': 2500, 'luck': 2501, 'lucky': 2502, 'lucy': 2503, 'lug': 2504, 'lumber': 2505, 'lummox': 2506, 'lumps': 2507, 'lunch': 2508, 'lurches': 2509, 'luscious': 2510, 'lush': 2511, 'lust': 2512, 'luxuriously': 2513, 'lydecker': 2514, 'lydeker': 2515, 'lying': 2516, 'm': 2517, 'ma': 2518, \"ma'am\": 2519, 'machine': 2520, 'mad': 2521, 'madam': 2522, 'maddy': 2523, 'made': 2524, 'madelaine': 2525, 'madeleine': 2526, 'madeline': 2527, 'madly': 2528, 'magic': 2529, 'magician': 2530, 'mahogany': 2531, 'maid': 2532, 'mail': 2533, 'maintain': 2534, 'major': 2535, 'make': 2536, 'maker': 2537, 'makes': 2538, 'makeup': 2539, 'making': 2540, 'male': 2541, 'malted': 2542, 'man': 2543, 'manages': 2544, 'mandarin': 2545, 'manic': 2546, 'manicure': 2547, 'manilla': 2548, 'manipulates': 2549, 'mann': 2550, 'manner': 2551, 'mannered': 2552, 'mannerless': 2553, 'mano': 2554, 'manslaughter': 2555, 'manufacture': 2556, 'many': 2557, 'map': 2558, 'maple': 2559, 'mapped': 2560, 'marches': 2561, 'marilyn': 2562, 'marked': 2563, 'marks': 2564, 'marlon': 2565, 'maroon': 2566, 'married': 2567, 'martell': 2568, 'mashes': 2569, 'masse': 2570, 'mast': 2571, 'match': 2572, 'matter': 2573, 'mattress': 2574, 'may': 2575, 'maybe': 2576, 'mayday': 2577, 'mayo': 2578, 'me': 2579, 'meals': 2580, 'mean': 2581, 'meaning': 2582, 'meaningful': 2583, 'means': 2584, 'meant': 2585, 'meantime': 2586, 'measure': 2587, 'meatloaf': 2588, 'medication': 2589, 'medicine': 2590, 'meet': 2591, 'meeting': 2592, 'meets': 2593, 'megaphone': 2594, 'melange': 2595, 'melanie': 2596, 'melody': 2597, 'melts': 2598, 'member': 2599, 'members': 2600, 'memorial': 2601, 'memories': 2602, 'memory': 2603, 'memphis': 2604, 'men': 2605, 'mention': 2606, 'menu': 2607, 'merciful': 2608, 'merry': 2609, 'mess': 2610, 'message': 2611, 'messages': 2612, 'messed': 2613, 'messy': 2614, 'met': 2615, 'metabolism': 2616, 'metal': 2617, 'method': 2618, 'michael': 2619, 'mid': 2620, 'mid-': 2621, 'middle': 2622, 'middleman': 2623, 'midge': 2624, 'midget': 2625, 'midnight': 2626, 'midst': 2627, 'midthirties': 2628, 'might': 2629, 'mike': 2630, \"mike're\": 2631, 'mild': 2632, 'mildew': 2633, 'mile': 2634, 'miles': 2635, 'milford': 2636, 'milkballs': 2637, 'mill': 2638, 'million': 2639, 'mind': 2640, 'mindbody': 2641, 'mindedly': 2642, 'mindless': 2643, 'mine': 2644, 'mingle': 2645, 'miniature': 2646, 'minimum': 2647, 'mink': 2648, 'minor': 2649, 'minute': 2650, 'minutes': 2651, 'mirror': 2652, 'mischief': 2653, 'misguided': 2654, 'mismatched': 2655, 'miss': 2656, 'missed': 2657, 'misses': 2658, 'missoula': 2659, 'mistake': 2660, 'mister': 2661, 'mittens': 2662, 'mixed': 2663, 'moan': 2664, 'moaning': 2665, 'moans': 2666, 'mocking': 2667, 'model': 2668, 'modern': 2669, 'modifications': 2670, 'mom': 2671, 'moment': 2672, 'moments': 2673, 'momentum': 2674, 'monday': 2675, 'money': 2676, 'monitor': 2677, 'monroe': 2678, 'monstrous': 2679, 'montana': 2680, 'month': 2681, 'months': 2682, 'mood': 2683, 'moon': 2684, 'mooney': 2685, 'moonless': 2686, 'moran': 2687, 'more': 2688, 'morgue': 2689, 'morn': 2690, 'morning': 2691, 'morons': 2692, 'mortally': 2693, 'mortician': 2694, 'most': 2695, 'mostly': 2696, 'motel': 2697, 'mother': 2698, 'motherly': 2699, 'motions': 2700, 'motorcycle': 2701, 'mound': 2702, 'mountain': 2703, 'mountains': 2704, 'mourners': 2705, 'mouse': 2706, 'mouth': 2707, 'mouthful': 2708, 'mouthing': 2709, 'mouthpiece': 2710, 'mouths': 2711, 'move': 2712, 'moved': 2713, 'moves': 2714, 'moving': 2715, 'mr.': 2716, 'mrs': 2717, 'mrs.': 2718, 'ms.': 2719, 'much': 2720, 'mucho': 2721, 'mug': 2722, 'mugs': 2723, 'mule': 2724, 'murder': 2725, 'murdered': 2726, 'muse': 2727, 'mushrooms': 2728, 'music': 2729, 'must': 2730, 'mute': 2731, 'muted': 2732, 'mutters': 2733, 'my': 2734, 'mynah': 2735, 'myself': 2736, 'mystery': 2737, 'mystified': 2738, 'n': 2739, \"n't\": 2740, 'na': 2741, 'nadine': 2742, \"nadine'd\": 2743, 'nail': 2744, 'name': 2745, 'named': 2746, 'names': 2747, 'napkin': 2748, 'narrows': 2749, 'native': 2750, 'naturally': 2751, 'nature': 2752, 'naturedly': 2753, 'naughty': 2754, 'nautically': 2755, 'near': 2756, 'nearby': 2757, 'nearly': 2758, 'neatnik': 2759, 'necessarily': 2760, 'necessary': 2761, 'neck': 2762, 'necklace': 2763, 'neckline': 2764, 'need': 2765, 'needed': 2766, 'needle': 2767, 'needlepoint': 2768, 'needles': 2769, 'needless': 2770, 'needs': 2771, 'negative': 2772, 'negligee': 2773, 'neither': 2774, 'nelson': 2775, 'neon': 2776, 'nepal': 2777, 'nephew': 2778, 'nervous': 2779, 'nervously': 2780, 'neurons': 2781, 'never': 2782, 'new': 2783, 'newborn': 2784, 'newly': 2785, 'news': 2786, 'newspaper': 2787, 'next': 2788, 'nice': 2789, 'niceties': 2790, 'niece': 2791, 'night': 2792, 'nighter': 2793, 'nightfall': 2794, 'nightgown': 2795, 'nightmare': 2796, 'nights': 2797, 'nine': 2798, 'no': 2799, 'nobody': 2800, 'nod': 2801, 'nods': 2802, 'noise': 2803, 'noiseless': 2804, 'noises': 2805, 'nominally': 2806, 'none': 2807, 'nonsense': 2808, 'nonsmoking': 2809, 'noon': 2810, 'nope': 2811, 'nor': 2812, 'norma': 2813, \"norma'll\": 2814, 'normal': 2815, 'normally': 2816, 'north': 2817, 'northern': 2818, 'northwest': 2819, 'norwegian': 2820, 'norweigans': 2821, 'nose': 2822, 'nostalgic': 2823, 'nostril': 2824, 'nostrils': 2825, 'not': 2826, 'note': 2827, 'notebook': 2828, 'notes': 2829, \"nothin'\": 2830, 'nothing': 2831, 'notice': 2832, 'noticed': 2833, 'notices': 2834, 'notion': 2835, 'now': 2836, 'nowhere': 2837, 'number': 2838, 'numbers': 2839, 'numerous': 2840, 'nurse': 2841, 'nursing': 2842, 'nuts': 2843, 'o': 2844, \"o'clock\": 2845, \"o'reilly\": 2846, 'o.': 2847, 'o.o.j.': 2848, 'oars': 2849, 'object': 2850, 'obligated': 2851, 'oblivious': 2852, 'obscured': 2853, 'obsessively': 2854, 'obstructing': 2855, 'obvious': 2856, 'obviously': 2857, 'occasion': 2858, 'occupy': 2859, 'odd': 2860, 'oddly': 2861, 'odysseus': 2862, 'of': 2863, 'off': 2864, 'offend': 2865, 'offer': 2866, 'offering': 2867, 'offers': 2868, 'office': 2869, 'officer': 2870, 'offscreen': 2871, 'often': 2872, 'og': 2873, 'oh': 2874, 'ohio': 2875, 'oil': 2876, 'okay': 2877, 'old': 2878, 'older': 2879, 'oldest': 2880, 'on': 2881, 'once': 2882, 'one': 2883, 'onearmed': 2884, 'ones': 2885, 'ongoing': 2886, 'only': 2887, 'onto': 2888, 'ooper': 2889, 'open': 2890, 'opened': 2891, 'opening': 2892, 'opens': 2893, 'opera': 2894, 'operatic': 2895, 'operating': 2896, 'operation': 2897, 'operator': 2898, 'opinion': 2899, 'opportunities': 2900, 'opportunity': 2901, 'opposite': 2902, 'options': 2903, 'or': 2904, 'oration': 2905, 'order': 2906, 'ordering': 2907, 'orders': 2908, 'ordinance': 2909, 'ordinary': 2910, 'organize': 2911, 'organized': 2912, 'orgasmic': 2913, 'originating': 2914, 'ornament': 2915, 'orthopedic': 2916, 'other': 2917, 'others': 2918, 'otter': 2919, 'ought': 2920, \"ought'a\": 2921, 'ounce': 2922, 'our': 2923, 'ourselves': 2924, 'out': 2925, 'outfit': 2926, 'outside': 2927, 'outstanding': 2928, 'over': 2929, 'overall': 2930, 'overcome': 2931, 'overhead': 2932, 'overheads': 2933, \"ow'st\": 2934, 'owe': 2935, 'owl': 2936, 'own': 2937, 'owned': 2938, 'owner': 2939, 'owns': 2940, 'p': 2941, 'pace': 2942, 'paces': 2943, 'package': 2944, 'packard': 2945, 'packet': 2946, 'pact': 2947, 'pad': 2948, 'paddle': 2949, 'page': 2950, 'paid': 2951, 'pain': 2952, 'pains': 2953, 'paint': 2954, 'pair': 2955, 'pajamas': 2956, 'pal': 2957, 'pale': 2958, 'palm': 2959, 'palmer': 2960, 'palmers': 2961, 'palms': 2962, 'pan': 2963, 'pancakes': 2964, 'panel': 2965, 'panne': 2966, 'pans': 2967, 'pantomime': 2968, 'pants': 2969, 'paper': 2970, 'papers': 2971, 'paradox': 2972, 'parakeet': 2973, 'parent': 2974, 'parents': 2975, 'paris': 2976, 'park': 2977, 'parked': 2978, 'parking': 2979, 'parole': 2980, 'parrot': 2981, 'part': 2982, 'partially': 2983, 'particles': 2984, 'particular': 2985, 'parting': 2986, 'partition': 2987, 'partner': 2988, 'parts': 2989, 'paso': 2990, 'pass': 2991, 'passed': 2992, 'passes': 2993, 'passing': 2994, 'passionate': 2995, 'passionately': 2996, 'past': 2997, 'pasting': 2998, 'patch': 2999, 'patches': 3000, 'patent': 3001, 'path': 3002, 'pathologist': 3003, 'patience': 3004, 'patient': 3005, 'patio': 3006, 'patrol': 3007, 'pats': 3008, 'patterns': 3009, 'paulson': 3010, 'pause': 3011, 'pauses': 3012, 'pay': 3013, 'peabrain': 3014, 'peace': 3015, 'peak': 3016, 'peaks': 3017, 'peanuts': 3018, 'pebble': 3019, 'pecking': 3020, 'peek': 3021, 'peeks': 3022, 'peering': 3023, 'peers': 3024, 'pen': 3025, 'pencil': 3026, 'penetrate': 3027, 'peninsula': 3028, 'pension': 3029, 'pensively': 3030, 'people': 3031, 'percent': 3032, 'perched': 3033, 'percolator': 3034, 'perfect': 3035, 'perfectly': 3036, 'perform': 3037, 'performance': 3038, 'performed': 3039, 'perfume': 3040, 'perhaps': 3041, 'perish': 3042, 'permission': 3043, 'perpetrator': 3044, 'perpetrators': 3045, 'perplexed': 3046, 'person': 3047, 'personal': 3048, 'persuaded': 3049, 'pertain': 3050, 'pertaining': 3051, 'pet': 3052, 'pete': 3053, 'pharmaceuticals': 3054, 'phd': 3055, 'philadelphia': 3056, 'phillip': 3057, 'phone': 3058, 'phones': 3059, 'photo': 3060, 'photograph': 3061, 'physical': 3062, 'physically': 3063, 'pick': 3064, 'picked': 3065, 'picking': 3066, 'picks': 3067, 'picnic': 3068, 'picture': 3069, 'pictures': 3070, 'pie': 3071, 'piece': 3072, 'pieces': 3073, 'pierce': 3074, 'pies': 3075, 'pile': 3076, 'pilgrimage': 3077, 'piling': 3078, 'pill': 3079, 'pilot': 3080, 'pimentos': 3081, 'pinches': 3082, 'pine': 3083, 'pines': 3084, 'piping': 3085, 'pistol': 3086, 'pitch': 3087, 'pitcher': 3088, 'place': 3089, 'placed': 3090, 'places': 3091, 'placid': 3092, 'placing': 3093, 'plain': 3094, 'plan': 3095, 'plane': 3096, 'planning': 3097, 'plans': 3098, 'plant': 3099, 'plastic': 3100, 'plate': 3101, 'plates': 3102, 'platform': 3103, 'platter': 3104, 'play': 3105, 'player': 3106, 'playfully': 3107, 'playing': 3108, 'plays': 3109, 'pleading': 3110, 'pleadings': 3111, 'pleads': 3112, 'pleasant': 3113, 'pleasantly': 3114, 'please': 3115, 'pleased': 3116, 'pleasure': 3117, 'plenty': 3118, 'plight': 3119, 'plops': 3120, 'plowing': 3121, 'plugs': 3122, 'plum': 3123, 'plunging': 3124, 'plus': 3125, 'plush': 3126, 'pm': 3127, 'pocket': 3128, 'pockets': 3129, 'poem': 3130, 'poet': 3131, 'point': 3132, 'pointed': 3133, 'pointer': 3134, 'points': 3135, 'poker': 3136, 'pole': 3137, 'police': 3138, 'polishes': 3139, 'polishing': 3140, 'pond': 3141, 'ponder': 3142, 'ponderosa': 3143, 'ponders': 3144, 'pony': 3145, 'poodle': 3146, 'pool': 3147, 'poor': 3148, 'pop': 3149, 'popped': 3150, 'pops': 3151, 'porch': 3152, 'porcupine': 3153, 'portable': 3154, 'portrait': 3155, 'position': 3156, 'positive': 3157, 'possession': 3158, 'possible': 3159, 'possibly': 3160, 'posssess': 3161, 'post': 3162, 'postgraduate': 3163, 'postmortem': 3164, 'pot': 3165, 'potato': 3166, 'pound': 3167, 'pour': 3168, 'poured': 3169, 'pouring': 3170, 'pours': 3171, 'pouts': 3172, 'pov': 3173, 'powder': 3174, 'power': 3175, 'practically': 3176, 'practice': 3177, 'practicing': 3178, 'pray': 3179, 'prayer': 3180, 'prayers': 3181, 'preaching': 3182, 'precipitated': 3183, 'prefer': 3184, 'prejudice': 3185, 'preoccupied': 3186, 'preparation': 3187, 'prepare': 3188, 'prepares': 3189, 'presence': 3190, 'present': 3191, 'press': 3192, 'pressed': 3193, 'presses': 3194, 'pressure': 3195, 'presume': 3196, 'pretend': 3197, 'pretty': 3198, 'previous': 3199, 'price': 3200, 'priced': 3201, 'pride': 3202, 'primal': 3203, 'primitive': 3204, 'prison': 3205, 'prisoner': 3206, 'private': 3207, 'probably': 3208, 'problem': 3209, 'problems': 3210, 'proceeds': 3211, 'produces': 3212, 'productive': 3213, 'profound': 3214, 'program': 3215, 'prom': 3216, 'promise': 3217, 'promised': 3218, 'proof': 3219, 'proper': 3220, 'property': 3221, 'proposes': 3222, 'protect': 3223, 'protecting': 3224, 'protection': 3225, 'protectors': 3226, 'proud': 3227, 'prove': 3228, 'psychiatrist': 3229, 'psychological': 3230, 'public': 3231, 'pulaski': 3232, 'pull': 3233, 'pulled': 3234, 'pulls': 3235, 'pulsating': 3236, 'pumice': 3237, 'pumped': 3238, 'punch': 3239, 'punches': 3240, 'punk': 3241, 'puppets': 3242, 'purchasing': 3243, 'purse': 3244, 'pursue': 3245, 'pursued': 3246, 'pursuit': 3247, 'pushes': 3248, 'pussycat': 3249, 'put': 3250, 'puts': 3251, 'putting': 3252, 'puzzled': 3253, 'quaint': 3254, 'qualification': 3255, 'quality': 3256, \"quarter'll\": 3257, 'quarterback': 3258, 'quarters': 3259, 'quartet': 3260, 'queen': 3261, 'question': 3262, 'questions': 3263, 'quick': 3264, 'quickly': 3265, 'quiet': 3266, 'quieter': 3267, 'quietly': 3268, 'quietness': 3269, 'quit': 3270, 'quite': 3271, 'quittin': 3272, 'qvath': 3273, 'r': 3274, 'r.': 3275, 'raccoons': 3276, 'racing': 3277, 'racket': 3278, 'radio': 3279, 'rage': 3280, 'rail': 3281, 'railing': 3282, 'railroad': 3283, 'rails': 3284, 'rain': 3285, 'rainbow': 3286, 'rainbows': 3287, 'raises': 3288, 'raising': 3289, 'ramp': 3290, 'ran': 3291, 'range': 3292, 'ranges': 3293, 'ransacked': 3294, 'rap': 3295, 'rapid': 3296, 'rapidly': 3297, 'rare': 3298, 'raspberry': 3299, 'rather': 3300, 're': 3301, 'reach': 3302, 'reaches': 3303, 'react': 3304, 'reacting': 3305, 'reaction': 3306, 'reactions': 3307, 'reacts': 3308, 'read': 3309, 'reads': 3310, 'ready': 3311, 'real': 3312, 'realize': 3313, 'realized': 3314, 'realizes': 3315, 'realizing': 3316, 'really': 3317, 'rear': 3318, 'reason': 3319, 'reasonable': 3320, 'reasonably': 3321, 'reasons': 3322, 'reassured': 3323, 'reassuring': 3324, 'rebellion': 3325, 'rebellious': 3326, 'rec': 3327, 'recall': 3328, 'recedes': 3329, 'receipts': 3330, 'receive': 3331, 'received': 3332, 'receiver': 3333, 'receives': 3334, 'recently': 3335, 'reception': 3336, 'receptionist': 3337, 'recipe': 3338, 'recite': 3339, 'recognize': 3340, 'recommend': 3341, 'recommendation': 3342, 'reconstruction': 3343, 'record': 3344, 'recorder': 3345, 'recording': 3346, 'rectangular': 3347, 'red': 3348, 'redhead': 3349, 'redlight': 3350, 'redoes': 3351, 'reeling': 3352, 'reenacts': 3353, 'reenters': 3354, 'reference': 3355, 'refill': 3356, 'refills': 3357, 'reflected': 3358, 'refuses': 3359, 'regains': 3360, 'regard': 3361, 'regarding': 3362, 'register': 3363, 'registers': 3364, 'regular': 3365, 'regulars': 3366, 'reigneth': 3367, 'reinforcement': 3368, 'relation': 3369, 'relations': 3370, 'relationship': 3371, 'release': 3372, 'released': 3373, 'releases': 3374, 'relief': 3375, 'relieving': 3376, 'relishing': 3377, 'reluctance': 3378, 'remain': 3379, 'remainder': 3380, 'remains': 3381, 'remember': 3382, 'remembered': 3383, 'remembering': 3384, 'remembers': 3385, 'remind': 3386, 'reminder': 3387, 'remote': 3388, 'remove': 3389, 'removes': 3390, 'removing': 3391, 'renault': 3392, 'rend': 3393, 'rending': 3394, 'rep.': 3395, 'repeats': 3396, 'repetition': 3397, 'replaces': 3398, 'replayed': 3399, 'replied': 3400, 'report': 3401, 'representative': 3402, 'repulses': 3403, 'requires': 3404, 'rescue': 3405, 'research': 3406, 'resemblance': 3407, 'reserve': 3408, 'residence': 3409, 'resist': 3410, 'resort': 3411, 'respect': 3412, 'respond': 3413, 'response': 3414, 'responsibilities': 3415, 'responsibility': 3416, 'responsible': 3417, 'rest': 3418, 'restlessly': 3419, 'rests': 3420, 'results': 3421, 'resume': 3422, 'resumes': 3423, 'resurrection': 3424, 'rethinks': 3425, 'retirement': 3426, 'retrieve': 3427, 'retrieves': 3428, 'return': 3429, 'returning': 3430, 'returns': 3431, 'reveal': 3432, 'revealing': 3433, 'reveals': 3434, 'reverse': 3435, 'reversed': 3436, 'reverses': 3437, 'rhizome': 3438, 'rhythm': 3439, 'rhythmic': 3440, 'ribbon': 3441, 'rich': 3442, 'ricochets': 3443, 'ride': 3444, 'ridiculous': 3445, 'riding': 3446, 'rig': 3447, 'right': 3448, 'right--': 3449, 'rights': 3450, 'rigorously': 3451, 'rime': 3452, 'ring': 3453, 'ringing': 3454, 'rings': 3455, 'rip': 3456, 'rise': 3457, 'rises': 3458, 'rising': 3459, 'risks': 3460, 'river': 3461, 'riverbank': 3462, 'ro': 3463, 'road': 3464, 'roadhouse': 3465, 'roadside': 3466, 'roaring': 3467, 'roars': 3468, 'roast': 3469, 'robe': 3470, 'robert': 3471, 'robin': 3472, 'rock': 3473, 'rockers': 3474, 'rocket': 3475, 'rocks': 3476, 'rodeo': 3477, 'roll': 3478, 'rolled': 3479, 'rolling': 3480, 'rolls': 3481, 'romantic': 3482, 'romeo': 3483, 'romeos': 3484, 'ronette': 3485, 'room': 3486, 'rooms': 3487, 'rooster': 3488, 'root': 3489, 'rope': 3490, 'ropes': 3491, 'rose': 3492, 'rosenferd': 3493, 'rosenfield': 3494, 'rotweiler': 3495, 'rough': 3496, 'roughing': 3497, 'round': 3498, 'rounds': 3499, 'route': 3500, 'routine': 3501, 'roving': 3502, 'row': 3503, 'rower': 3504, 'rowing': 3505, 'rub': 3506, 'rubber': 3507, 'rubbing': 3508, 'rubs': 3509, 'rude': 3510, 'ruin': 3511, 'ruled': 3512, 'rummaging': 3513, 'rumple': 3514, 'run': 3515, 'runabout': 3516, 'rundown': 3517, 'runner': 3518, 'runners': 3519, 'running': 3520, 'runs': 3521, 'runway': 3522, 'rush': 3523, 'rushes': 3524, 'rustic': 3525, 'rusticated': 3526, 's.': 3527, 'sabine': 3528, 'sad': 3529, 'saddened': 3530, 'sadly': 3531, 'sadness': 3532, 'safe': 3533, 'safely': 3534, 'safety': 3535, 'said': 3536, 'sail': 3537, 'sailing': 3538, 'sailor': 3539, 'sails': 3540, 'saint': 3541, 'saith': 3542, 'sake': 3543, 'sales': 3544, 'salesman': 3545, 'salesperson': 3546, 'same': 3547, 'sample': 3548, 'samples': 3549, 'samsonite': 3550, 'san': 3551, 'sand': 3552, 'sandpaper': 3553, 'sandwich': 3554, 'sandwiches': 3555, 'sarah': 3556, 'sashays': 3557, 'sat': 3558, 'satisfactorily': 3559, 'satisfied': 3560, 'sauna': 3561, 'sausage': 3562, 'save': 3563, 'saw': 3564, 'sawmill': 3565, 'saxophone': 3566, 'say': 3567, 'saying': 3568, 'says': 3569, 'scaled': 3570, 'scallywag': 3571, 'scans': 3572, 'scantily': 3573, 'scare': 3574, 'scared': 3575, 'scaring': 3576, 'scarlet': 3577, 'scattered': 3578, 'scatters': 3579, 'scene': 3580, 'scheduled': 3581, 'school': 3582, 'schoolgirl': 3583, 'schoolwork': 3584, 'science': 3585, 'score': 3586, 'scores': 3587, 'scorn': 3588, 'scornfully': 3589, 'scotch': 3590, 'scotches': 3591, 'scowls': 3592, 'scraping': 3593, 'scream': 3594, 'screaming': 3595, 'screams': 3596, 'screech': 3597, 'screen': 3598, 'script': 3599, 'scrubbing': 3600, 'scrubs': 3601, 'sculpture': 3602, 'scurries': 3603, 'seal': 3604, 'searches': 3605, 'searching': 3606, 'seat': 3607, 'seated': 3608, 'seats': 3609, 'seattle': 3610, 'sec': 3611, 'second': 3612, 'secret': 3613, 'secretary': 3614, 'secretly': 3615, 'secrets': 3616, 'section': 3617, 'security': 3618, 'sedated': 3619, 'seduce': 3620, 'seductively': 3621, 'see': 3622, 'seedy': 3623, 'seeing': 3624, 'seeking': 3625, 'seem': 3626, 'seemed': 3627, 'seemingly': 3628, 'seems': 3629, 'seen': 3630, 'seepage': 3631, 'sees': 3632, 'seized': 3633, 'sejir': 3634, 'selects': 3635, 'self': 3636, 'sell': 3637, 'sellin': 3638, 'selling': 3639, 'semi': 3640, 'send': 3641, 'sender': 3642, 'senfield': 3643, 'sensational': 3644, 'sense': 3645, 'sensibility': 3646, 'sensually': 3647, 'sent': 3648, 'sentiment': 3649, 'sentimental': 3650, 'separate': 3651, 'separately': 3652, 'separates': 3653, 'september': 3654, 'serial': 3655, 'series': 3656, 'serious': 3657, 'serves': 3658, 'service': 3659, 'serving': 3660, 'session': 3661, 'set': 3662, 'sets': 3663, 'setting': 3664, 'settle': 3665, 'seven': 3666, 'several': 3667, 'severally': 3668, 'severe': 3669, 'sew': 3670, 'sewn': 3671, 'sex': 3672, 'sexes': 3673, 'sexual': 3674, 'shade': 3675, 'shades': 3676, 'shadow': 3677, 'shadows': 3678, 'shake': 3679, 'shakes': 3680, 'shaking': 3681, 'shall': 3682, 'shallow': 3683, 'shape': 3684, 'shapiro': 3685, 'share': 3686, 'sharp': 3687, 'she': 3688, 'sheer': 3689, 'sheet': 3690, 'shelf': 3691, 'shellshocked': 3692, 'shelly': 3693, 'shenanigans': 3694, 'sheriff': 3695, 'sherlock': 3696, \"sherrif's\": 3697, 'shh': 3698, 'shift': 3699, 'shifts': 3700, 'shimmers': 3701, 'shiner': 3702, 'shines': 3703, 'shirt': 3704, 'shirt--': 3705, 'shit': 3706, 'shivers': 3707, 'shivery': 3708, 'shock': 3709, 'shocked': 3710, 'shocking': 3711, 'shoe': 3712, 'shoes': 3713, 'shoot': 3714, 'shooting': 3715, 'shoots': 3716, 'shop': 3717, 'shopper': 3718, 'short': 3719, 'shortest': 3720, 'shortly': 3721, 'shorts': 3722, 'shortstack': 3723, 'shot': 3724, 'shotgun': 3725, 'shots': 3726, 'should': 3727, 'shoulder': 3728, 'shoulders': 3729, 'shouting': 3730, 'shouts': 3731, 'shoving': 3732, 'show': 3733, 'shower': 3734, 'shows': 3735, 'shrieks': 3736, 'shrug': 3737, 'shushes': 3738, 'shut': 3739, 'shuts': 3740, 'shutting': 3741, 'shy': 3742, 'shyly': 3743, 'sick': 3744, 'sickened': 3745, 'side': 3746, 'sidebenefit': 3747, 'sided': 3748, 'sifts': 3749, 'sighs': 3750, 'sight': 3751, 'sign': 3752, 'signal': 3753, 'signals': 3754, 'signature': 3755, 'signing': 3756, 'signs': 3757, 'silence': 3758, 'silences': 3759, 'silent': 3760, 'silently': 3761, 'silk': 3762, 'silver': 3763, 'simple': 3764, 'simply': 3765, 'simultaneously': 3766, 'since': 3767, 'sincerely': 3768, 'sing': 3769, 'singin': 3770, 'singing': 3771, 'single': 3772, 'singular': 3773, 'sink': 3774, 'sinkhole': 3775, 'sip': 3776, 'sipping': 3777, 'sips': 3778, 'sir': 3779, 'siren': 3780, 'sister': 3781, 'sisters': 3782, 'sit': 3783, 'site': 3784, 'sits': 3785, 'sitting': 3786, 'situation': 3787, 'six': 3788, 'sixty': 3789, 'skeptical': 3790, 'sketch': 3791, 'sketched': 3792, 'sketches': 3793, 'skimpy': 3794, 'skin': 3795, 'skip': 3796, 'skips': 3797, 'sky': 3798, 'slams': 3799, 'slant': 3800, 'slaps': 3801, 'slats': 3802, 'sleep': 3803, 'sleeping': 3804, 'sleeps': 3805, 'sleeve': 3806, 'sleight': 3807, 'slept': 3808, 'slice': 3809, 'slicker': 3810, 'slickly': 3811, 'slides': 3812, 'slight': 3813, 'slightly': 3814, 'slinks': 3815, 'slipped': 3816, 'slipping': 3817, 'slips': 3818, 'slipshod': 3819, 'slit': 3820, 'slow': 3821, 'slowly': 3822, 'slows': 3823, 'slumber': 3824, 'sly': 3825, 'small': 3826, 'smaller': 3827, 'smalltown': 3828, 'smell': 3829, 'smells': 3830, 'smile': 3831, 'smiles': 3832, 'smiling': 3833, 'smoke': 3834, 'smokes': 3835, 'smokey': 3836, 'smoking': 3837, 'smorgasbord': 3838, 'snacking': 3839, 'snake': 3840, 'snaps': 3841, 'snapshot': 3842, 'sneaks': 3843, 'sneers': 3844, 'sniffing': 3845, 'sniffs': 3846, 'snooping': 3847, 'snowballs': 3848, 'snuck': 3849, 'snuffling': 3850, 'so': 3851, 'soap': 3852, 'soar': 3853, 'soaring': 3854, 'sob': 3855, 'sociable': 3856, 'social': 3857, 'society': 3858, 'sock': 3859, 'sockets': 3860, 'socks': 3861, 'sofa': 3862, 'sofas': 3863, 'soft': 3864, 'softie': 3865, 'softly': 3866, 'soggy': 3867, 'soiled': 3868, 'solid': 3869, 'solo': 3870, 'solution': 3871, 'solutions': 3872, 'solve': 3873, 'some': 3874, 'somebody': 3875, 'someday': 3876, 'someone': 3877, 'someplace': 3878, 'something': 3879, 'sometime': 3880, 'sometimes': 3881, 'somewhere': 3882, 'son': 3883, 'song': 3884, 'sonny': 3885, 'soon': 3886, 'sooner': 3887, 'soothing': 3888, 'sophistication': 3889, 'sorry': 3890, 'sort': 3891, 'sorts': 3892, 'sos': 3893, 'sotto': 3894, 'sought': 3895, 'soul': 3896, 'souls': 3897, 'sound': 3898, 'soundly': 3899, 'sounds': 3900, 'sour': 3901, 'source': 3902, 'south': 3903, 'southeast': 3904, 'space': 3905, 'spacious': 3906, 'spanky': 3907, 'spark': 3908, 'sparkling': 3909, 'sparkwood': 3910, 'sparky': 3911, 'sparsely': 3912, 'speak': 3913, 'speaker': 3914, 'speaking': 3915, 'speaks': 3916, 'spears': 3917, 'special': 3918, 'species': 3919, 'specific': 3920, 'specifically': 3921, 'specifics': 3922, 'speech': 3923, 'speechless': 3924, 'speed': 3925, 'speeding': 3926, 'spell': 3927, 'spells': 3928, 'spend': 3929, 'spent': 3930, 'spilled': 3931, 'spills': 3932, 'spin': 3933, 'spins': 3934, 'spirit': 3935, 'spirits': 3936, 'spiritual': 3937, 'spite': 3938, 'spoke': 3939, 'spoken': 3940, 'sponges': 3941, 'spooky': 3942, 'spool': 3943, 'spooling': 3944, 'spoon': 3945, 'sports': 3946, 'spot': 3947, 'spots': 3948, 'spotting': 3949, 'spread': 3950, 'spring': 3951, 'squad': 3952, 'square': 3953, 'squarejaw': 3954, 'squares': 3955, 'squaw': 3956, 'squawks': 3957, 'squeal': 3958, 'squeezed': 3959, 'squeezing': 3960, 'squints': 3961, 'squish': 3962, 'stack': 3963, 'stainless': 3964, 'stairs': 3965, 'stairway': 3966, 'stairwell': 3967, 'staked': 3968, 'staking': 3969, 'stalemate': 3970, 'stand': 3971, 'standing': 3972, 'stands': 3973, 'stare': 3974, 'stares': 3975, 'staring': 3976, 'stars': 3977, 'start': 3978, 'started': 3979, 'starting': 3980, 'startled': 3981, 'startles': 3982, 'starts': 3983, 'stash': 3984, 'stashed': 3985, 'stashes': 3986, 'state': 3987, 'stately': 3988, 'statement': 3989, 'stating': 3990, 'station': 3991, 'statistic': 3992, 'stay': 3993, 'staying': 3994, 'steady': 3995, 'steak': 3996, 'stealing': 3997, 'steaming': 3998, 'steamy': 3999, 'steel': 4000, 'steers': 4001, 'stenciled': 4002, 'steno': 4003, 'step': 4004, 'stepped': 4005, 'steppin': 4006, 'stepping': 4007, 'steps': 4008, 'stereo': 4009, 'stern': 4010, 'steve': 4011, 'stick': 4012, 'sticking': 4013, 'sticks': 4014, 'stiff': 4015, 'stiffens': 4016, 'stifles': 4017, 'still': 4018, 'stilted': 4019, 'stitches': 4020, 'stomach': 4021, 'stood': 4022, 'stop': 4023, 'stopped': 4024, 'stops': 4025, 'store': 4026, 'stories': 4027, 'storm': 4028, 'storms': 4029, 'story': 4030, 'stove': 4031, 'straight': 4032, 'strains': 4033, 'strange': 4034, 'strangely': 4035, 'stranger': 4036, 'stray': 4037, 'streaming': 4038, 'streamliner': 4039, 'street': 4040, 'streetcars': 4041, 'streets': 4042, 'strength': 4043, 'stretches': 4044, 'strict': 4045, 'strides': 4046, 'strikes': 4047, 'strikingly': 4048, 'string': 4049, 'strokes': 4050, 'strong': 4051, 'struck': 4052, 'structure': 4053, 'structures': 4054, 'struggles': 4055, 'struggling': 4056, 'strung': 4057, 'stuck': 4058, 'studies': 4059, 'study': 4060, 'studying': 4061, 'stuff': 4062, 'stunned': 4063, 'stunt': 4064, 'stupid': 4065, 'stupidity': 4066, 'style': 4067, 'stylized': 4068, 'stylus': 4069, 'sub': 4070, 'subconsciously': 4071, 'subdued': 4072, 'subject': 4073, 'submission': 4074, 'submitted': 4075, 'substitute': 4076, 'subtle': 4077, 'suburbis': 4078, 'successful': 4079, 'such': 4080, 'sucker': 4081, 'suction': 4082, 'sudden': 4083, 'suddenly': 4084, 'sue': 4085, 'suffer': 4086, 'suffered': 4087, 'suffice': 4088, 'suggest': 4089, 'suggesting': 4090, 'suggests': 4091, 'suicide': 4092, 'suit': 4093, 'suitcase': 4094, 'suitcases': 4095, 'suite': 4096, 'suits': 4097, 'sultan': 4098, 'summer': 4099, 'summing': 4100, 'sun': 4101, 'sunday': 4102, 'sundays': 4103, 'sunglasses': 4104, 'sunlight': 4105, 'super': 4106, 'supervises': 4107, 'supervisor': 4108, 'supper': 4109, 'supply': 4110, 'support': 4111, 'suppose': 4112, 'supposed': 4113, 'sure': 4114, 'surface': 4115, 'surgical': 4116, 'surprise': 4117, 'surprised': 4118, 'surprisingly': 4119, 'surrounded': 4120, 'surrounding': 4121, 'surrounds': 4122, 'suspect': 4123, 'suspects': 4124, 'suspended': 4125, 'suspense': 4126, 'suspicion': 4127, 'suspicious': 4128, 'swabbie': 4129, 'swallows': 4130, 'swaying': 4131, 'sways': 4132, 'swear': 4133, 'sweating': 4134, 'sweats': 4135, 'swede': 4136, 'sweeper': 4137, 'sweepers': 4138, 'sweeps': 4139, 'sweet': 4140, 'sweetheart': 4141, 'sweetly': 4142, 'sweetness': 4143, 'swells': 4144, 'swim': 4145, 'swing': 4146, 'swinging': 4147, 'swings': 4148, 'switchblade': 4149, 'switchboard': 4150, 'switches': 4151, 'swivels': 4152, 'swizzle': 4153, 'swoops': 4154, 'sylvia': 4155, 'sympathetic': 4156, 'sympathy': 4157, 'syringe': 4158, 'syrup': 4159, 'syrupy': 4160, 'system': 4161, 't': 4162, 't.': 4163, 'ta': 4164, 'tab': 4165, 'table': 4166, 'tables': 4167, 'tacked': 4168, 'tackle': 4169, 'tad': 4170, 'tag': 4171, 'tail': 4172, 'tails': 4173, 'take': 4174, 'taken': 4175, 'takes': 4176, 'taking': 4177, 'talk': 4178, 'talked': 4179, 'talkie': 4180, 'talking': 4181, 'talks': 4182, 'talky': 4183, 'tall': 4184, 'tallied': 4185, 'tao': 4186, 'tape': 4187, 'taps': 4188, 'target': 4189, 'targeted': 4190, 'targets': 4191, 'tarp': 4192, 'taste': 4193, 'tattoo': 4194, 'taught': 4195, 'teach': 4196, 'teak': 4197, 'team': 4198, 'tear': 4199, 'tearful': 4200, 'tears': 4201, 'tearyeyed': 4202, 'technique': 4203, 'teenage': 4204, 'teeth': 4205, 'telephone': 4206, 'telescopic': 4207, 'telescoping': 4208, 'television': 4209, 'televison': 4210, 'tell': 4211, 'telling': 4212, 'tells': 4213, 'temperate': 4214, 'temperature': 4215, 'tempered': 4216, 'tempestuous': 4217, 'tempo': 4218, 'ten': 4219, 'tender': 4220, 'tenderly': 4221, 'tending': 4222, 'tennessee': 4223, 'tense': 4224, 'tentative': 4225, 'tentatively': 4226, 'term': 4227, 'terrible': 4228, 'terribly': 4229, 'terrified': 4230, 'test': 4231, 'testament': 4232, 'testing': 4233, 'tests': 4234, 'than': 4235, 'thank': 4236, 'thankful': 4237, 'thanks': 4238, 'that': 4239, 'the': 4240, 'the-(leaving': 4241, 'thee': 4242, 'their': 4243, 'them': 4244, 'themed': 4245, 'themselves': 4246, 'then': 4247, 'there': 4248, 'thereafter': 4249, 'therefore': 4250, 'therenadine': 4251, 'theresa': 4252, 'these': 4253, 'they': 4254, \"they're\": 4255, 'thin': 4256, 'thing': 4257, 'things': 4258, \"things'll\": 4259, \"things're\": 4260, 'think': 4261, 'thinking': 4262, 'thinks': 4263, 'third': 4264, 'thirsty': 4265, 'thirty': 4266, 'this': 4267, 'this--': 4268, 'thorn': 4269, 'those': 4270, 'thou': 4271, 'though': 4272, 'thought': 4273, 'thoughts': 4274, 'thousand': 4275, 'thousands': 4276, 'thread': 4277, 'threat': 4278, 'three': 4279, 'threesome': 4280, 'throat': 4281, 'throng': 4282, 'through': 4283, 'throughout': 4284, 'throwing': 4285, 'throws': 4286, 'thru': 4287, 'thuds': 4288, 'thumb': 4289, 'thumbnail': 4290, 'thumbs': 4291, 'thump': 4292, 'thursday': 4293, 'thy': 4294, 'tibet': 4295, 'tibetan': 4296, 'ticket': 4297, 'tie': 4298, 'tied': 4299, 'ties': 4300, 'tight': 4301, 'til': 4302, 'till': 4303, 'tilt': 4304, 'tilting': 4305, 'timber': 4306, 'time': 4307, 'timed': 4308, 'timer': 4309, 'times': 4310, 'tip': 4311, 'tips': 4312, 'tiptoe': 4313, 'tired': 4314, 'tires': 4315, 'title': 4316, 'titles': 4317, 'to': 4318, 'toad': 4319, 'toast': 4320, 'tobacco': 4321, 'today': 4322, 'tofive': 4323, 'together': 4324, 'told': 4325, 'tolerant': 4326, 'tomes': 4327, 'tommy': 4328, 'tomorrow': 4329, 'tongue': 4330, 'tonight': 4331, 'tonk': 4332, 'too': 4333, 'took': 4334, 'toot': 4335, 'tooth': 4336, 'top': 4337, 'topic': 4338, 'torch': 4339, 'torches': 4340, 'torment': 4341, 'toss': 4342, 'tosses': 4343, 'tossin': 4344, 'total': 4345, 'tottering': 4346, 'totters': 4347, 'touch': 4348, 'touched': 4349, 'touches': 4350, 'tough': 4351, 'tour': 4352, 'toward': 4353, 'towards': 4354, 'towel': 4355, 'towering': 4356, 'towers': 4357, 'town': 4358, 'townsfolk': 4359, 'toxicology': 4360, 'toxological': 4361, 'toys': 4362, 'traces': 4363, 'track': 4364, 'tracked': 4365, 'tracker': 4366, 'trading': 4367, 'traditional': 4368, 'traffic': 4369, 'tragedies': 4370, 'tragedy': 4371, 'train': 4372, 'trained': 4373, 'trains': 4374, 'trait': 4375, 'transactions': 4376, 'transfer': 4377, 'transformer': 4378, 'translator': 4379, 'transmission': 4380, 'trash': 4381, 'travel': 4382, 'traveled': 4383, 'traveler': 4384, 'traveling': 4385, 'travels': 4386, 'tray': 4387, 'treacherous': 4388, 'treat': 4389, 'treating': 4390, 'treats': 4391, 'tree': 4392, 'trees': 4393, 'trellis': 4394, 'tremble': 4395, 'trembles': 4396, 'trembling': 4397, 'tri': 4398, 'trick': 4399, 'tried': 4400, 'tries': 4401, 'trigger': 4402, 'trips': 4403, 'triumphant': 4404, 'trooper': 4405, 'trouble': 4406, 'troubled': 4407, 'troubling': 4408, 'trout': 4409, 'troy': 4410, 'truck': 4411, 'trucker': 4412, 'trudy': 4413, 'true': 4414, 'truly': 4415, 'truman': 4416, 'trumpet': 4417, 'trust': 4418, 'trusted': 4419, 'truth': 4420, 'try': 4421, 'trying': 4422, 'tube': 4423, 'tucked': 4424, 'tuesday': 4425, 'tugs': 4426, 'tumbler': 4427, 'tumbles': 4428, 'tumult': 4429, 'tundra': 4430, 'tune': 4431, 'tuned': 4432, 'turkey': 4433, 'turkeys': 4434, 'turn': 4435, 'turned': 4436, 'turning': 4437, 'turnover': 4438, 'turnpike': 4439, 'turns': 4440, 'turntable': 4441, 'tutor': 4442, 'tutored': 4443, 'tv': 4444, 'twelve': 4445, 'twenty': 4446, 'twentyfive': 4447, 'twice': 4448, 'twin': 4449, 'twine': 4450, 'twinkle': 4451, 'twist': 4452, 'twists': 4453, 'two': 4454, 'tycoons': 4455, 'u.s.': 4456, 'ugly': 4457, 'uh': 4458, 'unaffected': 4459, 'unafraid': 4460, 'unarmed': 4461, 'unbutton': 4462, 'unbuttoning': 4463, 'uncertain': 4464, 'uncle': 4465, 'uncomfortable': 4466, 'uncomfortably': 4467, 'uncomplicated': 4468, 'uncovers': 4469, 'under': 4470, 'undercover': 4471, 'understand': 4472, 'understanding': 4473, 'understands': 4474, 'undeterred': 4475, 'undone': 4476, 'undressing': 4477, 'uneasily': 4478, 'uneasy': 4479, 'unfocused': 4480, 'unforeseen': 4481, 'unhandcuffed': 4482, 'unidentified': 4483, 'uniform': 4484, 'unknown': 4485, 'unless': 4486, 'unloading': 4487, 'unnoticed': 4488, 'unprofessional': 4489, 'unseen': 4490, 'unsettled': 4491, 'unshaven': 4492, 'unsolvable': 4493, 'unsophisticated': 4494, 'unsteadily': 4495, 'unsure': 4496, 'untied': 4497, 'until': 4498, 'unto': 4499, \"untrimm'd\": 4500, 'untroubled': 4501, 'unusual': 4502, 'unwavering': 4503, 'unwraps': 4504, 'up': 4505, 'upon': 4506, 'upper': 4507, 'upright': 4508, 'uprising': 4509, 'ups': 4510, 'upset': 4511, 'upside': 4512, 'upstairs': 4513, 'upstream': 4514, 'urgent': 4515, 'urgently': 4516, 'urinate': 4517, 'us': 4518, 'use': 4519, 'used': 4520, 'uses': 4521, 'using': 4522, 'usual': 4523, 'usually': 4524, 'uturn': 4525, 'v': 4526, 'vacation': 4527, 'vagrant': 4528, 'vain': 4529, 'value': 4530, 'vampires': 4531, 'vanilla': 4532, 'vanished': 4533, 'variety': 4534, 'varsity': 4535, 'vase': 4536, 'vcr': 4537, 'vehicle': 4538, 'venality': 4539, 'verification': 4540, 'very': 4541, 'veterinarian': 4542, 'vets': 4543, 'vette': 4544, 'vicious': 4545, 'video': 4546, 'videotape': 4547, 'view': 4548, 'vigorously': 4549, 'vikings': 4550, 'violation': 4551, 'violently': 4552, 'violins': 4553, 'virgin': 4554, 'vis': 4555, 'visa': 4556, 'visible': 4557, 'vision': 4558, 'visions': 4559, 'visit': 4560, 'visiting': 4561, 'voce': 4562, 'voice': 4563, 'void': 4564, 'voltage': 4565, 'volume': 4566, 'vowed': 4567, 'vulnerable': 4568, 'wage': 4569, 'wagon': 4570, 'wailing': 4571, 'waist': 4572, 'wait': 4573, \"wait'll\": 4574, 'waitin': 4575, 'waiting': 4576, 'waitress': 4577, 'waits': 4578, 'wake': 4579, 'waken': 4580, 'waking': 4581, 'walk': 4582, 'walkie': 4583, 'walking': 4584, 'walks': 4585, 'walky': 4586, 'wall': 4587, 'wallet': 4588, 'walls': 4589, 'waltz': 4590, 'wan': 4591, 'wander': 4592, \"wander'st\": 4593, 'wanders': 4594, 'want': 4595, 'wanted': 4596, 'wants': 4597, 'war': 4598, 'ward': 4599, 'warm': 4600, 'warmly': 4601, 'warmup': 4602, 'warn': 4603, 'warning': 4604, 'warrants': 4605, 'was': 4606, 'was-(thinks': 4607, 'wash': 4608, 'washed': 4609, 'washer': 4610, 'washington': 4611, 'wasted': 4612, 'watch': 4613, 'watches': 4614, 'watching': 4615, 'water': 4616, 'waterfall': 4617, 'watson': 4618, 'waves': 4619, 'wax': 4620, 'waxing': 4621, 'way': 4622, 'ways': 4623, 'we': 4624, \"we're\": 4625, 'weak': 4626, 'weakly': 4627, 'weakness': 4628, 'weapon': 4629, 'weapons': 4630, 'wear': 4631, 'wearing': 4632, 'wearinghis-': 4633, 'wears': 4634, 'weathered': 4635, 'wednesday': 4636, 'week': 4637, 'weeks': 4638, 'weeps': 4639, 'weepy': 4640, 'weighs': 4641, 'weight': 4642, 'weird': 4643, 'weirdo': 4644, 'welcome': 4645, 'well': 4646, 'went': 4647, 'were': 4648, 'west': 4649, 'western': 4650, 'wet': 4651, 'what': 4652, 'whatever': 4653, 'wheat': 4654, 'wheel': 4655, 'wheelchair': 4656, 'wheels': 4657, 'when': 4658, 'where': 4659, 'whether': 4660, 'which': 4661, 'while': 4662, 'whine': 4663, 'whips': 4664, 'whisks': 4665, 'whisper': 4666, 'whispered': 4667, 'whispering': 4668, 'whispers': 4669, 'whistle': 4670, 'whistles': 4671, 'whistling': 4672, 'white': 4673, 'whittle': 4674, 'whittled': 4675, 'whittling': 4676, 'who': 4677, 'whole': 4678, 'whom': 4679, 'whosoever': 4680, 'why': 4681, 'wide': 4682, 'widen': 4683, 'widow': 4684, 'wienie': 4685, 'wife': 4686, 'wiggles': 4687, 'wild': 4688, 'wildly': 4689, 'will': 4690, 'willing': 4691, 'willow': 4692, 'wilson': 4693, 'wind': 4694, 'window': 4695, 'windowless': 4696, 'windows': 4697, 'winds': 4698, 'wine': 4699, 'wing': 4700, 'wings': 4701, 'wink': 4702, 'winks': 4703, 'wipe': 4704, 'wipes': 4705, 'wiry': 4706, 'wisdom': 4707, 'wise': 4708, 'wish': 4709, 'wished': 4710, 'wishes': 4711, 'with': 4712, 'withdraws': 4713, 'within': 4714, 'without': 4715, 'witnessed': 4716, 'witnessing': 4717, 'wits': 4718, 'wizened': 4719, 'wo': 4720, 'woke': 4721, 'woken': 4722, 'woman': 4723, 'women': 4724, 'women--': 4725, 'wonder': 4726, 'wonderful': 4727, 'wonders': 4728, 'wood': 4729, 'wooded': 4730, 'woods': 4731, 'word': 4732, 'words': 4733, 'work': 4734, 'workboots': 4735, 'worked': 4736, 'workers': 4737, 'working': 4738, 'workout': 4739, 'works': 4740, 'world': 4741, 'worldly': 4742, 'worlds': 4743, 'worn': 4744, 'worried': 4745, 'worry': 4746, 'worse': 4747, 'worth': 4748, 'would': 4749, 'wound': 4750, 'wounded': 4751, 'wounds': 4752, 'wrapped': 4753, 'wrench': 4754, 'wrenches': 4755, 'wrist': 4756, 'wrists': 4757, 'wristwatch': 4758, 'write': 4759, 'writes': 4760, 'writing': 4761, 'written': 4762, 'wrong': 4763, 'wrote': 4764, 'y': 4765, 'ya': 4766, 'yard': 4767, 'yeah': 4768, 'year': 4769, 'yearns': 4770, 'yearold': 4771, 'years': 4772, 'yelling': 4773, 'yellow': 4774, 'yells': 4775, 'yep': 4776, 'yes': 4777, 'yesterday': 4778, 'yesterday--': 4779, 'yet': 4780, 'yip': 4781, 'yoga': 4782, 'yokel': 4783, 'you': 4784, \"you're\": 4785, 'young': 4786, 'younger': 4787, 'your': 4788, 'yours': 4789, 'yourself': 4790, 'z.': 4791, 'zap': 4792, 'zip': 4793}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriCBCC9WOuO"
      },
      "source": [
        "## create sequences\n",
        "Now, we have to create the input data for our LSTM. We create two lists:\n",
        " - **sequences**: this list will contain the sequences of words used to train the model,\n",
        " - **next_words**: this list will contain the next words for each sequences of the **sequences** list.\n",
        " \n",
        "In this exercice, we assume we will train the network with sequences of 30 words (seq_length = 30).\n",
        "\n",
        "So, to create the first sequence of words, we take the 30th first words in the **wordlist** list. The word 31 is the next word of this first sequence, and is added to the **next_words** list.\n",
        "\n",
        "Then we jump by a step of 1 (sequences_step = 1 in our example) in the list of words, to create the second sequence of words and retrieve the second \"next word\".\n",
        "\n",
        "We iterate this task until the end of the list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAecFMYhWOuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310faed2-5057-44fa-fe1b-af133fde5299"
      },
      "source": [
        "#create sequences\n",
        "sequences = []\n",
        "next_words = []\n",
        "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
        "    sequences.append(wordlist[i: i + seq_length])\n",
        "    next_words.append(wordlist[i + seq_length])\n",
        "\n",
        "print('nb sequences:', len(sequences))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 51156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZZMPiXqWOuQ"
      },
      "source": [
        "When we iterate over the whole list of words, we create 172104 sequences of words, and retrieve, for each of them, the next word to be predicted.\n",
        "\n",
        "However, these lists cannot be used \"as is\". We have to transform them in order to ingest them in the LSTM. Text will not be understood by neural net, we have to use digits.\n",
        "However, we cannot only map a words to its index in the vocabulary, as it does not represent intrasinqly the word. It is better to reorganize a sequence of words as a matrix of booleans.\n",
        "\n",
        "So, we create the matrix X and y :\n",
        " - X : the matrix of the following dimensions:\n",
        "     - number of sequences,\n",
        "     - number of words in sequences,\n",
        "     - number of words in the vocabulary.\n",
        " - y : the matrix of the following dimensions:\n",
        "     - number of sequences,\n",
        "     - number of words in the vocabulary.\n",
        " \n",
        "For each word, we retrieve its index in the vocabulary, and we set to 1 its position in the matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB6Js0cAWOuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770ed283-d42f-4f0a-e0e0-3d5fadd9073e"
      },
      "source": [
        "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
        "for i, sentence in enumerate(sequences):\n",
        "    for t, word in enumerate(sentence):\n",
        "        X[i, t, vocab[word]] = 1\n",
        "    y[i, vocab[next_words[i]]] = 1\n",
        "\n",
        "print(np.shape(X))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51156, 30, 4794)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9kAzHF1WOuR"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaYn6zl3WOuS"
      },
      "source": [
        "Now, here come the fun part. The creation of the neural network.\n",
        "As you will see, I am using Keras which provide very good abstraction to design an architecture.\n",
        "\n",
        "In this example, I create the following neural network:\n",
        " - bidirectional LSTM,\n",
        " - with size of 256 and using RELU as activation,\n",
        " - then a dropout layer of 0,6 (it's pretty high, but necesseray to avoid quick divergence)\n",
        " \n",
        "\n",
        "The net should provide me a probability for each word of the vocabulary to be the next one after a given sentence. So I end it with:\n",
        "\n",
        " - a simple dense layer of the size of the vocabulary,\n",
        " - a softmax activation.\n",
        " \n",
        "I use ADAM as otpimizer and the loss calculation is done on the categorical crossentropy.\n",
        "\n",
        "Here is the function to build the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_yzdAy6vWOuS"
      },
      "source": [
        "def bidirectional_lstm_model(seq_length, vocab_size, rnn_size, batch_size, learning_rate):\n",
        "    print('Build LSTM model.')\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(1024, activation=\"tanh\", recurrent_activation = \"sigmoid\",return_sequences=True, recurrent_dropout = 0, use_bias=True,unroll= False),input_shape=(seq_length, vocab_size)))\n",
        "    model.add(Bidirectional(LSTM(rnn_size, activation=\"tanh\", recurrent_activation = \"sigmoid\", recurrent_dropout = 0, use_bias=True,unroll= False),input_shape=(seq_length, vocab_size)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iUluxwv5WOuT"
      },
      "source": [
        "# #Hiper-parametros\n",
        "\n",
        "# rnn_size = 256 # size of RNN\n",
        "# batch_size = 32 # minibatch size\n",
        "# seq_length = seq_length # sequence length\n",
        "# num_epochs = 3 # number of epochs\n",
        "# learning_rate = 0.001 #learning rate\n",
        "# sequences_step = 1 #step to create sequences"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpxNZxQeWOuT"
      },
      "source": [
        "# md = bidirectional_lstm_model(seq_length, vocab_size, rnn_size, batch_size, learning_rate)\n",
        "# print(md.summary())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blH8DiXZWOuT"
      },
      "source": [
        "If a print the summary of this model, you can see it has close to 61 millions of trainable parameters. It is huge, and the compute will take some time to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AFdvB03WOuU"
      },
      "source": [
        "## train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsALVBmdWOuU"
      },
      "source": [
        "Enough speech, we train the model now. We shuffle the training set and extract 10% of it as validation sample. We simply run :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vql6vZJQWOuU"
      },
      "source": [
        "def fit_model(num_epochs, batch):\n",
        "  #fit the model\n",
        "  callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
        "            ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences_lstm.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
        "                            monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
        "\n",
        "  history = md.fit(X, y,\n",
        "                  batch_size=batch,\n",
        "                  shuffle=True,\n",
        "                  epochs=num_epochs,\n",
        "                  callbacks=[callbacks],\n",
        "                  validation_split=0.1)\n",
        "\n",
        "  #save the model\n",
        "  md.save(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')\n",
        "  return history"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do6n9-T3WOuV"
      },
      "source": [
        "# Generate phrase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6OyIAnxWOuW"
      },
      "source": [
        "Great !\n",
        "We have now trained a model to predict the next word of a given sequence of words. In order to generate text, the task is pretty simple:\n",
        "\n",
        " - we define a \"seed\" sequence of 30 words (30 is the number of words required by the neural net for the sequences),\n",
        " - we ask the neural net to predict word number 31,\n",
        " - then we update the sequence by moving words by a step of 1, adding words number 31 at its end,\n",
        " - we ask the neural net to predict word number 32,\n",
        " - etc. For as long as we want.\n",
        " \n",
        "Doing this, we generate phrases, word by word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVht5nitWOuW"
      },
      "source": [
        "def load_vocabulary():\n",
        "  #load vocabulary\n",
        "  print(\"loading vocabulary...\")\n",
        "  vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
        "\n",
        "  with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
        "          words, vocab, vocabulary_inv = cPickle.load(f)\n",
        "\n",
        "  vocab_size = len(words)\n",
        "  return vocab_size"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uTwG2VeWOuW"
      },
      "source": [
        "def load_the_model():\n",
        "  # load the model\n",
        "  print(\"loading model...\")\n",
        "  model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')\n",
        "  return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUc3XgxPWOuX"
      },
      "source": [
        "To improve the word generation, and tune a bit the prediction, we introduce a specific function to pick-up words.\n",
        "\n",
        "We will not take the words with the highest prediction (or the generation of text will be boring), but would like to insert some uncertainties, and let the solution sometime pick-up words with less good prediction.\n",
        "\n",
        "That is the purpose of the function **sample**, that will draw radomly a word from the vocabulary.\n",
        "\n",
        "The probabilty for a word to be drawn will depends directly on its probability to be the next word. In order to tune this probability, we introduce a \"temperature\" to smooth or sharpen its value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Npi--Zf6WOuY"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qHC-i-HWOuY"
      },
      "source": [
        "#initiate sentences\n",
        "def initiate_sentences(sentence, generated):\n",
        "  seed_sentences = 'but the true test of any hotel , as you know , is that morning cup of coffee . '\n",
        "  # generated = ''\n",
        "  # sentence = []\n",
        "  for i in range (seq_length):\n",
        "      sentence.append(\"a\")\n",
        "\n",
        "  seed = seed_sentences.split()\n",
        "\n",
        "  for i in range(len(seed)):\n",
        "      sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
        "\n",
        "  generated += ' '.join(sentence)\n",
        "  #print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
        "  return sentence, generated"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2j6_mxuWOuZ"
      },
      "source": [
        "def generate_text(sentence, generated):\n",
        "  words_number = 300\n",
        "  #generate the text\n",
        "  for i in range(words_number):\n",
        "      #create the vector\n",
        "      x = np.zeros((1, seq_length, vocab_size))\n",
        "      for t, word in enumerate(sentence):\n",
        "          x[0, t, vocab[word]] = 1.\n",
        "      #print(x.shape)\n",
        "\n",
        "      #calculate next word\n",
        "      preds = model.predict(x, verbose=0)[0]\n",
        "      next_index = sample(preds, 0.34)\n",
        "      next_word = vocabulary_inv[next_index]\n",
        "\n",
        "      #add the next word to the text\n",
        "      generated += \" \" + next_word\n",
        "      # shift the sentence by one, and and the next word at its end\n",
        "      sentence = sentence[1:] + [next_word]\n",
        "\n",
        "  #print(generated)\n",
        "  return sentence, generated\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOyPBlQwqS8-"
      },
      "source": [
        "def post_process(generated):\n",
        "  generated = generated.replace(\" .\", \".\\n\")\n",
        "  generated = generated.replace(\" )\", \")\")\n",
        "  generated = generated.replace(\"( \", \"(\")\n",
        "  generated = generated.replace(\" '\", \"'\")\n",
        "  generated = generated.replace(\" , \", \", \")\n",
        "  generated = generated.replace(\" : \", \": \")\n",
        "  generated = generated.replace(\" ? \", \"? \")\n",
        "  generated = generated.replace(\"did n't\", \"didn't\")\n",
        "  generated = generated.replace(\"should n't\", \"shouldn't\")\n",
        "  generated = generated.replace(\"do n't\", \"don't\")\n",
        "  generated = generated.replace(\"could n't\", \"couldn't\")\n",
        "  generated = generated.replace(\"gon na\", \"gonna\")\n",
        "  generated = generated.replace(\"got ta\", \"gotta\")\n",
        "  generated = generated.replace(\"was n't\", \"wasn't\")\n",
        "\n",
        "  print(generated)\n",
        "  return generated"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL5YFHqgM4pe"
      },
      "source": [
        "def save_metrics(rnn_size, batch_size, seq_length, num_epochs, learning_rate, sequences_step, history, generated, md):\n",
        "  # Stream the metrics to a file in JSON format.\n",
        "  json_log = open(save_dir + \"/\" +'log' + '_R' + str(rnn_size) + '_B' + str(batch_size) + '_L' + str(learning_rate) + '.json', mode='wt', buffering=1)\n",
        "  json_log.write(\n",
        "          json.dumps({'rnn_size': rnn_size, \n",
        "                      'batch_size': batch_size,\n",
        "                      'seq_length': seq_length, \n",
        "                      'num_epochs': num_epochs, \n",
        "                      'learning_rate': learning_rate, \n",
        "                      'sequences_step': sequences_step, \n",
        "\n",
        "                      'history': history.history,\n",
        "\n",
        "                      'predicted_text': generated\n",
        "          })\n",
        "  )\n",
        "  json_log.close()\n",
        "\n",
        "  # Open the file\n",
        "  with open(save_dir + \"/\" +'model_summary' + '_R' + str(rnn_size) + '_B' + str(batch_size) + '_L' + str(learning_rate) + '.txt', mode='w') as fh:\n",
        "      # Pass the file handle in as a lambda function to make it callable\n",
        "      model.summary(line_length=80,print_fn=lambda x: fh.write(x + '\\n')) "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQVj9yv8UoOC"
      },
      "source": [
        "Combinaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdeYI3Zr_HUs"
      },
      "source": [
        "#Hiper-parametros\n",
        "\n",
        "# rnn_size = [128, 256, 512] # size of RNN\n",
        "# batch_size = [8, 16, 32, 64] # minibatch size\n",
        "rnn_size = [256, 512, 1024] # size of RNN\n",
        "batch_size = [32, 64] # minibatch size\n",
        "\n",
        "\n",
        "seq_length = seq_length # sequence length\n",
        "num_epochs = 50 # number of epochs\n",
        "learning_rate = 0.001 #learning rate\n",
        "sequences_step = 1 #step to create sequences"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_WfnULTUnb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8453e6-bc71-4b6f-dcce-ecfb654dca37"
      },
      "source": [
        "for rnn in rnn_size:\n",
        "  for batch in batch_size:\n",
        "    md = bidirectional_lstm_model(seq_length, vocab_size, rnn, batch, learning_rate)\n",
        "    print(md.summary())\n",
        "    print('RNN Size= ' + str(rnn), ' Batch Size= ' + str(batch), ' Learning Rate= ' + str(learning_rate))\n",
        "    history = fit_model(num_epochs, batch)\n",
        "    #load_vocabulary()\n",
        "    model = load_the_model()\n",
        "    generated = ''\n",
        "    \n",
        "    sentence = []\n",
        "    sentence, generated = initiate_sentences(sentence, generated)\n",
        "    sentence, generated = generate_text(sentence, generated)\n",
        "    generated = post_process(generated)\n",
        "    save_metrics(rnn, batch, seq_length, num_epochs, learning_rate, sequences_step, history, generated, md) "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build LSTM model.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 30, 2048)          47669248  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 512)               4720640   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 54,849,210\n",
            "Trainable params: 54,849,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "RNN Size= 256  Batch Size= 32  Learning Rate= 0.001\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1439/1439 [==============================] - 190s 115ms/step - loss: 6.4152 - categorical_accuracy: 0.0691 - val_loss: 5.8586 - val_categorical_accuracy: 0.1048\n",
            "Epoch 2/50\n",
            "1439/1439 [==============================] - 166s 115ms/step - loss: 5.7417 - categorical_accuracy: 0.1096 - val_loss: 5.6656 - val_categorical_accuracy: 0.1364\n",
            "Epoch 3/50\n",
            "1439/1439 [==============================] - 166s 116ms/step - loss: 5.4135 - categorical_accuracy: 0.1374 - val_loss: 5.6251 - val_categorical_accuracy: 0.1499\n",
            "Epoch 4/50\n",
            "1439/1439 [==============================] - 166s 115ms/step - loss: 5.1928 - categorical_accuracy: 0.1520 - val_loss: 5.6162 - val_categorical_accuracy: 0.1587\n",
            "Epoch 5/50\n",
            "1439/1439 [==============================] - 166s 116ms/step - loss: 5.0257 - categorical_accuracy: 0.1641 - val_loss: 5.6307 - val_categorical_accuracy: 0.1755\n",
            "Epoch 6/50\n",
            "1439/1439 [==============================] - 166s 115ms/step - loss: 4.8372 - categorical_accuracy: 0.1840 - val_loss: 5.7133 - val_categorical_accuracy: 0.1730\n",
            "Epoch 7/50\n",
            "1439/1439 [==============================] - 167s 116ms/step - loss: 4.6615 - categorical_accuracy: 0.1989 - val_loss: 5.7428 - val_categorical_accuracy: 0.1740\n",
            "Epoch 8/50\n",
            "1439/1439 [==============================] - 166s 116ms/step - loss: 4.5130 - categorical_accuracy: 0.2066 - val_loss: 5.8917 - val_categorical_accuracy: 0.1783\n",
            "loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a a a a a a a a a a a but the true test of any hotel, as you know, is that morning cup of coffee.\n",
            " cooper and the dining room - day ed hurley's voice (a spirit.\n",
            " catherine hurley's voice (a beat.\n",
            " cooper (the phone.\n",
            " cooper (his arms.\n",
            " truman (as leland palmer? cooper i'm not us.\n",
            " cooper (so, i'm not seen on the diner, sees her.\n",
            " cut to: int.\n",
            " hayward house - day that's a's house.\n",
            " cooper (, johnny.\n",
            " i'll have you have to be a little diner - night establish.\n",
            " truman i'm not a phone of the phone.\n",
            " cooper (was a set.\n",
            " he was, sees the table, we're to don't be a cigarette.\n",
            " truman i'm not the kitchen.\n",
            " cut to: int.\n",
            " hayward's house.\n",
            " he's voice (in the door.\n",
            " cooper (a dark.\n",
            " shelly (downstairs, he's voice (an bag.\n",
            " he's truck.\n",
            " cooper (to the phone.\n",
            " he's the phone.\n",
            " cooper i'm gonna get the door.\n",
            " truman (this store.\n",
            " bobby (sheriff truman's.\n",
            " cooper takes a couple of the couple, the head.\n",
            " he's voice (with her, but i'll be, i'm not a counter.\n",
            " cooper i'm not a cruiser, takes a beat.\n",
            " cooper (with the phone.\n",
            " cooper (the \" old.\n",
            " cooper (the man and shelly, i'm not a hint of my wife.\n",
            " cooper i'll get her.\n",
            " cooper and i'm not roll.\n",
            " i'm not a the door.\n",
            " cooper i was a team, there's a couple ,\n",
            "Build LSTM model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_2 (Bidirection (None, 30, 2048)          47669248  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 512)               4720640   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4794)              2459322   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 54,849,210\n",
            "Trainable params: 54,849,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "RNN Size= 256  Batch Size= 64  Learning Rate= 0.001\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "720/720 [==============================] - 149s 200ms/step - loss: 6.5033 - categorical_accuracy: 0.0663 - val_loss: 5.8669 - val_categorical_accuracy: 0.1013\n",
            "Epoch 2/50\n",
            "720/720 [==============================] - 141s 196ms/step - loss: 5.7460 - categorical_accuracy: 0.1065 - val_loss: 5.6138 - val_categorical_accuracy: 0.1272\n",
            "Epoch 3/50\n",
            "720/720 [==============================] - 141s 196ms/step - loss: 5.4419 - categorical_accuracy: 0.1332 - val_loss: 5.6288 - val_categorical_accuracy: 0.1323\n",
            "Epoch 4/50\n",
            "720/720 [==============================] - 141s 196ms/step - loss: 5.2671 - categorical_accuracy: 0.1398 - val_loss: 5.5067 - val_categorical_accuracy: 0.1529\n",
            "Epoch 5/50\n",
            "720/720 [==============================] - 141s 196ms/step - loss: 5.0360 - categorical_accuracy: 0.1581 - val_loss: 5.5452 - val_categorical_accuracy: 0.1648\n",
            "Epoch 6/50\n",
            "720/720 [==============================] - 141s 196ms/step - loss: 4.8749 - categorical_accuracy: 0.1688 - val_loss: 5.5467 - val_categorical_accuracy: 0.1677\n",
            "Epoch 7/50\n",
            "720/720 [==============================] - 142s 197ms/step - loss: 4.7207 - categorical_accuracy: 0.1837 - val_loss: 5.6099 - val_categorical_accuracy: 0.1673\n",
            "Epoch 8/50\n",
            "720/720 [==============================] - 142s 197ms/step - loss: 4.5942 - categorical_accuracy: 0.1973 - val_loss: 5.6494 - val_categorical_accuracy: 0.1763\n",
            "loading model...\n",
            "a a a a a a a a a a a but the true test of any hotel, as you know, is that morning cup of coffee.\n",
            " bobby you don't have you're going to see it up and you're going to get the phone.\n",
            " cooper what's a nurse.\n",
            " truman (we're that.\n",
            " cut to: int.\n",
            " hayward, i've got a small funeral to the counter.\n",
            " they should never.\n",
            " cooper i'm going to do.\n",
            " he's going to truman.\n",
            " cut to: ext.\n",
            " great's voice (when i'm going to the cigarette, i don't know, he's like a couple act the door.\n",
            " nadine (a door.\n",
            " cut to: int.\n",
            " great northern hotel - day cooper and i'll have to get a beat.\n",
            " cooper (the phone.\n",
            " cut to: int.\n",
            " great northern hotel - day cooper and we have to get the table and the kitchen) i've got out of the table, but you're.\n",
            " cut to: int.\n",
            " blue pine lodge - night cooper? truman (you're going to know what's a little house - night sarah's voice (takes a large hand at the hands.\n",
            " cooper and you have to do.\n",
            " cooper (a sketch of my pocket.\n",
            " cut to: int.\n",
            " blue pine lodge - night \" the same hands.\n",
            " truman i've got the \" of the more last night, i don't have to james.\n",
            " truman (a man, honey, i'll be a large killer.\n",
            " cooper is that.\n",
            " cooper (a beat.\n",
            " cut to: int.\n",
            " hayward and the other.\n",
            " cooper (a bag.\n",
            " cooper (takes the out of a red - night establish.\n",
            " cut\n",
            "Build LSTM model.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_4 (Bidirection (None, 30, 2048)          47669248  \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 1024)              10489856  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 63,072,954\n",
            "Trainable params: 63,072,954\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "RNN Size= 512  Batch Size= 32  Learning Rate= 0.001\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1439/1439 [==============================] - 192s 130ms/step - loss: 6.3974 - categorical_accuracy: 0.0702 - val_loss: 5.8684 - val_categorical_accuracy: 0.1061\n",
            "Epoch 2/50\n",
            "1439/1439 [==============================] - 183s 127ms/step - loss: 5.7476 - categorical_accuracy: 0.1075 - val_loss: 5.6917 - val_categorical_accuracy: 0.1243\n",
            "Epoch 3/50\n",
            "1439/1439 [==============================] - 183s 127ms/step - loss: 5.4600 - categorical_accuracy: 0.1293 - val_loss: 5.5911 - val_categorical_accuracy: 0.1495\n",
            "Epoch 4/50\n",
            "1439/1439 [==============================] - 183s 127ms/step - loss: 5.1880 - categorical_accuracy: 0.1529 - val_loss: 5.5515 - val_categorical_accuracy: 0.1638\n",
            "Epoch 5/50\n",
            "1439/1439 [==============================] - 183s 127ms/step - loss: 4.9883 - categorical_accuracy: 0.1670 - val_loss: 5.6121 - val_categorical_accuracy: 0.1738\n",
            "Epoch 6/50\n",
            "1439/1439 [==============================] - 183s 127ms/step - loss: 4.8346 - categorical_accuracy: 0.1822 - val_loss: 5.6290 - val_categorical_accuracy: 0.1761\n",
            "Epoch 7/50\n",
            "1439/1439 [==============================] - 183s 127ms/step - loss: 4.6342 - categorical_accuracy: 0.1958 - val_loss: 5.6901 - val_categorical_accuracy: 0.1804\n",
            "Epoch 8/50\n",
            "1439/1439 [==============================] - 183s 127ms/step - loss: 4.4420 - categorical_accuracy: 0.2156 - val_loss: 5.8093 - val_categorical_accuracy: 0.1833\n",
            "loading model...\n",
            "a a a a a a a a a a a but the true test of any hotel, as you know, is that morning cup of coffee.\n",
            " bobby (a whisper.\n",
            " cooper i'm going to truman.\n",
            " cooper (sarah's department store.\n",
            " cooper (to see me.\n",
            " he's a voice.\n",
            ".. emerald and the grave.\n",
            " cooper (, i knew.\n",
            ".. i'm going to you, i'm going to get him in a hand.\n",
            " leo johnson's voice (a safe.\n",
            " truman i'm going to get it.\n",
            " cooper (a middle of the room.\n",
            " cut to: int.\n",
            " sheriff's voice (everything in the.\n",
            " james (a other, but we've got into the car, mr. palmer.\n",
            " cooper i've got a new record, it's norma's voice (a to you.\n",
            " cooper i have to get out of the room.\n",
            " (a beat.\n",
            " truman and i'm going to jade.\n",
            " a little man, in the eye.\n",
            " he hayward's voice and i couldn't be his tape.\n",
            " cooper and i'm going to have to see me.\n",
            " cooper (a bad recorder, takes the fish.\n",
            " cooper i'm going to be a middle of the phone.\n",
            " cooper (smiles.\n",
            " they're going to the man, he was the room.\n",
            " cooper i'm going to see you, i've got a beat) i'm going to be a little man, we're been by the dream.\n",
            " cooper (he takes the car, several.\n",
            " cooper i'm a morning, and the sock.\n",
            " donna (a closer on a family, the house.\n",
            " she's.\n",
            ".. leo johnson's voice i'm not a the coffee.\n",
            " he's the day, i'm\n",
            "Build LSTM model.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_6 (Bidirection (None, 30, 2048)          47669248  \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 1024)              10489856  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4794)              4913850   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 63,072,954\n",
            "Trainable params: 63,072,954\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "RNN Size= 512  Batch Size= 64  Learning Rate= 0.001\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "720/720 [==============================] - 166s 222ms/step - loss: 6.4123 - categorical_accuracy: 0.0678 - val_loss: 5.8461 - val_categorical_accuracy: 0.1007\n",
            "Epoch 2/50\n",
            "720/720 [==============================] - 157s 218ms/step - loss: 5.6917 - categorical_accuracy: 0.1092 - val_loss: 5.6565 - val_categorical_accuracy: 0.1243\n",
            "Epoch 3/50\n",
            "720/720 [==============================] - 158s 219ms/step - loss: 5.4173 - categorical_accuracy: 0.1362 - val_loss: 5.5496 - val_categorical_accuracy: 0.1454\n",
            "Epoch 4/50\n",
            "720/720 [==============================] - 158s 219ms/step - loss: 5.1689 - categorical_accuracy: 0.1468 - val_loss: 5.5205 - val_categorical_accuracy: 0.1575\n",
            "Epoch 5/50\n",
            "720/720 [==============================] - 158s 219ms/step - loss: 4.9893 - categorical_accuracy: 0.1643 - val_loss: 5.5740 - val_categorical_accuracy: 0.1632\n",
            "Epoch 6/50\n",
            "720/720 [==============================] - 157s 219ms/step - loss: 4.7906 - categorical_accuracy: 0.1813 - val_loss: 5.6064 - val_categorical_accuracy: 0.1699\n",
            "Epoch 7/50\n",
            "720/720 [==============================] - 158s 219ms/step - loss: 4.6820 - categorical_accuracy: 0.1871 - val_loss: 5.6527 - val_categorical_accuracy: 0.1746\n",
            "Epoch 8/50\n",
            "720/720 [==============================] - 158s 219ms/step - loss: 4.4872 - categorical_accuracy: 0.2078 - val_loss: 5.7284 - val_categorical_accuracy: 0.1761\n",
            "loading model...\n",
            "a a a a a a a a a a a but the true test of any hotel, as you know, is that morning cup of coffee.\n",
            " james no.\n",
            " he's voice hurley's good, and i'm not trying to be.\n",
            " cooper and i'm not going to be a sign of hand.\n",
            " cooper and i'm going to know.\n",
            " i've got a small tape and all his eyes.\n",
            " he's a note.\n",
            " he was a great northern hotel - night \" a little man, who's voice i'm going to know it.\n",
            " he's a moment.\n",
            " catherine, i'm not all.\n",
            ".. leland palmer's voice and then i'm going to the door, i'm a long store.\n",
            " cooper i know you'll get him.\n",
            " i'm not to have to get the ground and the half.\n",
            " that was two.\n",
            " he's not out of the room, we're a feet, i'll be a couple of a beat.\n",
            " cooper and i'm going to fight? norma (tears.\n",
            " cooper (? shelly (the middle of a little man who's voice i'll be a hand.\n",
            " cooper (we're a beat.\n",
            " lucy and i'm going to be a small man? cooper (she'd have.\n",
            " he's voice i know, i've got a beat.\n",
            " cooper i'll be in the door, i'm not out.\n",
            " he's a kitchen - day establish.\n",
            " cut to: int.\n",
            " great northern hotel - day cooper i'm going to be a little man, the room.\n",
            " he's voice i'm going to even the table.\n",
            " the door.\n",
            " cooper i'll take a small phone) i'm going to know, i'm going to\n",
            "Build LSTM model.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_8 (Bidirection (None, 30, 2048)          47669248  \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4794)              9822906   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 82,666,170\n",
            "Trainable params: 82,666,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "RNN Size= 1024  Batch Size= 32  Learning Rate= 0.001\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "1439/1439 [==============================] - 243s 165ms/step - loss: 6.4406 - categorical_accuracy: 0.0668 - val_loss: 6.0157 - val_categorical_accuracy: 0.1042\n",
            "Epoch 2/50\n",
            "1439/1439 [==============================] - 234s 163ms/step - loss: 6.2455 - categorical_accuracy: 0.0984 - val_loss: 6.7172 - val_categorical_accuracy: 0.1272\n",
            "Epoch 3/50\n",
            "1439/1439 [==============================] - 235s 163ms/step - loss: 7.0369 - categorical_accuracy: 0.1003 - val_loss: 6.4947 - val_categorical_accuracy: 0.1360\n",
            "Epoch 4/50\n",
            "1439/1439 [==============================] - 235s 163ms/step - loss: 7.0329 - categorical_accuracy: 0.1036 - val_loss: 5.6010 - val_categorical_accuracy: 0.1587\n",
            "Epoch 5/50\n",
            "1439/1439 [==============================] - 236s 164ms/step - loss: 5.3120 - categorical_accuracy: 0.1605 - val_loss: 5.7482 - val_categorical_accuracy: 0.1642\n",
            "Epoch 6/50\n",
            "1439/1439 [==============================] - 236s 164ms/step - loss: 5.1769 - categorical_accuracy: 0.1718 - val_loss: 5.6284 - val_categorical_accuracy: 0.1718\n",
            "Epoch 7/50\n",
            "1439/1439 [==============================] - 236s 164ms/step - loss: 4.7607 - categorical_accuracy: 0.1897 - val_loss: 5.7308 - val_categorical_accuracy: 0.1755\n",
            "Epoch 8/50\n",
            "1439/1439 [==============================] - 236s 164ms/step - loss: 4.5474 - categorical_accuracy: 0.2097 - val_loss: 5.8207 - val_categorical_accuracy: 0.1804\n",
            "loading model...\n",
            "a a a a a a a a a a a but the true test of any hotel, as you know, is that morning cup of coffee.\n",
            " a beat.\n",
            " norma i'm sorry.\n",
            " cooper i'm going to him, i'll get a car, if you'll get a small.\n",
            " he's voice i'm not.\n",
            " she's voice is, i'm going to come in the sheriff.\n",
            " cooper (a beat.\n",
            " truman i'll get the phone.\n",
            " cooper i'll see me.\n",
            " leland moves into the phone, i don't know you.\n",
            " cooper and i'm not the same man and sees a beat.\n",
            " cut to: int.\n",
            " blue pine lodge kitchen - night re - day re - truman is you're, you'll come down, i'd like to the room.\n",
            " leland's.\n",
            " cooper (i'll be.\n",
            " benjamin (a little man, i'm not that, i'm not a beat.\n",
            " a little man.\n",
            " cooper it's voice i'll find it.\n",
            " cooper (will do you see a small death.\n",
            " cooper (a little dog.\n",
            " a beat.\n",
            " norma i'm gonna see the door.\n",
            " cooper (at the safe of your more.\n",
            " cooper i'll be to be down.\n",
            " cooper i'm a couple of a little man, the door.\n",
            " cut to: int.\n",
            " motel room b - day re - mike is a little man and i'm going to see you.\n",
            " cooper and i'm gonna need into the room, a little man.\n",
            " cooper's voice i'm going to help you, i'll have to get the phone.\n",
            " truman (a coffee.\n",
            " i'll get the table.\n",
            " she's voice i have you, i'm\n",
            "Build LSTM model.\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_10 (Bidirectio (None, 30, 2048)          47669248  \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 2048)              25174016  \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4794)              9822906   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4794)              0         \n",
            "=================================================================\n",
            "Total params: 82,666,170\n",
            "Trainable params: 82,666,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "RNN Size= 1024  Batch Size= 64  Learning Rate= 0.001\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "720/720 [==============================] - 210s 283ms/step - loss: 6.4224 - categorical_accuracy: 0.0662 - val_loss: 5.8561 - val_categorical_accuracy: 0.1052\n",
            "Epoch 2/50\n",
            "720/720 [==============================] - 200s 278ms/step - loss: 5.7334 - categorical_accuracy: 0.1121 - val_loss: 5.6061 - val_categorical_accuracy: 0.1407\n",
            "Epoch 3/50\n",
            "720/720 [==============================] - 202s 280ms/step - loss: 5.3770 - categorical_accuracy: 0.1401 - val_loss: 5.4819 - val_categorical_accuracy: 0.1560\n",
            "Epoch 4/50\n",
            "720/720 [==============================] - 202s 281ms/step - loss: 5.1145 - categorical_accuracy: 0.1601 - val_loss: 5.5215 - val_categorical_accuracy: 0.1695\n",
            "Epoch 5/50\n",
            "720/720 [==============================] - 202s 281ms/step - loss: 4.8737 - categorical_accuracy: 0.1772 - val_loss: 5.4621 - val_categorical_accuracy: 0.1759\n",
            "Epoch 6/50\n",
            "720/720 [==============================] - 202s 280ms/step - loss: 4.7391 - categorical_accuracy: 0.1909 - val_loss: 5.5366 - val_categorical_accuracy: 0.1779\n",
            "Epoch 7/50\n",
            "720/720 [==============================] - 202s 281ms/step - loss: 4.5494 - categorical_accuracy: 0.2004 - val_loss: 5.5533 - val_categorical_accuracy: 0.1798\n",
            "Epoch 8/50\n",
            "720/720 [==============================] - 202s 281ms/step - loss: 4.3942 - categorical_accuracy: 0.2132 - val_loss: 5.6721 - val_categorical_accuracy: 0.1802\n",
            "Epoch 9/50\n",
            "720/720 [==============================] - 203s 281ms/step - loss: 4.2224 - categorical_accuracy: 0.2277 - val_loss: 5.7253 - val_categorical_accuracy: 0.1769\n",
            "loading model...\n",
            "a a a a a a a a a a a but the true test of any hotel, as you know, is that morning cup of coffee.\n",
            " i'm not going to get her husband, when i'm not like it.\n",
            " cooper (to truman, i'm not like you? truman (to be a little record ,.\n",
            " i don't know what happened? james (to be a young man, and the pie.\n",
            " cooper (to the phone.\n",
            " cooper (she is at the kitchen.\n",
            " cut to: int.\n",
            " blue pine lodge kitchen - day cooper, i'm so much.\n",
            " cooper (to the stairs and a large suit.\n",
            " i'm not so much.\n",
            " cooper (to the phone.\n",
            " audrey (pleasantly) i don't know, i'm so much.\n",
            " cooper (to see it.\n",
            " i'm not not like a small long place.\n",
            " the tape and sees the key.\n",
            " he's not.\n",
            " a little man.\n",
            " cooper (he takes a pair of fact of a small beautiful young man, i'm not like it.\n",
            " jerry is a small small hand, takes out a little man are you, i'll be on a small phone.\n",
            " (he's going to have to get this.\n",
            " he's going to have to be out of the phone.\n",
            " leo i'm not not like it.\n",
            " (she was here.\n",
            " cooper (quietly and a small note.\n",
            " cooper (he starts to get it for you, i'm like that.\n",
            " he's going to get it.\n",
            " cooper (to be a beat.\n",
            " cut to: int.\n",
            " black lake cemetery - night josie sets the funeral.\n",
            " cooper (to see you? cooper i've been in the room.\n",
            " cut\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}